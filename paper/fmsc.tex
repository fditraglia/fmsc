\documentclass[12pt]{article}
\usepackage[margin=1.2in]{geometry}
\usepackage{rotating}
\usepackage{amssymb, amsmath, amsthm} 
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{todonotes}
\usepackage{mathrsfs}
\usepackage{setspace}
% \singlespacing
\onehalfspacing
% \doublespacing
% \setstretch{<factor>} % for custom spacing

\RequirePackage{natbib}
\RequirePackage[colorlinks]{hyperref}
\RequirePackage{hypernat}


\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]

%Bold proof titles
\makeatletter \renewenvironment{proof}[1][\proofname] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother



\begin{document}



\title{Using Invalid Instruments on Purpose: Focused Moment Selection and Averaging for GMM\footnote{I thank Aislinn Bohren, Gerda Claeskens, Bruce Hansen, Byunghoon Kang, Toru Kitagawa, Hannes Leeb, Serena Ng, Alexei Onatski, Hashem Pesaran, Benedikt P\"{o}tscher, Frank Schorfheide, Neil Shephard,  Richard J.\ Smith, Stephen Thiele, Melvyn Weeks, as well as seminar participants at Cambridge, the University of Vienna, Queen Mary, St Andrews, George Washington, UPenn, Columbia, Oxford, and the 2011 Econometric Society European Meetings for their many helpful comments and suggestions. I thank Kai Carstensen for providing data for my empirical example.}}

\author{Francis J.\ DiTraglia \\ University of Pennsylvania}

\date{\normalsize First Version: November 9, 2011 \\ This Version: \today}

\maketitle 
\begin{abstract}
	\input{abstract.tex}
\end{abstract}

\input{introduction.tex}

\input{asymptotic_framework.tex}

\input{OLSvsIVlowlevel.tex}



\subsection{Example: Choosing Instrumental Variables}
\label{sec:chooseIVlowlevel}
The preceding example was quite specific, but it a sense it amounted to a problem of instrument selection: if $x$ is exogenous, it is clearly its ``own best instrument.'' 
Viewed from this perspective, the FMSC amounted to trading off endogeneity against instrument strength. We now consider instrument selection \emph{in general} for linear GMM estimators in an iid setting. 
Consider the following model:
\begin{eqnarray}
    y_i &=& \mathbf{x}_i' \beta +  \epsilon_i\\
    \mathbf{x}_i &=&  \Pi_1' \mathbf{z}_{i}^{(1)} + \Pi_2'\mathbf{z}_{i}^{(2)} + \mathbf{v}_i
\end{eqnarray}
where $y$ is an outcome of interest, $\mathbf{x}$ is an $r$-vector of regressors, some of which are endogenous, $\mathbf{z}^{(1)}$ is a $p$-vector of instruments known to be exogenous, and $\mathbf{z}^{(2)}$ is a $q$-vector  of \emph{potentially endogenous} instruments. 
The $r$-vector $\beta$, $p\times r$ matrix $\Pi_1$, and $q\times r$ matrix $\Pi_2$ contain unknown constants. Stacking observations in the usual way, let $\mathbf{y}' = (y_1, \hdots, y_n)$, $X' = (\mathbf{x}_1, \hdots, \mathbf{x}_n)$, $\boldsymbol{\epsilon} = (\epsilon_1, \hdots, \epsilon_n)$, 
$Z_1' = (\mathbf{z}_{1}^{(1)}, \hdots, \mathbf{z}_{n}^{(1)})$, $Z_2' = (\mathbf{z}_{1}^{(2)}, \hdots, \mathbf{z}_{n}^{(2)})$, and $V' = (\mathbf{v}_1, \hdots, \mathbf{v}_n)$ where $n$ is the sample size.
Using this notation, $\mathbf{y} = X\beta +\boldsymbol{\epsilon}$ and $X =  Z \Pi + V$, where $Z = (Z_1, Z_2)$ and $\Pi = (\Pi_1', \Pi_2')'$. 

The idea behind this setup is that the instruments contained in $Z_2$ are expected to be strong. 
If we were confident that they were exogenous, we would certainly use them in estimation. 
Yet the very fact that we expect them to be strongly correlated with $\mathbf{x}$ gives us reason to fear that the instruments contained in $Z_2$ may be endogenous. 
The exact opposite is true of $Z_1$. These are the instruments that we are prepared to assume are exogenous. 
But when is such an assumption plausible? Precisely when the instruments contained in $Z_1$ are \emph{not especially strong}. 
\todo[inline]{Should briefly mention some situations in which this setup arises. Panel data, etc. Also refer to my empirical example.}
In this setting, the FMSC attempts to trade off a small increase in bias from using a \emph{slightly} endogenous instrument against a larger decrease in variance from increasing the overall strength of the instruments used in estimation.

To this end, consider a general linear GMM estimator of the form
$$\widehat{\beta}_S = (X'Z_S \widetilde{W}_S Z_S' X)^{-1}X'Z_S \widetilde{W}_S  Z_S' \mathbf{y}$$
where $S$ indexes the instruments used in estimation, $Z_S'  = \Xi_S Z'$ is the matrix containing only those instruments included in $S$, $|S|$ is the number of instruments used in estimation and $\widetilde{W}_S$ is an $|S|\times|S|$ positive semi-definite weighting matrix. In this example, the local mis-specification assumption is given by
\begin{equation}
  E\left[\begin{array}
    {c}
    \textbf{z}_{ni}^{(1)} (y_i - \textbf{x}_i\beta) \\
    \textbf{z}_{ni}^{(2)} (y_i - \textbf{x}_i \beta)
\end{array}\right] = \left[
  \begin{array}
    {c}
    \textbf{0} \\ \boldsymbol{\tau}/\sqrt{n}
  \end{array}
\right]
\end{equation}
where $\mathbf{0}$ is a $p$-vector of zeros, and $\boldsymbol{\tau}$ is a $q$-vector of unknown constants. 
The following low-level conditions are sufficient for the asymptotic normality of $\widehat{\beta}_S$.

\begin{assump}[Choosing IVs]
\label{assump:chooseIV} 
	Let $\{(\mathbf{z}_{ni}, \mathbf{v}_{ni}, \epsilon_{ni})\colon 1\leq i \leq n, n = 1, 2, \hdots\}$ be a triangular array of random variables with $\mathbf{z}_{ni} = (\mathbf{z}_{ni}^{(1)}$, $\mathbf{z}_{ni}^{(1)})$ such that
	\begin{enumerate}[(a)]
		\item $(\mathbf{z}_{ni}, \mathbf{v}_{ni}, \epsilon_{ni}) \sim$ iid within each row of the array (i.e.\ for fixed $n$)
		\item $E[\mathbf{v}_{ni}\mathbf{z}_{ni}']=\mathbf{0}$, $E[\mathbf{z}^{(1)}_{ni} \epsilon_{ni}]=\mathbf{0}$, and $E[\mathbf{z}^{(2)}_{ni} \epsilon_{ni}] = \boldsymbol{\tau}/\sqrt{n}$ for all $n$
		\item $E[\left|\mathbf{z}_{ni}\right|^{4+\eta}] <C$, $E[\left|\epsilon_{ni}\right|^{4+\eta}] <C$, and $E[\left|\mathbf{v}_{ni}\right|^{4+\eta}] <C$ for some $\eta >0$, $C <\infty$
		\item $E[\mathbf{z}_{ni} \mathbf{z}_{ni}'] \rightarrow Q>0$ and $E[\epsilon_{ni}^2 \mathbf{z}_{ni} \mathbf{z}_{ni}'] \rightarrow \Omega >0$ as $n\rightarrow \infty$
		\item $\mathbf{x}_{ni} =  \Pi_1' \mathbf{z}_{ni}^{(1)} + \Pi_2'\mathbf{z}_{ni}^{(2)} + \mathbf{v}_{ni}$ where $\Pi_1 \neq \mathbf{0}$, $\Pi_2 \neq \mathbf{0}$, and $y_i = \mathbf{x}_{ni}' \beta +  \epsilon_{ni}$
	\end{enumerate}
\end{assump}

The preceding conditions are similar to although more general than those contained in Assumption \ref{assump:OLSvsIV}. 
While we no longer assume homoskedasticity, for simplicity we retain the assumption that the triangular array is iid in each row. 

\begin{thm}[Choosing IVs Limit Distribution]
\label{thm:chooseIV} Suppose that $\widetilde{W}_S \rightarrow_p W_S >0$. Then, under Assumption \ref{assump:chooseIV}
$$\sqrt{n}\left(\widehat{\beta}_S - \beta \right) \overset{d}{\rightarrow} -K_S \Xi_S \left(\left[\begin{array}
           {c} \mathbf{0} \\ \boldsymbol{\tau}
         \end{array}\right] + M \right)$$
where
         $$-K_S = \left(\Pi' Q_S W_S Q_S'\Pi\right)^{-1} \Pi'Q_SW_S$$
$M \sim N(\mathbf{0}, \Omega)$, $Q_S = Q \Xi_S'$, and $Q$ and $\Omega$ are defined in Assumption \ref{assump:chooseIV}.
\end{thm}



\section{The Focused Moment Selection Criterion}
\label{sec:FMSC}

\subsection{The General Case}
FMSC chooses among the potentially invalid moment conditions contained in $h$ to minimize estimator AMSE for a target parameter. 
Denote this target parameter by $\mu$, a real-valued, $Z$-almost continuous function of the parameter vector $\theta$ that is differentiable in a neighborhood of $\theta_0$. 
Further, define the GMM estimator of $\mu$ based on $\widehat{\theta}_S$ by $\widehat{\mu}_S = \mu(\widehat{\theta}_S)$ and the true value of $\mu$ by $\mu_0 = \mu(\theta_0)$. 
Applying the Delta Method to Theorem \ref{thm:normality} gives the AMSE of $\widehat{\mu}_S$.

\begin{cor}[AMSE of Target Parameter]
\label{cor:target}
Under the hypotheses of Theorem \ref{thm:normality}, 
$$\sqrt{n}\left(\widehat{\mu}_S - \mu_0\right)\rightarrow_d-\nabla_\theta\mu(\theta_0)'K_S \Xi_S \left(M +  \left[\begin{array}
	{c} 0 \\ \tau
\end{array} \right]\right)$$ 
where $M$ is defined in Theorem \ref{thm:normality}.
Hence,
	$$\mbox{AMSE}\left(\widehat{\mu}_S\right) = \nabla_\theta\mu(\theta_0)'K_S \Xi_S \left\{\left[\begin{array}{cc}0&0\\0&\tau\tau'\end{array}\right] + \Omega\right\}\Xi_S'K_S'\nabla_\theta\mu(\theta_0).$$
\end{cor}

For the valid estimator $\widehat{\theta}_v$ we have $K_v = \left[G'W_{v}G\right]^{-1}G' W_{v}$ and $\Xi_v =\left[\begin{array}{cc} \mathbf{I}_p& \mathbf{0}_{p\times q} \end{array} \right]$. 
Thus, the valid estimator $\widehat{\mu}_v$ of $\mu$ has zero asymptotic bias. 
In contrast, any candidate estimator $\widehat{\mu}_S$ that includes moment conditions from $h$ inherits an asymptotic bias from the corresponding elements of $\tau$. 
We see that the extent and direction of this bias depends both on $K_S$ and $\nabla_\theta\mu(\theta_0)$. 
Adding moment conditions from $h$, however, generally decreases asymptotic variance. 
In particular, the usual proof that adding moment conditions cannot increase asymptotic variance under efficient GMM \citep[see for example][ch.\ 6]{Hallbook} continues to hold under local mis-specification, because all moment conditions are correctly specified in the limit. 
Thus, we see that local mis-specification gives an asymptotic analogue of the bias-variance tradeoff that we encounter in finite samples.
\footnote{The general result for adding moment conditions in GMM is only relevant in situations where the valid moment set is strictly nested inside of all other candidate moment sets. When this does not hold, such as in the OLS verus IV example, we establish an analogous ordering of asymptotic variances by direct calculation.} 

To use this framework for moment selection, we need to construct estimators of the unknown quantities: $\theta_0$, $K_S$, $\Omega$, and $\tau$. 
Under local mis-specification, the estimator of $\theta$ under \emph{any} moment set is consistent. 
A natural estimator is $\widehat{\theta}_v$, although there are other possibilities. 
Recall that $K_S = [F_S'W_SF_S]^{-1} F_S'W_S \Xi_S$.
Now, $\Xi_S$ is known because it is simply the selection matrix defining moment set $S$. 
The remaining quantities $F_S$ and $W_S$ that make up $K_S$ are consistently estimated by their sample analogues under Assumption \ref{assump:highlevel}.
Similarly, consistent estimators of $\Omega$ are readily available under local mis-specification, although the precise form depends on the situation.
We consider this point further below as it relates to our two running examples.

The only remaining unknown is $\tau$. Local mis-specification is essential for making meaningful comparisons of AMSE because it prevents the bias term from dominating the comparison. U
nfortunately, it also prevents us from consistently estimating this asymptotic bias parameter. 
Under Assumption \ref{assump:Identification}, however, we can construct an \emph{asymptotically unbiased} estimator $\widehat{\tau}$ of $\tau$ by substituting $\widehat{\theta}_v$, the estimator of $\theta_0$ that uses only correctly specified moment conditions, into $h_n$, the sample analogue of the (potentially) mis-specified moment conditions. 
In other words,  $\widehat{\tau} = \sqrt{n} h_n(\widehat{\theta}_v)$. 

\begin{thm}[Asymptotic Distribution of $\widehat{\tau}$] 
\label{thm:tau}
Let $\widehat{\tau} = \sqrt{n} h_n(\widehat{\theta}_v)$ where $\widehat{\theta}_v$ is the valid estimator, based only on the moment conditions contained in $g$. 
Then under Assumptions \ref{assump:drift}, \ref{assump:highlevel} and \ref{assump:Identification}
$$\widehat{\tau} \rightarrow_d \Psi\left( M + \left[\begin{array}
	{c} 0 \\ \tau
\end{array} \right]\right), \quad \Psi = \left[\begin{array}{cc} -HK_v & \mathbf{I}_q \end{array}\right]$$ 
where $K_v$ is defined in Corollary \ref{cor:valid}.
Thus, $\widehat{\tau}\rightarrow_d (\Psi M + \tau) \sim N_q(\tau, \Psi \Omega \Psi')$.
\end{thm}

Returning to Corollary $\ref{cor:target}$, however, we see that it is $\tau \tau'$ rather than $\tau$ that enters the expression for AMSE. 
Although $\widehat{\tau}$ is an asymptotically unbiased estimator of $\tau$, the limiting expectation of $\widehat{\tau} \widehat{\tau}'$ is not $\tau\tau'$ because $\widehat{\tau}$ has an asymptotic variance.  
To obtain an asymptotically unbiased estimator of $\tau\tau'$ we proceed as follows, subtracting a consistent estimate of the asymptotic variance.


\begin{cor}[Asymptotically Unbiased Estimator of $\tau \tau'$]
\label{cor:tautau}
If $\widehat{\Omega}$ and $\widehat{\Psi}$ are consistent for $\Omega$ and $\Psi$, then $ \widehat{\tau}\widehat{\tau}' - \widehat{\Psi}\widehat{\Omega}\widehat{\Psi}$ is an asymptotically unbiased estimator of $\tau\tau'$.
\end{cor}
It follows that
\begin{equation}
\label{eq:fmsc}
	\mbox{FMSC}_n(S) = \nabla_\theta\mu(\widehat{\theta})'\widehat{K}_S\Xi_S \left\{\left[\begin{array}{cc}0&0\\0&\widehat{\tau}\widehat{\tau}' - \widehat{\Psi}\widehat{\Omega}\widehat{\Psi}'\end{array}\right] + \widehat{\Omega}\right\}\Xi_S'\widehat{K}_S' \nabla_\theta\mu(\widehat{\theta})
\end{equation}
provides an asymptotically unbiased estimator of AMSE.
\todo[inline]{Comment on how we could look at other risk functions. Talk about why an asymptotically unbiased estimator can make sense. At least try to justify why this is reasonable.}


\subsection{Digression: Failure of the Identification Condition}
\label{subsec:digress}
When $r > p$, Assumption \ref{assump:Identification} fails: $\theta_0$ is not estimable by $\widehat{\theta}_v$ so $\widehat{\tau}$ is an infeasible estimator of $\tau$. 
A na\"{i}ve approach to this problem would be to substitute another consistent estimator of $\theta_0$ and proceed analogously. 
Unfortunately, this approach fails. To understand why, consider the case in which all moment conditions are potentially invalid so that the $g$--block is empty. 
Letting $\widehat{\theta}_f$ denote the estimator based on the full set of moment conditions in $h$,  $\sqrt{n}h_n(\widehat{\theta}_f) \rightarrow_d\Gamma  \mathcal{N}_q(\tau, \Omega)$ where $\Gamma = \mathbf{I}_q - H \left(H'WH\right)^{-1}H'W$, using an argument similar to that in the proof of Theorem \ref{thm:tau}. 
The mean, $\Gamma \tau$, of the resulting limit distribution does not equal $\tau$, and because $\Gamma$ has rank $q-r$ we cannot pre-multiply by its inverse to extract an estimate of $\tau$.
Intuitively, $q-r$ over-identifying restrictions are insufficient to estimate a $q$-vector: $\tau$ is not identified unless we have a minimum of $r$ valid moment conditions. 
However, the limiting distribution of $\sqrt{n}h_n(\widehat{\theta}_f)$ partially identifies $\tau$ even when we have no valid moment conditions at our disposal. 
A combination of this information with prior restrictions on the magnitude of the components of $\tau$ allows the use of the FMSC framework to carry out a sensitivity analysis when $r>p$. 
For example, the worst-case estimate of AMSE over values of $\tau$ in the identified region could still allow certain moment sets to be ruled out.
This idea shares similarities with \citet{Kraay} and \citet{Conleyetal}, two recent papers that suggest methods for evaluating the robustness of conclusions drawn from IV regressions when the instruments used may be invalid.



\subsection{FMSC for OLS versus 2SLS Example}
\label{sec:FMSCforOLSvsIV}
The FMSC has a particularly convenient and transparent form in the OLS versus 2SLS example introduced in Section \ref{sec:OLSvsIVlowlevel}.
Since the target parameter in this case is simply $\beta$, the FMSC amounts to comparing the AMSE of OLS to that of 2SLS. As an immediate consequence of Theorem \ref{thm:OLSvsIV}, we have
$$\mbox{AMSE(OLS)} = \frac{\tau^2}{\sigma_x^4} + \frac{\sigma_\epsilon^2}{\sigma_x^2},\quad \quad
  \mbox{AMSE(2SLS)} = \frac{\sigma_\epsilon^2}{\gamma^2}$$
 Rearranging, we see that the AMSE of the OLS estimator is strictly less than that of the 2SLS estimator whenever $\tau^2  < \sigma_x^2 \sigma_\epsilon^2\sigma_v^2/\gamma^2$. 
 To use this expression for moment selection we need to estimate the unknown parameters.
 Fortunately, the familiar estimators of $\sigma_x^2, \gamma^2$, and $\sigma_v^2$ remain consistent under Assumption \ref{assump:OLSvsIV} so we set
 $$\widehat{\sigma}_x^2 = n^{-1}\mathbf{x}'\mathbf{x}, \quad \widehat{\gamma}^2 = n^{-1}\mathbf{x}'Z(Z'Z)^{-1}Z'\mathbf{x}, \quad \widehat{\sigma}_v^2 =  \widehat{\sigma}_x^2 - \widehat{\gamma}^2.$$
To estimate $\sigma_\epsilon^2$ we have two choices: we can use either use the residuals from the OLS estimator or those from the 2SLS estimator.
Under local mis-specification, both provide consistent estimators of $\sigma_\epsilon^2$. 
We would expect the estimator based on the 2SLS residuals to be more robust, however, unless the instruments are quite weak. 
This is because the exogeneity of $x$, even though it disappears in the limit under our asymptotics, is non-zero in finite samples. 
Thus, we use 
	$$\widehat{\sigma}_\epsilon^2 = n^{-1}\left(\textbf{y} - \textbf{x}\widetilde{\beta}_{2SLS} \right)'\left(\textbf{y} - \textbf{x}\widetilde{\beta}_{2SLS} \right)$$
to estimate $\sigma_\epsilon^2$ below. 
All that remains is to estimate $\tau^2$. Specializing Theorem \ref{thm:tau} and Corollary \ref{cor:tautau} to the present example gives the following result.
\begin{thm}
	\label{thm:tauOLSvsIV}
	Let $\widehat{\tau} =  n^{-1/2} \mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta}_{2SLS})$. Under Assumption \ref{assump:OLSvsIV} we have 
	$$\widehat{\tau}\rightarrow_d N(\tau,V), \quad V = \sigma_\epsilon^2 \sigma_x^2(\sigma_v^2/\gamma^2).$$ 
\end{thm}
It follows that $\widehat{\tau}^2 -  \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2 \left(\widehat{\sigma}_v^2/\widehat{\gamma}^2\right)$ is an asymptotically unbiased estimator of $\tau^2$ and hence, substituting into the AMSE inequality from above and rearranging, the FMSC instructs us to choose OLS whenever $\widehat{T}_{FMSC} = \widehat{\tau}^2/\widehat{V} < 2$
where $\widehat{V} = \widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2/\widehat{\gamma}^2$. 
The quantity $\widehat{T}_{FMSC}$ looks very much like a test statistic and indeed it can be viewed as such. 
By Theorem \ref{thm:tauOLSvsIV} and the continuous mapping theorem, $\widehat{T}_{FMSC} \rightarrow_d \chi^2(1)$. 
This means that we can interpret the FMSC as a test of the null hypothesis $H_0\colon \tau = 0$ against the two-sided alternative with a critical value of $2$. 
This corresponds to a significance level of $\alpha \approx 0.16$. 

But how does this novel ``test'' compare to something more familiar, say the Durbin-Hausman-Wu (DHW) test? 
It turns out that in this particular example, although not in general, carrying out moment selection via the FMSC is \emph{numerically equivalent} to using OLS unless the DHW test rejects at the 16\% level. 
In other words, $\widehat{T}_{FMSC} = \widehat{T}_{DHW}$. 
To see why this is so first note that 
$$ \sqrt{n}\left(\widehat{\beta}_{OLS} - \widetilde{\beta}_{2SLS}\right) =\left[\begin{array}{cc}
    1 & -1
    \end{array}\right] \sqrt{n} \left(\begin{array}{c}
    \widehat{\beta}_{OLS} - \beta  \\ \widetilde{\beta}_{2SLS} - \beta
    \end{array} \right) \rightarrow_d N\left(\tau/\sigma_x^2, \Sigma
    \right).$$
by Theorem \ref{thm:tauOLSvsIV}, where
	$$\Sigma = \mbox{AVAR(2SLS)} - \mbox{AVAR(OLS)} = \sigma_\epsilon^2 \left(1/\gamma^2 - 1/\sigma_x^2 \right).$$
Thus, under $H_0 \colon \tau = 0$, the DHW test statistic 
$$\widehat{T}_{DHW} = n\, \widehat{\Sigma}^{-1}(\widehat{\beta}_{OLS} - \widetilde{\beta}_{2SLS})^2 = \frac{n(\widehat{\beta}_{OLS} - \widetilde{\beta}_{2SLS})^2}{ \widehat{\sigma}_\epsilon^2 \left(1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2 \right)}$$
converges in distribution to a $\chi^2(1)$ random variable. 
Now, rewriting $\widehat{V}$, we find that
$$\widehat{V} = \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2\left(\frac{\widehat{\sigma}_v^2 }{\widehat{\gamma}^2}\right) = \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2\left(\frac{\widehat{\sigma}_x^2 - \widehat{\gamma}^2 }{\widehat{\gamma}^2}\right) = \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^4\left(\frac{1}{\widehat{\gamma}^2} - \frac{1}{\widehat{\sigma}_x^2}\right) = \widehat{\sigma}_x^4 \,\widehat{\Sigma}$$
using the fact that $\widehat{\sigma}_v = \widehat{\sigma}_x^2 - \widehat{\gamma}^2$. 
Thus, to show that $\widehat{T}_{FMSC} = \widehat{T}_{DHW}$, all that remains is to establish that $\widehat{\tau}^2 = n\widehat{\sigma}_x^4 (\widehat{\beta}_{OLS} - \widetilde{\beta}_{2SLS})^2$, which we obtain as follows:
    $$\widehat{\tau}^2  =  \left[n^{-1/2} \textbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})\right]^2 = n^{-1}\left[\mathbf{x}'\mathbf{x} \left( \widehat{\beta} - \widetilde{\beta}\right) \right]^2 = n^{-1}\left[n \widehat{\sigma}_x^2 \left( \widehat{\beta} - \widetilde{\beta}\right) \right]^2.$$

The equivalence between FMSC selection and a DHW test in this example is helpful for two reasons. 
First, it provides a novel justification for the use of the DHW test to select between OLS and 2SLS. So long as it is carried out with $\alpha \approx 16\%$, the DHW test is equivalent to selecting the estimator that minimizes an asymptotically unbiased estimator of AMSE. 
Note that this significance level differs from the more usual values of 5 or 10\% in that it leads us to select 2SLS \emph{more often}: OLS should indeed be given the benefit of the doubt, but not by so wide a margin as traditional practice suggests. 
Second, this equivalence shows that the FMSC can be viewed as an \emph{extension} of the machinery behind the familiar DHW test to more general GMM environments. 
Naturally each application of the FMSC should be evaluated on its own merits, but it is reassuring that the local mis-specification framework leads to a reasonable procedure in a simple setting where we can compare it to more familiar techniques.

\subsection{Simulation Study: OLS versus 2SLS Example}
\todo[inline]{Run this simulation properly and add in the details and results!}

\subsection{FMSC for Instrument Selection Example}
\label{sec:chooseIVFMSC}
As explained above, the OLS versus 2SLS example can be viewed as a special case of the more general instrument selection problem from Section \ref{sec:chooseIVlowlevel}.
To apply the FMSC to this problem, we simply need to specialize Equation \ref{eq:fmsc}.
Recall that $Z_1$ is the matrix containing the $p$ instruments known to be exogenous, $Z_2$ is the matrix containing the $q$ potentially endogenous instruments and $Z = (Z_1, Z_2)$.
To simplify the notation in this section, let
\begin{equation}
\label{eq:xi12}
\Xi_1 = \left[\begin{array}{cc} I_{p} & 0_{p \times q}  \end{array}\right], \quad
    \Xi_2 = \left[ \begin{array}{cc}
              0_{q \times p}& I_{q}
            \end{array}\right]	
\end{equation}
where $0_{m\times n}$ denotes an $n\times m$ matrix of zeros and $I_m$ denotes the $m\times m$ identity matrix.
Using this convention, $Z_1 = Z \Xi_1'$ and $Z_2 = Z \Xi_2'$.
In this example the valid estimator, defined in Assumption \ref{assump:Identification}, is given by
\begin{equation}
\label{eq:betav}
\widehat{\beta}_v = \left(X'Z_1 \widetilde{W}_v Z_1' X\right)^{-1}X'Z_1 \widetilde{W}_v Z_1' \mathbf{y}	
\end{equation}
and we estimate $\nabla_\beta \mu(\beta)$ with $\nabla_\beta \mu(\widehat{\beta}_v)$.  
Similarly, 
$$-\widehat{K}_S = n\left(X'Z \Xi_S' \widetilde{W}_S \Xi_S Z' X\right)^{-1}X' Z \Xi_S' \widetilde{W}_S$$
is the natural consistent estimator of $-K_S$ in this setting.\footnote{The negative sign is squared in the FMSC expression and hence disappears. We write it here only to be consistent with the notation of Theorem \ref{thm:normality}.}
Since $\Xi_S$ is known, the only remaining quantities from Equation \ref{eq:fmsc} are $\widehat{\boldsymbol{\tau}}$, $\widehat{\Psi}$ and $\widehat{\Omega}$. 
To proceed further, we first specialize Theorem \ref{thm:tau} as follows.
\begin{thm}
Let $\widehat{\boldsymbol{\tau}} = n^{-1/2} Z_2' ( \mathbf{y} - X\widehat{\beta}_v)$ where $\widehat{\beta}_v$ is as defined in Equation \ref{eq:betav}. Under Assumption \ref{assump:chooseIV} we have
$\widehat{\boldsymbol{\tau}} \rightarrow_d \boldsymbol{\tau} + \Psi M$
where $M$ is defined in Theorem \ref{thm:chooseIV},
\begin{eqnarray*}
	\Psi &=&\left[ \begin{array}{cc}-\Xi_2Q \Pi K_v  & I_{q} \end{array}\right] \\
	-K_v &=& \left(\Pi' Q \Xi'_1 W_v \Xi_1 Q'\Pi\right)^{-1} \Pi'Q \Xi_1' W_v
\end{eqnarray*}
$W_v$ is the probability limit of the weighting matrix from Equation \ref{eq:betav}, $I_q$ is the $q\times q$ identity matrix, $\Xi_1$ is defined in Equation \ref{eq:xi12},  and $Q$ is defined in Assumption \ref{assump:chooseIV}.
\end{thm}
Using the preceding result, we construct the asymptotically unbiased estimator $\widehat{\tau}\widehat{\tau}' - \widehat{\Psi}\widehat{\Omega} \widehat{\Psi}'$ of $\tau\tau'$ from
	$$\widehat{\Psi} = \left[ \begin{array}
		{cc}
		-n^{-1}Z_2'X \widehat{K}_v & I_q
	\end{array}\right], \quad -\widehat{K}_v = n\left(X'Z_1 \widetilde{W}_v Z_1' X\right)^{-1}X'Z_1 \widetilde{W}_v$$

All that remains before we can substitute into Equation \ref{eq:fmsc} is to estimate $\Omega$. 
There are many possible ways to proceed, depending on the problem at hand and the assumptions one is willing to make. 
In the simulation and empirical examples discussed below we examine the 2SLS estimator, that is $\widetilde{W}_S = (\Xi_S Z'Z\Xi_S)^{-1}$, and estimate $\Omega$ as follows. 
For all specifications \emph{except} the valid estimator $\widehat{\beta}_v$, we employ the centered, heteroscedasticity-consistent estimator
\begin{equation}
	\widehat{\Omega}= \frac{1}{n}\sum_{i=1}^n u_i(\widehat{\beta}_{full})^2\mathbf{z}_i \mathbf{z}_i'  - \left(\frac{1}{n}\sum_{i=1}^n u_i(\widehat{\beta}_{full})\mathbf{z}_i   \right)\left(\frac{1}{n}\sum_{i=1}^n  u_i(\widehat{\beta}_{full})\mathbf{z}_i'  \right)
\end{equation}
where $u_i(\beta) = y_i - \mathbf{x}_i'\beta$ and $\widehat{\beta}_{full} = (X'Z(Z'Z)^{-1}Z'X)^{-1}X'Z(Z'Z)^{-1}Z'\mathbf{y}$.
Centering allows moment functions to have non-zero means. 
While the local mis-specification framework implies that these means tend to zero in the limit, they are non-zero for any fixed sample size. 
Centering accounts for this fact, and thus provides added robustness. 
Since the valid estimator $\widehat{\beta}_v$ has no asymptotic bias, the AMSE of any target parameter based on this estimator equals its asymptotic variance. 
Accordingly, we use 
\begin{equation}
	\widetilde{\Omega}_{11}= n^{-1}\sum_{i=1}^n u_i(\widehat{\beta}_{full})^2\mathbf{z}_{1i}\mathbf{z}_{1i}'
\end{equation}
rather than the $(p\times p)$ upper left sub-matrix of $\widehat{\Omega}$ to estimate this quantity. 
This imposes the assumption that all instruments in $Z_1$ are valid so that no centering is needed, providing greater precision.

\subsection{Simulation Study: Choosing Instruments Example}\label{sec:fmscsim}
\todo[inline]{Update with revised simulations and shorten. Be sure to show that even when we use trimmed AMSE or median absolute deviation we're getting what we want. Emphasize how large the gains are.}
This section evaluates the performance of FMSC in a simple 2SLS instrument selection problem. The simulation setup is as follows:
\begin{eqnarray}
		\label{eq:secondstage}
		y_i &=& 0.5 x_i + u_i\\ 
		x_i &=& 0.1 (z_{1i} + z_{2i} + z_{3i}) + \gamma w_i + \epsilon_i 
		\label{eq:firststage}
	\end{eqnarray}
for $i=1, 2, \hdots, n$ where $(u_i, \epsilon_i, w_i)' \sim \mbox{ iid  } \mathcal{N}(0,\mathcal{V})$ with	
\begin{equation}
			\label{eq:varmatrix}
			\mathcal{V} = \left[  
				\begin{array}{cccc}
					1 & 0.5 - \gamma\rho & \rho\\
					0.5 - \gamma \rho & 1 & 0\\
					\rho & 0 & 1 \\
				\end{array}
		\right]
\end{equation}	
independently of $(z_{1i}, z_{2i}, z_{3i})\sim \mathcal{N}(0, \mathbf{I})$. This design keeps the endogeneity of $x$ fixed, $Cov(x,u) = 0.5$, while allowing the validity and relevance of $w$ to vary according to $Cov(w,u) =\rho$, $Cov(w,x) = \gamma$. The instruments $z_1, z_2, z_3$ are valid and relevant: they have first-stage coefficients of $0.1$ and are uncorrelated with the second stage error $u$. 

Our goal is to estimate the effect of $x$ on $y$ with minimum MSE by choosing between two estimators: the valid estimator that uses only $z_1, z_2,$ and $z_3$ as instruments, and the full estimator that uses $z_1, z_2, z_3,$ and $w$. The inclusion of $z_1, z_2$ and $z_3$ in both moment sets means that the order of over-identification is two for the valid estimator and three for the full estimator. Because the moments of the 2SLS estimator only exist up to the order of over-identification \citep{Phillips1980}, this ensures that the small-sample MSE is well-defined. All simulations are carried out over a grid of values for $(\gamma, \rho)$ with $10,000$ replications at each point. Estimation is by 2SLS without a constant term, using the expressions from Section \ref{sec:chooseIVFMSC}.

Table \ref{tab:trueRMSE} gives the difference in small-sample root mean squared error (RMSE) between the full and valid estimators for a sample size of 500. Negative values indicate parameter values at which the full instrument set has a lower RMSE. We see that even if $Cov(w,u)\neq 0$, so that $w$ is invalid, including it in the instrument set can dramatically lower RMSE provided that $Cov(w,x)$ is high. In other words, using an invalid but sufficiently relevant instrument can improve our estimates. Because a sample size of 500 effectively divides the parameter space into two halves, one where the full estimator has the advantage and one where the valid estimator does, I concentrate on this case. Summary results for smaller sample sizes appear in Table \ref{tab:summary}. (Details for sample sizes of 50 and 100 are available upon request.)
\begin{table}[!tbp]
\caption{Difference in RMSE between full and valid estimators.}

\label{tab:trueRMSE}

 \begin{center}
 \small
 \begin{tabular}{r|rrrrrrrrr}\hline\hline
&\multicolumn{9}{c}{$\rho = Cov(w,u)$}\\
\multicolumn{1}{c|}{$N = 500$}&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0.05}&\multicolumn{1}{c}{0.10}&\multicolumn{1}{c}{0.15}&\multicolumn{1}{c}{0.20}&\multicolumn{1}{c}{0.25}&\multicolumn{1}{c}{0.30}&\multicolumn{1}{c}{0.35}&\multicolumn{1}{c}{0.40}\tabularnewline
\hline
0.0&-0.01& 0.00& 0.02& 0.07& 0.13& 0.18& 0.25& 0.31&0.39\tabularnewline
0.1&-0.06& 0.00& 0.09& 0.19& 0.30& 0.42& 0.53& 0.65&0.79\tabularnewline
0.2&-0.10&-0.04& 0.07& 0.19& 0.32& 0.46& 0.58& 0.72&0.86\tabularnewline
0.3&-0.14&-0.09& 0.01& 0.12& 0.24& 0.36& 0.48& 0.61&0.72\tabularnewline
0.4&-0.17&-0.12&-0.03& 0.06& 0.16& 0.26& 0.36& 0.46&0.57\tabularnewline
0.5&-0.19&-0.15&-0.07& 0.01& 0.10& 0.19& 0.27& 0.34&0.45\tabularnewline
0.6&-0.20&-0.17&-0.10&-0.03& 0.04& 0.11& 0.19& 0.26&0.34\tabularnewline
\multirow{4}{5mm}{\begin{sideways}\parbox{1mm}{$\gamma\;$=$\;Cov(w,x)$}\end{sideways}}
0.7&-0.21&-0.18&-0.13&-0.07&-0.01& 0.07& 0.14& 0.20&0.26\tabularnewline
0.8&-0.22&-0.20&-0.15&-0.09&-0.04& 0.03& 0.09& 0.15&0.20\tabularnewline
0.9&-0.23&-0.21&-0.16&-0.12&-0.07&-0.01& 0.04& 0.10&0.14\tabularnewline
1.0&-0.25&-0.22&-0.19&-0.13&-0.08&-0.04& 0.01& 0.06&0.11\tabularnewline
1.1&-0.24&-0.22&-0.20&-0.16&-0.10&-0.07&-0.02& 0.03&0.07\tabularnewline
1.2&-0.26&-0.22&-0.19&-0.16&-0.12&-0.07&-0.05&-0.01&0.03\tabularnewline
1.3&-0.29&-0.24&-0.20&-0.17&-0.14&-0.09&-0.06&-0.01&0.02\tabularnewline
\hline
\end{tabular}

\end{center}
\footnotesize
\begin{tablenotes}
	\item Negative values indicate that including $w$ gives a smaller RMSE. Results are calculated by simulating from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications.
\end{tablenotes}
\end{table}

The FMSC chooses moment conditions to minimize an asymptotic approximation to small-sample MSE in the hope that this will provide reasonable performance in practice. The first question is how often the FMSC succeeds in identifying the instrument set that minimizes small sample MSE. Table \ref{tab:correctFMSC} gives the frequency of correct decisions made by the FMSC in percentage points for a sample size of 500. A correct decision is defined as an instance in which the FMSC selects the moment set that minimizes finite-sample MSE as indicated by Table \ref{tab:trueRMSE}. We see that the FMSC performs best when there are large differences in MSE between the full and valid estimators: in the top right and bottom left of the parameter space. The criterion performs less well in the borderline cases along the main diagonal.
% latex.default(correct.FMSC, file = outfilename, greek = TRUE,      numeric.dollar = FALSE, na.blank = TRUE, landscape = FALSE,      rowname = NULL, append = TRUE) 
%
\begin{table}[!tbp]
\caption{Correct decision rates for the FMSC in percentage points.}
\label{tab:correctFMSC}
\small
 \begin{center}
 \begin{tabular}{r|rrrrrrrrr}\hline\hline
&\multicolumn{9}{c}{$\rho = Cov(w,u)$}\\
\multicolumn{1}{c|}{$N = 500$}&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0.05}&\multicolumn{1}{c}{0.10}&\multicolumn{1}{c}{0.15}&\multicolumn{1}{c}{0.20}&\multicolumn{1}{c}{0.25}&\multicolumn{1}{c}{0.30}&\multicolumn{1}{c}{0.35}&\multicolumn{1}{c}{0.40}\tabularnewline
\hline
0.0&79&61&69&85&91&94& 94& 95& 96\tabularnewline
0.1&82&25&62&91&98&99& 99&100&100\tabularnewline
0.2&84&82&46&80&96&99&100&100&100\tabularnewline
0.3&85&85&31&60&82&94& 98& 99&100\tabularnewline
0.4&84&86&77&42&65&82& 92& 96& 98\tabularnewline
0.5&84&87&82&31&49&68& 81& 90& 95\tabularnewline
0.6&84&88&84&75&38&54& 68& 80& 87\tabularnewline
\multirow{4}{5mm}{\begin{sideways}\parbox{1mm}{$\gamma\;$=$\;Cov(w,x)$}\end{sideways}}
0.7&85&87&86&80&69&44& 57& 69& 79\tabularnewline
0.8&84&87&86&82&74&36& 48& 60& 71\tabularnewline
0.9&85&87&87&84&78&69& 41& 52& 61\tabularnewline
1.0&85&88&87&85&79&74& 35& 45& 53\tabularnewline
1.1&85&88&88&86&82&76& 68& 39& 48\tabularnewline
1.2&85&88&88&87&84&79& 72& 65& 43\tabularnewline
1.3&86&87&88&88&84&80& 75& 69& 39\tabularnewline
\hline
\end{tabular}
\begin{tablenotes}
\footnotesize
\item A correct decision is an instance in which the FMSC identifies the estimator that minimizes small sample MSE (see Table \ref{tab:trueRMSE}). Values are calculated by simulating from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications.
\end{tablenotes}
\end{center}

\end{table}

Ultimately, the goal of the FMSC is to produce estimators with low MSE. Because the FMSC is itself random, however, using it introduces an additional source of variation. Table \ref{tab:rmseFMSC} accounts for this fact by presenting the RMSE that results from using the estimator chosen by the FMSC. Because these values are difficult to interpret on their own, Tables \ref{tab:rmsevsVALID} and \ref{tab:rmsevsFULL} compare the realized RMSE of the FMSC to those of the valid and full estimators. Negative values indicate that the RMSE of the FMSC is lower. As we see from Table \ref{tab:rmsevsVALID}, the valid estimator outperforms the FMSC in the upper right region of the parameter space, the region where the valid estimator has a lower RMSE than the full. This is because the FMSC sometimes chooses the wrong instrument set, as indicated by Table \ref{tab:correctFMSC}. Accordingly, the FMSC performs substantially better in the bottom left of the parameter space, the region where the full estimator has a lower RMSE than the valid. Taken on the whole, however, the potential advantage of using the valid estimator is small: at best it yields an RMSE $0.06$ smaller than that of the FMSC. Indeed, many of the values in the top right of the parameter space are zero, indicating that the FMSC performs no worse than the valid estimator. In contrast, the potential advantage of using the FMSC is large: it can yield an RMSE $0.16$ smaller than the valid model. The situation is similar for the full estimator only in reverse, as shown in Table \ref{tab:rmsevsFULL}. The full estimator outperforms the FMSC in the bottom left of the parameter space, while the FMSC outperforms the full estimator in the top right. Again, the potential gains from using the FMSC are large compared to those of the full instrument set: a $0.86$ reduction in RMSE versus a $0.14$ reduction. Average and worst-case RMSE comparisons between the FMSC and the full and valid estimators appear in Table \ref{tab:summary}.
% latex.default(rmse.FMSC, file = outfilename, greek = TRUE, numeric.dollar = FALSE,      na.blank = TRUE, landscape = FALSE, rowname = NULL, append = TRUE) 
%
\begin{table}[!tbp]
\caption{RMSE of the estimator selected by the FMSC.}
\label{tab:rmseFMSC}
\small
 \begin{center}
 \begin{tabular}{r|rrrrrrrrr}\hline\hline
&\multicolumn{9}{c}{$\rho = Cov(w,u)$}\\
\multicolumn{1}{c|}{$N = 500$}&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0.05}&\multicolumn{1}{c}{0.10}&\multicolumn{1}{c}{0.15}&\multicolumn{1}{c}{0.20}&\multicolumn{1}{c}{0.25}&\multicolumn{1}{c}{0.30}&\multicolumn{1}{c}{0.35}&\multicolumn{1}{c}{0.40}\tabularnewline
\hline
0.0&0.26&0.27&0.27&0.27&0.27&0.27&0.27&0.27&0.27\tabularnewline
0.1&0.24&0.26&0.28&0.27&0.27&0.27&0.27&0.27&0.27\tabularnewline
0.2&0.22&0.25&0.30&0.31&0.28&0.27&0.28&0.27&0.27\tabularnewline
0.3&0.20&0.23&0.29&0.32&0.31&0.29&0.28&0.27&0.28\tabularnewline
0.4&0.20&0.22&0.27&0.31&0.32&0.31&0.30&0.30&0.28\tabularnewline
0.5&0.20&0.20&0.25&0.29&0.32&0.32&0.32&0.31&0.29\tabularnewline
0.6&0.19&0.19&0.23&0.27&0.30&0.33&0.33&0.32&0.31\tabularnewline
\multirow{4}{5mm}{\begin{sideways}\parbox{1mm}{$\gamma\;$=$\;Cov(w,x)$}\end{sideways}}
0.7&0.18&0.19&0.22&0.25&0.28&0.31&0.32&0.33&0.32\tabularnewline
0.8&0.18&0.19&0.21&0.24&0.27&0.30&0.31&0.32&0.32\tabularnewline
0.9&0.18&0.19&0.20&0.23&0.26&0.28&0.30&0.32&0.33\tabularnewline
1.0&0.18&0.18&0.19&0.22&0.25&0.27&0.29&0.30&0.32\tabularnewline
1.1&0.17&0.17&0.19&0.21&0.23&0.25&0.28&0.29&0.31\tabularnewline
1.2&0.17&0.17&0.18&0.20&0.22&0.24&0.26&0.28&0.29\tabularnewline
1.3&0.17&0.17&0.17&0.19&0.21&0.23&0.25&0.27&0.28\tabularnewline
\hline
\end{tabular}
\end{center}
\footnotesize
\begin{tablenotes}
 \item Values are calculated by simulating from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications..
\end{tablenotes}
\end{table}




% latex.default(rmse.vs.valid, file = outfilename, greek = TRUE,      numeric.dollar = FALSE, na.blank = TRUE, landscape = FALSE,      rowname = NULL, append = TRUE) 
%
\begin{table}[!tbp]
\caption{Difference in RMSE between FMSC and valid estimator.}
\label{tab:rmsevsVALID}
\small
 \begin{center}
 \begin{tabular}{r|rrrrrrrrr}\hline\hline
&\multicolumn{9}{c}{$\rho = Cov(w,u)$}\\
\multicolumn{1}{c|}{$N = 500$}&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0.05}&\multicolumn{1}{c}{0.10}&\multicolumn{1}{c}{0.15}&\multicolumn{1}{c}{0.20}&\multicolumn{1}{c}{0.25}&\multicolumn{1}{c}{0.30}&\multicolumn{1}{c}{0.35}&\multicolumn{1}{c}{0.40}\tabularnewline
\hline
0.0&-0.01&-0.01&-0.01& 0.00& 0.00& 0.00& 0.00& 0.00&0.00\tabularnewline
0.1&-0.04&-0.01& 0.01& 0.01& 0.00& 0.00& 0.00& 0.00&0.00\tabularnewline
0.2&-0.05&-0.02& 0.03& 0.03& 0.00& 0.00& 0.00& 0.00&0.00\tabularnewline
0.3&-0.07&-0.04& 0.02& 0.04& 0.04& 0.01& 0.01& 0.00&0.00\tabularnewline
0.4&-0.08&-0.05& 0.00& 0.04& 0.05& 0.04& 0.03& 0.02&0.01\tabularnewline
0.5&-0.08&-0.07&-0.02& 0.02& 0.05& 0.06& 0.05& 0.02&0.02\tabularnewline
0.6&-0.09&-0.08&-0.04& 0.00& 0.03& 0.04& 0.05& 0.04&0.04\tabularnewline
\multirow{4}{5mm}{\begin{sideways}\parbox{1mm}{$\gamma\;$=$\;Cov(w,x)$}\end{sideways}}
0.7&-0.09&-0.08&-0.06&-0.03& 0.00& 0.04& 0.05& 0.06&0.05\tabularnewline
0.8&-0.10&-0.09&-0.07&-0.03&-0.01& 0.02& 0.04& 0.05&0.04\tabularnewline
0.9&-0.10&-0.09&-0.08&-0.06&-0.03& 0.00& 0.02& 0.04&0.04\tabularnewline
1.0&-0.12&-0.11&-0.10&-0.06&-0.04&-0.02& 0.00& 0.02&0.04\tabularnewline
1.1&-0.11&-0.11&-0.11&-0.09&-0.05&-0.04&-0.02& 0.01&0.02\tabularnewline
1.2&-0.13&-0.11&-0.11&-0.09&-0.07&-0.04&-0.04&-0.01&0.00\tabularnewline
1.3&-0.16&-0.12&-0.11&-0.10&-0.09&-0.05&-0.04&-0.01&0.00\tabularnewline
\hline
\end{tabular}
\end{center}
\footnotesize
\begin{tablenotes}
\item Negative values indicate that the FMSC gives a lower realized RMSE. Results are calculated by simulating from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications.
\end{tablenotes}
\end{table}

% latex.default(rmse.vs.full, file = outfilename, greek = TRUE,      numeric.dollar = FALSE, na.blank = TRUE, landscape = FALSE,      rowname = NULL, append = TRUE) 
%
\begin{table}[!tbp]
\caption{Difference in RMSE between FMSC and full estimator.}
\label{tab:rmsevsFULL}
\small
 \begin{center}
 \begin{tabular}{r|rrrrrrrrr}\hline\hline
&\multicolumn{9}{c}{$\rho = Cov(w,u)$}\\
\multicolumn{1}{c|}{$N = 500$}&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0.05}&\multicolumn{1}{c}{0.10}&\multicolumn{1}{c}{0.15}&\multicolumn{1}{c}{0.20}&\multicolumn{1}{c}{0.25}&\multicolumn{1}{c}{0.30}&\multicolumn{1}{c}{0.35}&\multicolumn{1}{c}{0.40}\tabularnewline
\hline
0.0&0.00&-0.01&-0.03&-0.07&-0.13&-0.18&-0.25&-0.31&-0.39\tabularnewline
0.1&0.02&-0.01&-0.07&-0.18&-0.30&-0.42&-0.53&-0.65&-0.78\tabularnewline
0.2&0.05& 0.02&-0.04&-0.16&-0.31&-0.46&-0.58&-0.72&-0.86\tabularnewline
0.3&0.07& 0.05& 0.01&-0.08&-0.20&-0.34&-0.47&-0.61&-0.71\tabularnewline
0.4&0.09& 0.07& 0.03&-0.02&-0.11&-0.22&-0.33&-0.44&-0.56\tabularnewline
0.5&0.11& 0.08& 0.05& 0.01&-0.05&-0.13&-0.22&-0.32&-0.42\tabularnewline
0.6&0.11& 0.09& 0.07& 0.03&-0.01&-0.06&-0.14&-0.22&-0.30\tabularnewline
\multirow{4}{5mm}{\begin{sideways}\parbox{1mm}{$\gamma\;$=$\;Cov(w,x)$}\end{sideways}}
0.7&0.12& 0.10& 0.07& 0.04& 0.01&-0.03&-0.08&-0.14&-0.22\tabularnewline
0.8&0.13& 0.11& 0.08& 0.05& 0.03& 0.00&-0.05&-0.10&-0.15\tabularnewline
0.9&0.13& 0.11& 0.08& 0.06& 0.04& 0.01&-0.02&-0.06&-0.10\tabularnewline
1.0&0.13& 0.11& 0.09& 0.07& 0.05& 0.02&-0.01&-0.04&-0.07\tabularnewline
1.1&0.13& 0.11& 0.09& 0.07& 0.05& 0.03& 0.01&-0.02&-0.05\tabularnewline
1.2&0.14& 0.11& 0.09& 0.07& 0.05& 0.03& 0.02& 0.00&-0.03\tabularnewline
1.3&0.13& 0.12& 0.09& 0.07& 0.05& 0.04& 0.02& 0.00&-0.02\tabularnewline
\hline
\end{tabular}
\end{center}
\footnotesize
\begin{tablenotes}
\item Negative values indicate that the FMSC gives a lower realized RMSE. Results are calculated by simulating from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications.
\end{tablenotes}
\end{table}

I now compare the FMSC to a number of alternative procedures from the literature. \cite{Andrews1999} considers a family of moment selection critera that take the form $MSC(S) = J_n(S) - h(|S|)\kappa_n$, where $J_n(S)$ is the $J$-test statistic under moment set $S$ and we choose the moment set that \emph{minimizes} the criterion. If we take $h(|S|) = (p + |S| - r)$, then $\kappa_n = \log{n}$ gives a GMM analogue of Schwarz's Bayesian Information Criterion (GMM-BIC) while $\kappa_n = 2.01 \log{\log{n}}$ gives an analogue of the Hannan-Quinn Information Criterion (GMM-HQ), and $\kappa_n = 2$ gives an analogue of Akaike's Information Criterion (GMM-AIC). Under certain assumptions, the HQ and BIC-type criteria are consistent: they select any and all valid moment conditions with probability approaching one in the limit (w.p.a.1). When calculating the $J$-test statistic under potential mis-specification, Andrews recommends using a centered covariance matrix estimator and basing estimation on the weighting matrix that would be efficient under the assumption of correct specification. Accordingly, I calculate
	\begin{eqnarray}
		J_{Full} &=&n^{-1}\; u( \widehat{\theta}_{f})'\;Z \; \widehat{\Omega}^{-1} \;Z' \;u( \widehat{\theta}_{f})\\
		J_{Valid} &=&n^{-1}\; u( \widehat{\theta}_{v})'\;Z_1 \;\widetilde{\Omega}_{11}^{-1} \;Z_1'\;u( \widehat{\theta}_{v})
	\end{eqnarray}
for the full and valid instrument sets using the formulas from Section \ref{sec:chooseIVFMSC}. 

Because the Andrews-type criteria only take account of instrument validity, not relevance, \cite{HallPeixe2003} suggest combining them with their canonical correlations information criterion (CCIC). The CCIC aims to detect and eliminate redundant instruments, those that add no further information beyond that contained in the other instruments. While including such instruments has no effect on the asymptotic distribution of the estimator, it could lead to poor finite-sample performance. By combining the CCIC with an Andrews-type criterion, the idea is to eliminate invalid instruments and then redundant ones. For the present simulation example, with a single endogenous regressor and no constant term, 
	\begin{equation}
	\mbox{CCIC}(S) = n \log\left[1 - R_n^2(S) \right] + h(p + |S|)\kappa_n
	\end{equation}
where $R_n^2(S)$ is the first-stage $R^2$ based on instrument set $S$ and $h(p + |S|)\mu_n$ is a penalty term \citep{Jana2005}. If we take $h(p + |S|) = (p + |S| - r)$, setting $\kappa_n = \log{n}$ gives the CCIC-BIC, while $\kappa_n = 2.01 \log{\log{n}}$ gives the CCIC-HQ  and $\kappa_n = 2$ gives the CCIC-AIC. I consider procedures that combine CCIC criteria with the \emph{corresponding} criterion of \cite{Andrews1999}. For example, CC-MSC-BIC is shorthand for the rule ``include $w$ iff it minimizes both  GMM-BIC \emph{and} CCIC-BIC.'' I define CC-MSC-AIC and CC-MSC-HQ analogously.

A less formal but fairly common procedure for moment selection in practice is the downward $J$-test. In the present context this takes a particularly simple form: if the $J$-test fails to reject the null hypothesis of correct specification for the full instrument set, use this set for estimation; otherwise, use the valid instrument set. In addition to the moment selection criteria given above, I compare the FMSC to selection by a downward $J$-test at the 90\% and 95\% significance levels. 

Table \ref{tab:summary} compares average and worst-case RMSE over the parameter space given in Table \ref{tab:trueRMSE} for sample sizes of 50, 100, and 500 observations. (Pointwise RMSE comparisons are available upon request.) For each sample size the FMSC outperforms all other moment selection procedures in both average and worst-case RMSE. The gains are particularly large for smaller sample sizes. The results given here suggest that the FMSC may be of considerable value for instrument selection in practice.

% latex.default(out.table, file = outfilename, greek = TRUE, numeric.dollar = FALSE,      na.blank = TRUE, landscape = FALSE, rowname = row.names(out.table),      append = FALSE) 
%
\begin{table}[!tbp]
\caption{Summary of Simulation Results.}
\label{tab:summary}
 \begin{center}
 \begin{tabular}{rlrrr}\hline\hline
\multicolumn{2}{l}{Average RMSE}&\multicolumn{1}{c}{$N=50$}&\multicolumn{1}{c}{$N=100$}&\multicolumn{1}{c}{$N=500$}\tabularnewline
\hline
&Valid Estimator&0.69&0.59&0.28\tabularnewline
&Full Estimator&0.44&0.40&0.34\tabularnewline
&FMSC&0.47&0.41&0.26\tabularnewline
&GMM-BIC&0.61&0.52&0.29\tabularnewline
&GMM-HQ&0.64&0.56&0.29\tabularnewline
&GMM-AIC&0.67&0.58&0.28\tabularnewline
&Downward J-test 90\%&0.55&0.50&0.28\tabularnewline
&Downward J-test 95\%&0.51&0.47&0.28\tabularnewline
&CC-MSC-BIC&0.61&0.51&0.28\tabularnewline
&CC-MSC-HQ&0.64&0.55&0.28\tabularnewline
&CC-MSC-AIC&0.66&0.57&0.28\tabularnewline
\hline\hline
\multicolumn{2}{l}{Worst-case RMSE}&\multicolumn{1}{c}{$N=50$}&\multicolumn{1}{c}{$N=100$}&\multicolumn{1}{c}{$N=500$}\tabularnewline
\hline
&Valid Estimator&0.84&1.06&0.32\tabularnewline
&Full Estimator&1.04&1.12&1.14\tabularnewline
&FMSC&0.81&0.74&0.33\tabularnewline
&GMM-BIC&0.99&0.99&0.47\tabularnewline
&GMM-HQ&0.97&1.03&0.39\tabularnewline
&GMM-AIC&0.95&1.04&0.35\tabularnewline
&Downward J-test 90\%&0.99&0.98&0.41\tabularnewline
&Downward J-test 95\%&1.01&1.00&0.46\tabularnewline
&CC-MSC-BIC&0.86&0.99&0.47\tabularnewline
&CC-MSC-HQ&0.87&1.03&0.39\tabularnewline
&CC-MSC-AIC&0.87&1.04&0.35\tabularnewline
\hline
\end{tabular}
\end{center}
\footnotesize
\begin{tablenotes}
	\item Average and worst-case RMSE are calculated over the simulation grid from Table \ref{tab:trueRMSE}. All values are calculated by simulating from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications at each point on the grid.
\end{tablenotes}
\end{table}







\section{Moment Averaging \& Post-Selection Estimators}
\label{sec:avg}
In the preceding section we derived the FMSC as an asymptotically unbiased estimator of the AMSE of a candidate estimator.
Besides presenting simulation results, however, we have thus far said nothing about the sampling properties of the FMSC selection procedure itself.
Because it is constructed from $\widehat{\tau}$ the FMSC is a random variable, even in the limit.
Combining Corollary \ref{cor:tautau} with Equation \ref{eq:fmsc} gives the following.
\begin{cor}[Limit Distribution of FMSC]
\label{cor:FMSClimit}
	Under Assumptions \ref{assump:drift}, \ref{assump:highlevel} and \ref{assump:Identification}, $FMSC_n(S) \rightarrow_d FMSC_S(\tau, M)$, where
	\begin{eqnarray*}
		\mbox{FMSC}_S(\tau,M) &=& \nabla_\theta\mu(\theta_0)'K_S\Xi_S \left\{\left[\begin{array}{cc}0&0\\0& B(\tau,M) \end{array}\right] + \Omega\right\}\Xi_S'K_S'\nabla_\theta\mu(\theta_0)\\
		B(\tau,M) &=& (\Psi M + \tau)(\Psi M + \tau)' - \Psi \Omega \Psi.
	\end{eqnarray*}
\end{cor}


Because the FMSC is itself random, so is the FMSC-selected estimator.
This means that the FMSC is a ``conservative'' rather than ``consistent'' selection procedure.
While this lack of consistency may sound like a ``bug'' it is in fact a desirable feature of the FMSC for two reasons.
First, as discussed above, the goal of the FMSC is not to consistently select the correct moment conditions: it is to choose an estimator with a low finite-sample MSE as approximated by AMSE.
In fact, the goal of consistent selection is very much at odds with that of controlling estimator risk.
As explained by \cite{Yang2005} and \cite{LeebPoetscher2008}, the worst-case risk of a consistent selection procedure \emph{diverges} with sample size. 

Second, while we know from simulation studies that selection can dramatically change the sampling distribution of our estimators, the asymptotics of consistent selection give the misleading impression that this effect can be ignored.
For example Lemma 1.1 of \citet[p.\ 168]{Poetscher1991}, which states that the limit distributions of an estimator pre- and post-consistent selection are identical, has been interpreted by some as evidence that consistent selection is innocuous. 
\citet[pp.\ 179--180]{Poetscher1991} makes it very clear, however, this result does not hold uniformly in the parameter space and hence ``only creates an illusion of conducting valid inference'' \citep[p.\ 22]{LeebPoetscher2005}.
Figure \ref{fig:consist} illustrates this problem using the simulation experiment from Section \ref{sec:fmscsim} with a sample size of $500$ and $\gamma = 0.4$, $\rho= 0.2$.
At these parameter values, $w$ is an invalid instrument.
Because they are consistent criteria, and hence will exclude any invalid instruments in the limit, a na\"{i}ve reading of P\"{o}tscher's Lemma 1.1 would suggest that the post-selection distributions of GMM-BIC and HQ should be close to that of the valid estimator, given in dashed lines.
This is emphatically not the case: both post-selection distributions are highly non-normal mixtures. 
\begin{figure}[htbp]
\begin{center}
	\includegraphics[scale = 0.48]{GMM_BIC}
	\includegraphics[scale = 0.48]{GMM_HQ}
\caption{ \small Post-selection distributions for the estimated effect of $x$ on $y$ in Equation \ref{eq:secondstage} with $\gamma = 0.4$, $\rho = 0.2$, $N=500$. The distribution post-GMM-BIC selection appears in the top panel, while the distribution post-GMM-HQ selection appears in the bottom panel. The distribution of the full estimator is given in dotted lines while that of the valid estimator is given in dashed lines in each panel. All distributions are calculated by kernel density estimation based on 10,000 simulation replications generated from Equations \ref{eq:secondstage}--\ref{eq:varmatrix}.}
\label{fig:consist}
\end{center}
\end{figure}
While Figure \ref{fig:consist} examines only one point in the parameter space the problem is more general, as shown by Table \ref{tab:BICcov}. 
The empirical coverage probabilities of traditional $95\%$ confidence intervals are far lower than their nominal level over the majority of the parameter space and the lack of uniformity is striking: small changes in parameters lead to large changes in coverage.

% latex.default(cover.BIC, file = outfilename, greek = TRUE, numeric.dollar = FALSE,      na.blank = TRUE, landscape = FALSE, rowname = NULL, append = TRUE) 
%
\begin{table}[!tbp]
\caption{Coverage post-GMM-BIC moment selection (nominal 95\%).}
\label{tab:BICcov}
\small
 \begin{center}
 \begin{tabular}{r|rrrrrrrrr}\hline\hline
&\multicolumn{9}{c}{$\rho = Cov(w,u)$}\\
\multicolumn{1}{c|}{$N = 500$}&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0.05}&\multicolumn{1}{c}{0.10}&\multicolumn{1}{c}{0.15}&\multicolumn{1}{c}{0.20}&\multicolumn{1}{c}{0.25}&\multicolumn{1}{c}{0.30}&\multicolumn{1}{c}{0.35}&\multicolumn{1}{c}{0.40}\tabularnewline
\hline
0.0&0.92&0.92&0.92&0.93&0.92&0.92&0.92&0.92&0.93\tabularnewline
0.1&0.92&0.83&0.77&0.83&0.90&0.92&0.93&0.92&0.92\tabularnewline
0.2&0.93&0.76&0.55&0.57&0.74&0.86&0.89&0.90&0.91\tabularnewline
0.3&0.93&0.75&0.45&0.35&0.50&0.69&0.80&0.85&0.88\tabularnewline
0.4&0.93&0.75&0.40&0.22&0.31&0.48&0.63&0.74&0.80\tabularnewline
0.5&0.93&0.75&0.38&0.18&0.20&0.32&0.46&0.59&0.68\tabularnewline
0.6&0.94&0.76&0.38&0.14&0.14&0.23&0.32&0.43&0.53\tabularnewline
\multirow{4}{5mm}{\begin{sideways}\parbox{1mm}{$\gamma\;$=$\;Cov(w,x)$}\end{sideways}}
0.7&0.94&0.76&0.37&0.12&0.11&0.16&0.24&0.32&0.42\tabularnewline
0.8&0.93&0.76&0.37&0.11&0.08&0.12&0.18&0.25&0.33\tabularnewline
0.9&0.94&0.75&0.37&0.11&0.07&0.10&0.14&0.19&0.25\tabularnewline
1.0&0.93&0.76&0.37&0.10&0.06&0.08&0.11&0.16&0.20\tabularnewline
1.1&0.93&0.77&0.37&0.10&0.06&0.07&0.10&0.13&0.16\tabularnewline
1.2&0.94&0.77&0.38&0.10&0.05&0.06&0.08&0.11&0.14\tabularnewline
1.3&0.94&0.77&0.38&0.10&0.04&0.05&0.07&0.09&0.12\tabularnewline
\hline
\end{tabular}
\end{center}
\footnotesize
\begin{tablenotes}
\item Values are calculated by simulating from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications.
\end{tablenotes}
\end{table}

In constrast to those of consistent selection, the asymptotics of \emph{conservative} selection  under local mis-specification provide a far more accurate picture of the distribution of post-selection estimators.
The point is \emph{not} that conservative criteria -- such as the FMSC, GMM-AIC and $J$-test at a fixed significance level -- are immune to the effects of selection on inference.
%The coverage probabilities for post-FMSC estimators given in Table \ref{tab:FMSCconf}, for example, show that this is not the case.
Rather, it is that conservative criteria can be studied in a framework that allows us to capture the non-normality that is so apparent from Figure \ref{fig:consist} in our limit theory.  
To this end, the present section derives the asymptotic distribution of generic ``moment average'' estimators by extending the idea behind the frequentist model average estimators of \cite{HjortClaeskens}. 
Such estimators are interesting in their own right and include, as a special case, a variety of post-conservative moment selection estimators including the FMSC.
Although their limit distributions are complicated, it remains possible to construct asymptotically valid confidence intervals for moment average estimators using a two-step, simulation-based procedure.
We begin by defining moment average estimators in general and considering some examples before presenting the procedure for constructing valid confidence intervals.

\subsection{Moment Average Estimators}
A generic moment average estimator takes the form
\begin{equation}
	\label{eq:avg}
	\widehat{\mu}=\sum_{S \in \mathscr{S}} \widehat{\omega}_S\widehat{\mu}_S
\end{equation}
where $\widehat{\mu}_S = \mu(\widehat{\theta}_S)$ is the estimator of the target parameter $\mu$ under moment set $S$, $\mathscr{S}$ is the collection of all moment sets under consideration, and $\widehat{\omega}_S$ is shorthand for the value of a data-dependent weight function  $\widehat{\omega}_S=\omega(\cdot, \cdot)$ evaluated at moment set $S$ and the sample observations $Z_{n1}, \hdots, Z_{nn}$.  
As above $\mu(\cdot)$ is a $\mathbb{R}$-valued, $Z$-almost surely continuous function of $\theta$ that is differentiable in an open neighborhood of $\theta_0$. When $\widehat{\omega}_S$ is an indicator, taking on the value one at the moment set moment set that minimizes some moment selection criterion, $\widehat{\mu}$ is a post-moment selection estimator. To characterize the limit distribution of $\widehat{\mu}$, we impose the following conditions on $\widehat{\omega}_S$.
\begin{assump}[Conditions on the Weights]\mbox{}
\label{assump:weights}
\begin{enumerate}[(a)]
	\item $\sum_{S \in \mathscr{S}} \widehat{\omega}_S = 1$, almost surely 
	\item For each $S\in \mathscr{S}$, $\widehat{\omega}_S \rightarrow_d\varphi_S(\tau, M)$, an almost-surely continuous function of $\tau$, $M$ and consistently estimable constants only.
\end{enumerate}
\end{assump}

\begin{cor}[Asymptotic Distribution of Moment-Average Estimators]
\label{cor:momentavg}
Under Assumption \ref{assump:weights} and the conditions of Theorem \ref{thm:normality},
	$$\sqrt{n}\left(\widehat{\mu} -  \mu_0\right) \rightarrow_{d}\Lambda(\tau) =  -\nabla_\theta\mu(\theta_0)'\left[\sum_{S \in \mathscr{S}} \varphi_S(\tau,M) K_S\Xi_S\right] \left(M + \left[\begin{array}
	{c} 0 \\ \tau
\end{array} \right]\right).$$
\end{cor}
Notice that the limit random variable from Corollary \ref{cor:momentavg}, denoted $\Lambda(\tau)$, is a \emph{randomly weighted average} of the multivariate normal vector $M$. 
Hence, $\Lambda(\tau)$ is non-normal. 
This is precisely the behavior that Figure \ref{fig:consist} suggests our limit theory should capture.
The conditions of Assumption \ref{assump:weights} are fairly mild. 
Requiring that the weights sum to one ensures that $\widehat{\mu}$ is a consistent estimator of $\mu_0$ and leads to a simpler expression for the limit distribution. 
While somewhat less transparent, the second condition is satisfied by weighting schemes based on a number of familiar moment selection criteria.
We see immediately from Corollary \ref{cor:FMSClimit}, for example, that the FMSC converges in distribution to a function of $\tau$, $M$ and consistently estimable constants only. 
The same ss true for the $J$-test statistic, as we see from the following result. 
\begin{thm}[Distribution of $J$-Statistic under Local Mis-Specification] 
\label{pro:jstat}
	Define the J-test statistic as per usual by $J_n(S)  = n \left[\Xi_S f_n(\widehat{\theta}_S)\right]' \widehat{\Omega}^{-1}\left[\Xi_S f_n(\widehat{\theta}_S)\right]$ where $\widehat{\Omega}^{-1}_S$ is a consistent estimator of $\Omega_S^{-1}$. Then, under the conditions of Theorem \ref{thm:normality}, we have $J_n(S) \rightarrow_dJ_S(\tau, M)$ where
		$$J_S(\tau, M)=[\Omega_S^{-1/2}(M_S + \tau_S)]' (I - P_S)[\Omega_S^{-1/2}\Xi_S(M_S + \tau_S)],$$
$M_S = \Xi_S M$, $\tau_S' = (0', \tau')\Xi_S'$, and $P_S$ is the projection matrix formed from the GMM identifying restrictions $\Omega^{-1/2}_S F_S$.
\end{thm}
Hence, normalized weights constructed from almost-surely continuous functions of either the FMSC or the $J$-test statistic satisfy Assumption \ref{assump:weights}. 

Post-selection estimators are merely a special cases of moment average estimators.
To see why, consider the weight function
$$\widehat{\omega}_S^{MSC} = \mathbf{1}\left\{\mbox{MSC}_n(S) = \min_{S'\in \mathscr{S}} \mbox{MSC}_n(S')\right\}$$where $\mbox{MSC}_n(S)$ is the value of some moment selection criterion evaluated at the sample observations $Z_{n1}\hdots, Z_{nn}$. 
Now suppose $\mbox{MSC}_n(S) \rightarrow_d\mbox{MSC}_S(\tau,M)$, a function of $\tau$, $M$ and consistently estimable constants only. 
Then, so long as the probability of ties, $P\left\{\mbox{MSC}_S(\tau,M) = \mbox{MSC}_{S'}(\tau,M) \right\}$, is zero for all $S\neq S'$, the continuous mapping theorem gives 
	$$\widehat{\omega}_S^{MSC} \rightarrow_d \mathbf{1}\left\{\mbox{MSC}_S(\tau,M) = \min_{S'\in \mathscr{S}} \mbox{MSC}_{S'}(\tau,M)\right\}$$ 
satisfying Assumption \ref{assump:weights} (b). 
Thus, post-selection estimators based on the FMSC, the downward $J$-test procedure, GMM-BIC, GMM-HQ, and GMM-AIC all fall within the ambit of \ref{cor:momentavg}. 
GMM-BIC and GMM-HQ, however, are not particularly interesting under local mis-specification.
Intuitively, because they aim to select all valid moment conditions w.p.a.1, we would expect that under Assumption \ref{assump:drift} they simply choose the full moment set in the limit. 
The following result states that this intuition is correct. 
\begin{thm}[Consistent Criteria under Local Mis-Specification]
\label{pro:andrews}
Consider a moment selection criterion of the form $MSC(S) = J_n(S) - h(|S|)\kappa_n$, where $h$ is strictly increasing,  $\lim_{n\rightarrow \infty}\kappa_n = \infty$, and $\kappa_n = o(n)$. Under the conditions of Theorem \ref{thm:normality}, $MSC(S)$ selects the full moment set with probability approaching one.
\end{thm}
The preceding result is a special case of a more general phenomenon: consistent selection procedures cannot detect model violations of order $O(n^{-1/2})$.
Because moment selection using the GMM-BIC or HQ leads to weights with a degenerate asymptotic distribution, one that does not capture the effects of selection on inference, these criteria are not considered further below. 

\subsection{Moment Averaging Examples}
Although it is a special case of moment averaging, moment selection is a somewhat crude procedure: it gives full weight to the estimator that minimizes the moment selection criterion no matter how close its nearest competitor lies. 
Accordingly, when competing moment sets have similar criterion values in the population, sampling variation can be \emph{magnified} in the selected estimator. 
Thus, it may be possible to achieve better performance by using smooth weights rather than discrete selection.
In this section we explore this possibility via two examples: one based on a simple heursitic and another on more detailed analytical calculations.

\subsubsection{Exponential Weights}
\todo[inline]{Possibly remove this section later.}
In the context of maximum likelihood estimation, \cite{Burnhametal} suggest averaging the estimators resulting from a number of competing models using exponential weights of the form $w_k = \exp(-I_k/2)/\sum_{i=1}^K \exp(-I_i/2)$ where $I_k$ is an information criterion evaluated for model $k$, and $i$ indexes the set of $K$ candidate models. This expression, constructed by an analogy with Bayesian model averaging, gives more weight to models with lower values of the information criterion but non-zero weight to all models. Applying this idea to the moment selection criteria given above, consider
	\begin{equation}	
		\widehat{\omega}_S = \left.\exp\left\{-\frac{\kappa}{2} \mbox{MSC}(S)\right\}\right/\sum_{S' \in \mathscr{S}}\exp\left\{-\frac{\kappa}{2} \mbox{MSC}(S')\right\}
		\label{eq:expweight}
\end{equation}
where MSC$(\cdot)$ is a moment selection criterion and the parameter $\kappa$ varies the uniformity of the weighting. As $\kappa \rightarrow 0$ the weights become more uniform; as $\kappa \rightarrow \infty$ they approach the moment selection procedure given by minimizing the corresponding criterion. Table \ref{tab:avg} compares moment averaging against moment selection by substituting FMSC, GMM-AIC, BIC and HQ into Equation \ref{eq:expweight} using the simulation experiment described in Section \ref{sec:fmscsim}. Calculations are based on 10,000 replications, each with a sample size of 500. For FMSC averaging $\kappa = 1/100$ to account for the fact that the FMSC is generally more variable than criteria based on the $J$-test. Weights for GMM-BIC, HQ, and AIC averaging set $\kappa = 1$. Both in terms of average and worst-case RMSE, moment selection is inferior to moment averaging. The only exception is worst-case RMSE for the FMSC. (Pointwise comparisons are available upon request.) If our goal is estimators with low RMSE, moment averaging may be preferable to moment selection. 


% latex.default(RMSE.average.vs.select, file = outfilename, greek = TRUE,      numeric.dollar = FALSE, na.blank = TRUE, landscape = FALSE,      append = TRUE) 
%
\begin{table}[!tbp]
 \begin{center}
\caption{Average and worst-case RMSE of moment averaging versus selection.}
\label{tab:avg}
\small
 \begin{tabular}{lrr}\hline\hline
\multicolumn{1}{l}{Average RMSE}&\multicolumn{1}{c}{Averaging}&\multicolumn{1}{c}{Selection}\tabularnewline
\hline
FMSC&0.24&0.26\tabularnewline
GMM-BIC&0.26&0.29\tabularnewline
GMM-HQ&0.26&0.29\tabularnewline
GMM-AIC&0.26&0.28\tabularnewline
\hline
\multicolumn{1}{l}{Worst-Case RMSE}&\multicolumn{1}{c}{Averaging}&\multicolumn{1}{c}{Selection}\tabularnewline
\hline
FMSC&0.36&0.33\tabularnewline
GMM-BIC&0.41&0.47\tabularnewline
GMM-HQ&0.36&0.39\tabularnewline
GMM-AIC&0.33&0.35\tabularnewline
\hline
\end{tabular}
\end{center}
\footnotesize
\begin{tablenotes}
	\item  Averaging is based on $\kappa = 1/100$ for FMSC weights and $\kappa = 1$ for all other weights. Values are calculated by simulating from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications at each combination of parameter values from Table \ref{tab:trueRMSE} and a sample size of $500$.
\end{tablenotes}
\end{table}

\subsubsection{Minimum AMSE Weights for OLS versus 2SLS Example}
The preceding example was based on the simple heuristic of ``exponential smoothing.'' 
In some applications, however, it is possible to \emph{analytically} derive weights that minimize AMSE.\footnote{I thank Bruce Hansen for suggesting this idea.} 
The OLS versus 2SLS example from Sections \ref{sec:OLSvsIVlowlevel} and \ref{sec:FMSCforOLSvsIV} is one such case. 

To begin, define an arbitrary weighted average of the OLS and 2SLS estimators from Equations \ref{eq:OLS} and \ref{eq:2SLS} by  
\begin{equation}
	\widetilde{\beta}(\omega) = \omega \widehat{\beta}_{OLS} + (1 - \omega) \widetilde{\beta}_{2SLS}
\end{equation}
where $\omega \in [0,1]$ is the weight given to the OLS estimator.
Since the weights sum to one, we have
\begin{eqnarray*}
	\sqrt{n}\left[\widehat{\beta}(\omega) - \beta \right] &=& \left[ \begin{array}
	{cc} \omega & (1 - \omega)
\end{array}\right] \left[
\begin{array}{c}
  \sqrt{n}(\widehat{\beta}_{OLS} - \beta) \\
  \sqrt{n}(\widetilde{\beta}_{2SLS} - \beta)
\end{array}
\right]\\
& \overset{d}{\rightarrow} & N\left(\mbox{Bias}\left[\widehat{\beta}(\omega)\right], Var\left[\widehat{\beta}(\omega)\right] \right)
\end{eqnarray*}
by Theorem \ref{thm:OLSvsIV}, where
\begin{eqnarray*}
	\mbox{Bias}\left[\widehat{\beta}(\omega)\right] &=& \omega \left( \frac{\tau}{\sigma_x^2} \right) \\
	 Var\left[\widehat{\beta}(\omega)\right] &=&  \frac{\sigma_\epsilon^2}{\sigma_x^2} \left[(2\omega^2 - \omega)\left( \frac{\sigma_x^2}{\gamma^2} - 1\right)+\frac{\sigma_x^2}{\gamma^2} \right]
\end{eqnarray*}
and accordingly
\begin{equation}
	\mbox{AMSE}\left[\widehat{\beta}(\omega) \right] =  \omega^2 \left(\frac{\tau^2}{\sigma_x^4} \right) + (\omega^2 - 2 \omega)\left(\frac{\sigma_\epsilon^2}{\sigma_x^2}\right)\left( \frac{\sigma_x^2}{\gamma^2} - 1\right) + \frac{\sigma_\epsilon^2}{\gamma^2}.
\end{equation}
The preceding is a globally convex function of $\omega$. 
Taking the first order condition and rearranging, we find that the unique global minimizer is
\begin{equation}
\label{eq:AMSEoptimal}
	\omega^* = \underset{\omega \in [0,1]}{\mbox{arg min}}\; \mbox{AMSE}\left[\widehat{\beta}(\omega) \right] 
	=\left[1 + \frac{\tau^2/\sigma_x^4}{\sigma_\epsilon^2(1/\gamma^2 - 1/\sigma_x^2)}\right]^{-1}
\end{equation}
In other words,
$$\omega^* = \left[1 + \frac{\mbox{ABIAS(OLS)}^2}{\mbox{AVAR(2SLS)}-\mbox{AVAR(OLS)}} \right]^{-1}$$

The preceding expression has several important consequences. 
First, since the variance of the 2SLS estimator is always strictly greater than that of the OLS estimator, the optimal value of $\omega$ \emph{cannot} be zero. 
No matter how strong the endogeneity of $x$ as measured by $\tau$, we should always give some weight to the OLS estimator. 
Second, when $\tau = 0$ the optimal value of $\omega$ is one. If $x$ is exogenous, OLS is strictly preferable to 2SLS. 
Third, the optimal weights depend on the strength of the instruments $\mathbf{z}$ as measured by $\gamma$. 
For a given value of $\tau\neq 0$, the stronger the instruments, the less weight we should give to OLS.

Equation \ref{eq:AMSEoptimal} gives the AMSE-optimal weighted average of the OLS and 2SLS estimators. 
To actually use the corresponding moment average estimator in practice, however, we need to estimate the unknowns.
As discussed above in Section \ref{sec:FMSCforOLSvsIV} the usual estimators of $\sigma_x^2$ and $\gamma$ remain consistent under local mis-specification, and the residuals from the 2SLS estimator provide a robust estimator of $\sigma_\epsilon^2$.
As before, the problem is estimating $\tau^2$.
A natural idea is to substitute the asymptotically unbiased estimator that arises from Theorem \ref{thm:tauOLSvsIV}, namely $\widehat{\tau}^2 - \widehat{V}$. 
The problem with this approach is that, while $\tau^2$ is always greater than or equal to zero as is $\widehat{\tau}^2$, the difference $\widehat{\tau}^2 - \widehat{V}$ \emph{can easily be negative}, yielding a \emph{negative} estimate of $\mbox{ABIAS(OLS)}^2$.
To solve this problem, we borrow an idea from the literature on shrinkage estimation and use the \emph{positive part} instead, namely $\max\left\{0, \; \widehat{\tau}^2 - \widehat{V}\right\}$, as in the positive-part James-Stein estimator.
This ensures that our estimator of $\omega^*$ lies inside the interval $[0,1]$.
Accordingly, we define 
\begin{equation}
	\widehat{\beta}^*_{AVG} = \widehat{\omega}^* \widehat{\beta}_{OLS} + (1 - \widehat{\omega}^*)\widetilde{\beta}_{2SLS}
\end{equation}
where
\begin{equation}
\widehat{\omega }^* = \left[1 + \frac{\max \left\{0, \; \left(\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right) \right)/\;\widehat{\sigma}_x^4 \right\}}{\widehat{\sigma}_\epsilon^2 (1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2)}\right]^{-1}
\end{equation}
 
\todo[inline]{Add simulation study. Also talk about how the estimator of the weight isn't asymptotically unbiased and how we might want to try other approaches, a question we leave for future research. This section is meant to illustrate the possibilities for moment averaging.
In the simulations, it works well. How to get a CI for such a procedure? We'll see in the next section.}

\subsection{Valid Confidence Intervals}
While Corollary \ref{cor:momentavg} characterizes the limiting behavior of moment-average, and hence post-selection estimators, the limiting random variable $\Lambda(\tau)$ is a complicated function of the normal random vector $M$. 
Because this distribution is analytically intractable, I adapt a suggestion from \cite{ClaeskensHjortbook} and approximate it by simulation.
The result is a conservative procedure that provides asymptotically valid confidence intervals for moment average and hence post-conservative selection estimators.\footnote{Although I originally developed this procedure by analogy to \cite{ClaeskensHjortbook}, \cite{Leeb} kindly pointed out that constructions of the kind given here have appeared elsewhere in the statistics literature, notably in \cite{Loh1985}, \cite{Berger1994}, and \cite{Silvapulle1996}. 
More recently, \cite{McCloskey} uses a similar approach to study non-standard testing problems.}
 
First, suppose that $K_S$, $\varphi_S$, $\theta_0$, $\Omega$ and $\tau$ were known. 
Then, by simulating from $M$, as defined in Theorem \ref{thm:normality}, the distribution of $\Lambda(\tau)$, defined in Corollary \ref{cor:momentavg}, could be approximated to arbitrary precision. 
To operationalize this procedure, substitute consistent estimators of $K_S$, $\theta_0$, and $\Omega$, e.g.\ those used to calculate FMSC. 
To estimate $\varphi_S$, we first need to derive the limit distribution of $\widehat{\omega}_S$, the data-based weights specified by the user. 
As an example, consider the case of moment selection based on the FMSC. Here $\widehat{\omega}_S$ is simply the indicator function
\begin{equation}
	\label{eq:FMSCindicate}
	\widehat{\omega}_S = \mathbf{1}\left\{\mbox{FMSC}_n(S) = \min_{S'\in \mathscr{S}} \mbox{FMSC}_n(S')\right\}
\end{equation}
To estimate $\varphi_S$, we first substitute consistent estimators of $\Omega$, $K_S$ and $\theta_0$ into $\mbox{FMSC}_S(\tau,M)$, defined in Corollary \ref{cor:FMSClimit}, yielding,
\begin{equation}
	\widehat{\mbox{FMSC}}_S(\tau,M) = \nabla_\theta\mu(\widehat{\theta})'\widehat{K}_S\Xi_S \left\{\left[\begin{array}{cc}0&0\\0&\widehat{\mathcal{B}}(\tau,M) \end{array}\right] + \widehat{\Omega}\right\}\Xi_S'\widehat{K}_S'\nabla_\theta\mu(\widehat{\theta}).
\end{equation}
where
\begin{equation}
	\widehat{\mathcal{B}}(\tau,M) = (\widehat{\Psi} M + \tau)(\widehat{\Psi} M + \tau)' - \widehat{\Psi} \widehat{\Omega} \widehat{\Psi}
\end{equation}
Combining this with Equation \ref{eq:FMSCindicate},
\begin{equation}
\label{eq:omegahat}
	\widehat{\varphi}_S(\tau,M) = \mathbf{1}\left\{\widehat{\mbox{FMSC}}_S(\tau,M) = \min_{S'\in \mathscr{S}} \widehat{\mbox{FMSC}}_{S'}(\tau,M)\right\}
\end{equation}
For GMM-AIC moment selection or selection based on a downward $J$-test, $\varphi_S(\cdot,\cdot)$ may be estimated analogously, following  Theorem \ref{pro:jstat}. 

Although simulating draws from $M$, defined in Theorem \ref{thm:normality}, requires only an estimate of $\Omega$, the limit $\varphi_S$ of the weight function also depends on $\tau$. 
As discussed above, no consistent estimator of $\tau$ is available under local mis-specification: the estimator $\widehat{\tau}$ has a non-degenerate limit distribution (see Theorem \ref{thm:tau}). 
Thus, substituting $\widehat{\tau}$ for $\tau$ will give erroneous results by failing to account for the uncertainty that enters through $\widehat{\tau}$. 
The solution is to use a two-stage procedure. 
First construct a  $100(1-\delta)\%$ confidence region $\mathscr{T}(\widehat{\tau},\delta)$ for $\tau$ using Theorem \ref{thm:tau}. 
Then, for each $\tau^* \in \mathscr{T}(\widehat{\tau},\delta)$ simulate from the distribution of $\Lambda(\tau^*)$, defined in Corollary \ref{cor:momentavg}, to obtain a \emph{collection} of $(1-\alpha)\times 100\%$ confidence intervals indexed by $\tau^*$. 
Taking the lower and upper bounds of these yields a \emph{conservative} confidence interval for $\widehat{\mu}$, as defined in defined in Equation \ref{eq:avg}. 
This interval has asymptotic coverage probability of \emph{at least} $(1-\alpha-\delta)\times 100\%$.
The precise algorithm is as follows.
\begin{alg}[Simulation-based Confidence Interval for $\widehat{\mu}$]
\label{alg:conf}
\mbox{}
\begin{enumerate}
	\item For each $\tau^* \in \mathscr{T}(\widehat{\tau},\delta)$ 
		\begin{enumerate}[(i)]
			\item Generate $J$ independent draws $M_j \sim N_{p+q}( 0, \widehat{\Omega} )$
			\item Set $\Lambda_j(\tau^*) = -\nabla_\theta\mu(\widehat{\theta})'\left[\sum_{S \in \mathscr{S}} \widehat{\varphi}_S(\tau^*,M_j) \widehat{K}_S\Xi_S\right] (M_j + \tau^*)$
			\item Using the draws $\{\Lambda_j(\tau^*)\}_{j=1}^J$, calculate $\widehat{a}(\tau^*)$, $\widehat{b}(\tau^*)$ such that
		$$P\left\{ \widehat{a}(\tau^*) \leq\Lambda(\tau^*)\leq \widehat{b}(\tau^*) \right\} = 1 - \alpha$$
		\end{enumerate}
	\item Set $\displaystyle \widehat{a}_{min}(\widehat{\tau})=\min_{\tau^* \in \mathscr{T}(\widehat{\tau},\delta)} \widehat{a}(\tau^*)$ and $\displaystyle \widehat{b}_{max}(\widehat{\tau})= \max_{\tau^* \in \mathscr{T}(\widehat{\tau},\delta)} \widehat{b}(\tau^*)$ \vspace{0.5em}
	\item The confidence interval for $\mu$ is
				$\displaystyle \mbox{CI}_{sim}=\left[ \widehat{\mu} - \frac{\widehat{b}_{max}(\widehat{\tau})}{\sqrt{n}}, \;\;\; \widehat{\mu} - \frac{\widehat{a}_{min}(\widehat{\tau})}{\sqrt{n}} \right]$
\end{enumerate}
\end{alg}

\begin{thm}[Simulation-based Confidence Interval for $\widehat{\mu}$]
\label{pro:sim}
Let $\widehat{\Psi}$, $\widehat{\Omega}$, $\widehat{\theta}$, $\widehat{K}_S$, $\widehat{\varphi}_S$ be consistent estimators of $\Psi$, $\Omega$, $\theta_0$, $K_S$, $\varphi_S$ and define 
\begin{eqnarray*}
	\Delta_n(\widehat{\tau},\tau^*) &=& \left(\widehat{\tau} - \tau^*\right)' \left(\widehat{\Psi}\widehat{\Omega}\widehat{\Psi}'\right)^{-1} \left(\widehat{\tau} - \tau^*\right)\\
	\mathscr{T}(\widehat{\tau},\delta) &=& \left\{\tau^* \colon  \Delta_n(\widehat{\tau},\tau^*) \leq \chi^2_q(\delta)\right\}
\end{eqnarray*}
where $\chi^2_q(\delta)$ denotes the $1-\delta$ quantile of a $\chi^2$ distribution with $q$ degrees of freedom.
Then, the interval $\mbox{CI}_{sim}$ defined in Algorithm \ref{alg:conf} has asymptotic coverage probability no less than $1-(\alpha + \delta)$ as $J,n\rightarrow \infty$.
\end{thm}

\todo[inline]{Need to talk about coverage versus width. Explore in the simulations and empirical example. There is a cost to moment selection. But this cost is still present when selection is carried out informally: we're just trying to make it formal here. Also talk about how we could use the same procedure to get an interval for more general moment average estimators.}

To evaluate the performance of the procedure given in Algorithm \ref{alg:conf}, we revisit the simulation experiment described in Section \ref{sec:fmscsim}, considering FMSC moment selection. The following results are based on 10,000 replications, each with a sample size of 500. Table \ref{tab:FMSCconf} gives the empirical coverage probabilities of traditional 95\% confidence intervals post-FMSC selection. These are far below the nominal level over the vast majority of the parameter space. Table \ref{tab:FMSCcorrect} presents the empirical coverage of conservative 90\% confidence intervals constructed according to Algorithm \ref{alg:conf}, with $B=1000$.\footnote{Because this simulation is computationally intensive, I use a reduced grid of parameter values.} The two-stage simulation procedure performs remarkably well, achieving a minimum coverage probability of $0.89$ relative to its nominal level of $0.9$. Moreover, a na\"{i}ve one-step procedure that omits the first-stage and simply simulates from $M$ based on $\widehat{\tau}$ performs surprisingly well; see Table \ref{tab:FMSCnaive}. While the empirical coverage probabilities of the one-step procedure are generally lower than the nominal level of $0.95$, they represent a substantial improvement over the traditional intervals given in Table \ref{tab:FMSCconf}, with a worst-case coverage of $0.72$ compared to $0.15$. This suggests that the one-step intervals might be used as a rough but useful approximation to the correct but more computationally intensive intervals constructed according to Algorithm \ref{alg:conf}.

% latex.default(cover.FMSC, file = outfilename, greek = TRUE, numeric.dollar = FALSE,      na.blank = TRUE, landscape = FALSE, rowname = NULL, append = TRUE) 
%
\begin{table}[!tbp]
 \begin{center}
 \caption{Coverage post-FMSC moment selection (nominal 95\%).}
\label{tab:FMSCconf}
\small
 \begin{tabular}{r|rrrrrrrrr}\hline\hline
&\multicolumn{9}{c}{$\rho = Cov(w,u)$}\\
\multicolumn{1}{c|}{$N=500$}&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0.05}&\multicolumn{1}{c}{0.10}&\multicolumn{1}{c}{0.15}&\multicolumn{1}{c}{0.20}&\multicolumn{1}{c}{0.25}&\multicolumn{1}{c}{0.30}&\multicolumn{1}{c}{0.35}&\multicolumn{1}{c}{0.40}\tabularnewline
\hline
0.0&0.92&0.93&0.93&0.93&0.93&0.93&0.93&0.93&0.93\tabularnewline
0.1&0.91&0.87&0.88&0.91&0.93&0.93&0.93&0.93&0.93\tabularnewline
0.2&0.90&0.79&0.72&0.82&0.90&0.93&0.92&0.93&0.93\tabularnewline
0.3&0.90&0.76&0.58&0.64&0.80&0.90&0.92&0.93&0.93\tabularnewline
0.4&0.89&0.75&0.50&0.47&0.64&0.80&0.88&0.91&0.92\tabularnewline
0.5&0.89&0.74&0.45&0.36&0.50&0.67&0.79&0.87&0.91\tabularnewline
0.6&0.89&0.74&0.43&0.30&0.38&0.54&0.68&0.78&0.85\tabularnewline
\multirow{4}{5mm}{\begin{sideways}\parbox{1mm}{$\gamma\;$=$\;Cov(w,x)$}\end{sideways}}
0.7&0.90&0.74&0.41&0.24&0.31&0.44&0.57&0.68&0.78\tabularnewline
0.8&0.89&0.74&0.41&0.22&0.25&0.36&0.48&0.59&0.70\tabularnewline
0.9&0.91&0.74&0.41&0.20&0.21&0.31&0.41&0.52&0.61\tabularnewline
1.0&0.90&0.75&0.40&0.18&0.19&0.25&0.35&0.45&0.53\tabularnewline
1.1&0.90&0.76&0.40&0.17&0.17&0.23&0.32&0.39&0.47\tabularnewline
1.2&0.91&0.76&0.41&0.17&0.15&0.20&0.27&0.34&0.42\tabularnewline
1.3&0.92&0.77&0.41&0.16&0.15&0.19&0.24&0.31&0.39\tabularnewline
\hline
\end{tabular}
\end{center}
\footnotesize
\begin{tablenotes}
\item Values are calculated by simulating from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications.
\end{tablenotes}
\end{table}



% latex.default(conf.refined, file = outfilename, greek = TRUE,      numeric.dollar = FALSE, na.blank = TRUE, landscape = FALSE,      rowname = NULL, append = FALSE) 
%
\begin{table}[!tbp]
 \begin{center}
\caption{Coverage of conservative two-step interval post-FMSC (nominal $>90\%$)}
\label{tab:FMSCcorrect}
\small
 \begin{tabular}{r|rrrrr}\hline\hline
&\multicolumn{5}{c}{$\rho = Cov(w,u)$}\\
\multicolumn{1}{c|}{$N=500$}&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0.1}&\multicolumn{1}{c}{0.2}&\multicolumn{1}{c}{0.3}&\multicolumn{1}{c}{0.4}\tabularnewline
\hline
0.0&0.92&0.93&0.93&0.93&0.94\tabularnewline
0.2&0.95&0.91&0.93&0.95&0.97\tabularnewline
0.4&0.95&0.95&0.90&0.93&0.97\tabularnewline
0.6&0.95&0.95&0.92&0.90&0.92\tabularnewline
\multirow{4}{5mm}{\begin{sideways}\parbox{1mm}{$\gamma\;$=$\;Cov(w,x)$}\end{sideways}}
0.8&0.94&0.95&0.96&0.90&0.89\tabularnewline
1.0&0.94&0.94&0.96&0.93&0.90\tabularnewline
1.2&0.94&0.94&0.96&0.95&0.92\tabularnewline
\hline
\end{tabular}
\end{center}
\footnotesize
\begin{tablenotes}
	\item Intervals are calculated using Algorithm \ref{alg:conf} with $B = 1000$. Simulations are generated from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications.
\end{tablenotes}
\end{table}


% latex.default(conf.naive, file = outfilename, greek = TRUE, numeric.dollar = FALSE,      na.blank = TRUE, landscape = FALSE, rowname = NULL, append = FALSE) 
%
\begin{table}[!tbp]
\caption{Coverage of na\"{i}ve one-step interval post-FMSC (nominal 95\%)}
\label{tab:FMSCnaive}
\small
 \begin{center}
 \begin{tabular}{r|rrrrrrrrr}\hline\hline
&\multicolumn{9}{c}{$\rho = Cov(w,u)$}\\
\multicolumn{1}{c|}{$N=500$}&\multicolumn{1}{c}{0}&\multicolumn{1}{c}{0.05}&\multicolumn{1}{c}{0.10}&\multicolumn{1}{c}{0.15}&\multicolumn{1}{c}{0.20}&\multicolumn{1}{c}{0.25}&\multicolumn{1}{c}{0.30}&\multicolumn{1}{c}{0.35}&\multicolumn{1}{c}{0.40}\tabularnewline
\hline
0.0&0.93&0.92&0.93&0.93&0.93&0.93&0.93&0.93&0.94\tabularnewline
0.1&0.93&0.91&0.91&0.92&0.92&0.92&0.93&0.94&0.95\tabularnewline
0.2&0.94&0.91&0.86&0.87&0.92&0.93&0.94&0.95&0.96\tabularnewline
0.3&0.95&0.94&0.87&0.81&0.85&0.91&0.94&0.96&0.96\tabularnewline
0.4&0.95&0.95&0.91&0.82&0.77&0.84&0.90&0.94&0.95\tabularnewline
0.5&0.95&0.95&0.93&0.86&0.76&0.76&0.82&0.88&0.92\tabularnewline
0.6&0.94&0.94&0.94&0.90&0.80&0.74&0.75&0.81&0.87\tabularnewline
\multirow{4}{5mm}{\begin{sideways}\parbox{1mm}{$\gamma\;$=$\;Cov(w,x)$}\end{sideways}}
0.7&0.94&0.94&0.95&0.93&0.85&0.74&0.73&0.75&0.81\tabularnewline
0.8&0.94&0.94&0.95&0.94&0.88&0.79&0.73&0.73&0.76\tabularnewline
0.9&0.95&0.94&0.94&0.94&0.91&0.83&0.76&0.72&0.73\tabularnewline
1.0&0.95&0.94&0.94&0.94&0.92&0.86&0.78&0.73&0.73\tabularnewline
1.1&0.95&0.94&0.94&0.95&0.94&0.89&0.81&0.76&0.73\tabularnewline
1.2&0.95&0.94&0.94&0.95&0.94&0.90&0.85&0.79&0.75\tabularnewline
1.3&0.95&0.94&0.94&0.95&0.95&0.92&0.87&0.81&0.78\tabularnewline
\hline
\end{tabular}
\end{center}
\footnotesize
	\begin{tablenotes}
		\item Intervals are calculated by simulation with $B=1000$ using $\widehat{\tau}$ rather than constructing a confidence interval for $\tau$ (c.f.\ Algorithm \ref{alg:conf}). Simulations are generated from Equations \ref{eq:secondstage}--\ref{eq:varmatrix} with $10,000$ replications.
	\end{tablenotes}
\end{table}


\input{empirical_example.tex}

\input{conclusion.tex}


\bibliographystyle{elsarticle-harv}


\bibliography{fmsc_refs}


\appendix

\input{proofs.tex}

\end{document}