%!TEX root = main.tex
\subsection{Choosing Instrumental Variables Example}
\label{sec:chooseIVsim}
I now evaluate the performance of FMSC in the instrument selection example described in Section \ref{sec:chooseIVexample} using the following simulation design:
\begin{eqnarray}
		y_i &=& 0.5 x_i + \epsilon_i\\ 
		\label{eq:chooseIVDGP1}
		x_i &=& (z_{1i} + z_{2i} + z_{3i}) /3 + \gamma w_i + v_i 
		\label{eq:chooseIVDGP2}
	\end{eqnarray}
for $i=1, 2, \hdots, N$ where $(\epsilon_i, v_i, w_i, z_{i1}, z_{2i}, z_{3i})' \sim \mbox{ iid  } N(0,\mathcal{V})$ with

\begin{equation}	
	\mathcal{V} = \left[  \begin{array}
		{cc} \mathcal{V}_1 & 0 \\ 0 & \mathcal{V}_2
	\end{array}\right], \quad
	\mathcal{V}_1 = \left[ \begin{array}
		{ccc} 
		1 & (0.5 - \gamma \rho) & \rho \\
		(0.5 - \gamma \rho) & (8/9 - \gamma^2) & 0\\ 
		\rho & 0 & 1 \\ 
	\end{array} \right], \quad \mathcal{V}_2 = I_3 / 3
	\label{eq:chooseIVDGP3}
\end{equation}
This setup keeps the variance of $x$ fixed at one and the endogeneity of $x$, $Cor(x, \epsilon)$, fixed at $0.5$ while allowing the relevance, $\gamma = Cor(x,w)$, and endogeneity, $\rho = Cor(w, \epsilon)$, of the instrument $w$ to vary.
The instruments $z_1, z_2, z_3$ are valid and exogenous: they have first-stage coefficients of $1/3$ and are uncorrelated with the second stage error $\epsilon$.
The additional instrument $w$ is only relevant if $\gamma \neq 0$ and is only exogenous if $\rho = 0$.
Since $x$ has unit variance, the first-stage R-squared for this simulation design is simply $1 - \sigma_v^2 = 1/9 + \gamma^2$.
Hence, when  $\gamma = 0$, so that $w$ is irrelevant, the first-stage R-squared is just over 0.11.
Increasing $\gamma$ increases the R-squared of the first-stage.
This design satisfies the sufficient conditions for Theorem \ref{thm:chooseIV} given in Assumption \ref{assump:chooseIV}.
When $\gamma = 0$, it is a special case of the DGP from Section \ref{sec:OLSvsIVsim}.

As in Section \ref{sec:OLSvsIVsim}, the goal of moment selection in this exercise is to estimate the effect of $x$ on $y$, as before 0.5, with minimum MSE.
In this case, however, the choice is between two TSLS estimators rather than OLS and TSLS: the \emph{valid} estimator uses only $z_1, z_2,$ and $z_3$ as instruments, while the \emph{full} estimator uses $z_1, z_2, z_3,$ and $w$.
The inclusion of $z_1, z_2$ and $z_3$ in both moment sets means that the order of over-identification is two for the valid estimator and three for the full estimator. 
Because the moments of the TSLS estimator only exist up to the order of over-identification \citep{Phillips1980}, this ensures that the small-sample MSE is well-defined.\footnote{Alternatively, one could use fewer instruments for the valid estimator and compare the results using \emph{trimmed} MSE, as in \cite{HansenShrink}.}
All estimators in this section are calculated via TSLS without a constant term using the expressions from Section \ref{sec:chooseIVexample} and 20,000 simulation replications.
 
Figure \ref{fig:chooseIVsim_RMSEbaseline} presents RMSE values for the valid estimator, the full estimator, and the post-FMSC estimator for various combinations of $\gamma$, $\rho$, and $N$.
The results are broadly similar to those from the OLS versus TSLS example presented in Figure \ref{fig:OLSvsIV_RMSEbaseline}.
For any combination $(\gamma,N)$ there is a positive value of $\rho$ below which the full estimator yields a lower RMSE than the full estimator.
As the sample size increases, this cutoff becomes smaller; as $\gamma$ increases, it becomes larger.
As in the OLS versus TSLS example, the post-FMSC estimator represents a compromise between the two estimators over which the FMSC selects.
Unlike in the previous example, however, when $N$ is sufficiently small there is a range of values for $\rho$ within which the FMSC yields a lower RMSE than \emph{both} the valid and full estimators.
This comes from the fact that the valid estimator is quite erratic for small sample sizes.
Such behavior is unsurprising given that its first stage is not especially strong, $\mbox{R-squared}\approx 11\%$, and it has only two moments.
In contrast, the full estimator has three moments and a stronger first stage.
As in the OLS versus TSLS example, the post-FMSC estimator does not uniformly outperform the valid estimator for all parameter values, although it does for smaller sample sizes.
The FMSC never performs much worse than the valid estimator, however, and often performs substantially better, particularly for small sample sizes.
\begin{figure}
\centering
	\input{./SimulationChooseIVs/Results/RMSE_coarse_gamma_baseline.tex}
	\caption{RMSE values for the valid estimator, including only $(z_1, z_2, z_3)$, the full estimator, including $(z_1, z_2, z_3, w)$, and the post-Focused Moment Selection Criterion (FMSC) estimator based on 20,000 simulation draws from the DGP given in Equations \ref{eq:chooseIVDGP1}--\ref{eq:chooseIVDGP3} using the formulas described in Sections \ref{sec:chooseIVexample}.}
	\label{fig:chooseIVsim_RMSEbaseline}
\end{figure}

I now compare the FMSC to the GMM moment selection criteria of \cite{Andrews1999}, which take the form $MSC(S) = J_n(S) - h(|S|)\kappa_n$, where $J_n(S)$ is the $J$-test statistic under moment set $S$ and $-h(|S|)\kappa_n$ is a ``bonus term'' that rewards the inclusion of more moment conditions.
For each member of this family we choose the moment set that \emph{minimizes} $MSC(S)$. 
If we take $h(|S|) = (p + |S| - r)$, then $\kappa_n = \log{n}$ gives a GMM analogue of Schwarz's Bayesian Information Criterion (GMM-BIC) while $\kappa_n = 2.01 \log{\log{n}}$ gives an analogue of the Hannan-Quinn Information Criterion (GMM-HQ), and $\kappa_n = 2$ gives an analogue of Akaike's Information Criterion (GMM-AIC). 
Like the maximum likelihood model selection criteria upon which they are based, the GMM-BIC and GMM-HQ are consistent provided that Assumption \ref{assump:Andrews} holds, while the GMM-AIC, like the FMSC, is conservative.
Figure \ref{fig:chooseIVsim_RMSErelMSC} gives the RMSE values for the post-FMSC estimator alongside those of the post-GMM-BIC, HQ and AIC estimators.
I calculate the $J$-test statistic using a centered covariance matrix estimator, following the recommendation of \cite{Andrews1999}.
For small sample sizes, the GMM-BIC, AIC and HQ are quite erratic: indded for $N = 50$ the FMSC has a uniformly smaller RMSE.
This problem comes from the fact that the $J$-test statistic can be very badly behaved in small samples.\footnote{For more details, see Appendix \ref{sec:downwardJ}.}
As the sample size becomes larger, the classic tradeoff between consistent and conservative selection emerges.
For the smallest values of $\rho$ the consistent criteria outperform the conservative criteria; for moderate values the situation is reversed.
The consistent criteria, however, have the highest worst-case RMSE.
For a discussion of a combined strategy based on the GMM information criteria of \cite{Andrews1999} and the canonical correlations information criteria of \cite{HallPeixe2003}, see Appendix \ref{sec:CCIC}.
For a comparison with the downward $J$-test, see Appendix \ref{sec:downwardJ}.
\begin{figure}
\centering
	\input{./SimulationChooseIVs/Results/RMSE_coarse_gamma_rel_MSC.tex}
	\caption{RMSE values for the post-Focused Moment Selection Criterion (FMSC) estimator and the GMM-BIC, HQ, and AIC estimators based on 20,000 simulation draws from the DGP given in Equations \ref{eq:chooseIVDGP1}--\ref{eq:chooseIVDGP3} using the formulas described in Sections \ref{sec:chooseIVexample}.}
	\label{fig:chooseIVsim_RMSErelMSC}
\end{figure}
