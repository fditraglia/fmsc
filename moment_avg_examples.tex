%!TEX root = main.tex
\subsection{Digression: Minimum-AMSE Averaging for OLS and TSLS}
\label{sec:momentavgexample}
When competing moment sets have similar criterion values in the population, sampling variation can be \emph{magnified} in the selected estimator.
This motivates the idea of averaging estimators based on different moment conditions rather than selecting them.
To illustrate this idea, I now briefly revisit the OLS versus TSLS example from Section \ref{sec:OLSvsIVExample} and derive an AMSE-optimal weighted average of the two estimators.
Let $\widetilde{\beta}(\omega)$ be a convex combination of the OLS and TSLS estimators, namely  
\begin{equation}
	\widetilde{\beta}(\omega) = \omega \widehat{\beta}_{OLS} + (1 - \omega) \widetilde{\beta}_{TSLS}
\end{equation}
where $\omega \in [0,1]$ is the weight given to the OLS estimator.
\begin{thm}
	\label{thm:OLSvsIVavg} 
	Under the conditions of Theorem \ref{thm:OLSvsIV}, the AMSE of the weighted-average estimator $\sqrt{n}\left[\widehat{\beta}(\omega) - \beta \right]$ is minimized over $\omega \in [0,1]$ by taking $\omega = \omega^*$ where
	$$ \omega^* = \left[1 + \frac{\tau^2/\sigma_x^4}{\sigma_\epsilon^2(1/\gamma^2 - 1/\sigma_x^2)}\right]^{-1} = \left[1 + \frac{\mbox{ABIAS(OLS)}^2}{\mbox{AVAR(TSLS)}-\mbox{AVAR(OLS)}} \right]^{-1}.$$
\end{thm}

The preceding result has several important consequences. 
First, since the variance of the TSLS estimator is always strictly greater than that of the OLS estimator, the optimal value of $\omega$ \emph{cannot} be zero. 
No matter how strong the endogeneity of $x$,
as measured by $\tau$, we should always give some weight to the OLS estimator. 
Second, when $\tau = 0$ the optimal value of $\omega$ is one. If $x$ is exogenous, OLS is strictly preferable to TSLS. 
Third, the optimal weights depend on the strength of the instruments $\mathbf{z}$ as measured by $\gamma$.
All else equal, the stronger the instruments, the less weight we should give to OLS.
To operationalize the AMSE-optimal averaging estimator suggested from Theorem \ref{thm:OLSvsIVavg}, I propose the plug-in estimator 
\begin{equation}
	\widehat{\beta}^*_{AVG} = \widehat{\omega}^* \widehat{\beta}_{OLS} + (1 - \widehat{\omega}^*)\widetilde{\beta}_{TSLS}
	\label{eq:OLSvsIV_AVG1}
\end{equation}
where
\begin{equation}
\widehat{\omega }^* = \left[1 + \frac{\max \left\{0, \; \left(\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right) \right)/\;\widehat{\sigma}_x^4 \right\}}{\widehat{\sigma}_\epsilon^2 (1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2)}\right]^{-1}
	\label{eq:OLSvsIV_AVG2}
\end{equation}
This expression employs the same consistent estimators of $\sigma_x^2, \gamma$ and $\sigma_{\epsilon}$ as the FMSC expressions from Section \ref{sec:OLSvsIVExample}.
To ensure that $\widehat{\omega}^*$ lies in the interval $[0,1]$, however, I use a \emph{positive part} estimator for $\tau^2$, namely $\max\{0, \; \widehat{\tau}^2 - \widehat{V}\}$ rather than $\widehat{\tau}^2 - \widehat{V}$.\footnote{While $\widehat{\tau}^2 - \widehat{V}$ is an asymptotically unbiased estimator of $\tau^2$ it \emph{can} be negative.}
In the following section I show how one can construct a valid confidence interval for $\widehat{\beta}^*$ and related estimators.
