%!TEX root = main.tex
\section{Simulation Results}
\label{sec:simulations}
\subsection{OLS versus TSLS Example}
\label{sec:OLSvsIVsim}
I begin by examining the performance of the FMSC and averaging estimator in the OLS versus TSLS example.
All calculations in this section are based on the formulas from Sections \ref{sec:OLSvsIVExample} and \ref{sec:momentavgexample} with 10,000 simulation replications. 
The data generating process is given by 
\begin{eqnarray}
	y_i &=& 0.5 x_i + \epsilon_i\\
	\label{eq:OLSvsIVDGP1}
	x_i &=& \pi(z_{1i} + z_{2i} + z_{3i}) + v_i
	\label{eq:OLSvsIVDGP2}
\end{eqnarray}
with $(\epsilon_i, v_i, z_{1i}, z_{2i}, z_{3i}) \sim \mbox{ iid } N(0, \mathcal{S})$
\begin{equation}
	\mathcal{S} = \left[ \begin{array}
		{ccccc} 
		1 & \rho & 0 & 0 & 0\\
		\rho & 1 - \pi^2 & 0 & 0 & 0\\
		0 & 0 & 1/3 & 0 & 0\\
		0 & 0 & 0 & 1/3 & 0 \\
		0 & 0 & 0 & 0 & 1/3
	\end{array}\right]
	\label{eq:OLSvsIVDGP3}
\end{equation}
for $i= 1, \hdots, N$ where $N$, $\rho$ and $\pi$ over a grid.
The goal is to estimate the effect of $x$ on $y$, in this case 0.5, with minimum MSE: either by choosing between OLS and TSLS estimators or by averaging them.
To ensure that the finite-sample MSE of the TSLS estimator exists, this DGP includes three instruments leading to two overidentifying restrictions \citep{Phillips1980}.\footnote{Alternatively, one could use fewer instruments in the DGP and use work with trimmed MSE.}
This design satisfies regularity conditions that are sufficient for Theorem \ref{thm:OLSvsIV} -- in particular it satisfies Assumption \ref{assump:OLSvsIV} -- and keeps the variance of $x$ fixed at one so that $\pi = Corr(x_i, z_{1i} + z_{2i} + z_{3i})$ and $\rho = Corr(x_i,\epsilon_i)$.
The first-stage R-squared is simply $1 - \sigma_v^2/\sigma_x^2 = \pi^2$ so that larger values of $|\pi|$ \emph{reduce} the variance of the TSLS estimator.
Since $\rho$ controls the endogeneity of $x$, larger values of $|\rho|$ \emph{increase} the bias of the OLS estimator.

Figure \ref{fig:OLSvsIV_RMSEbaseline} compares the root mean-squared error (RMSE) of the post-FMSC estimator to those of the simple OLS and TSLS estimators.
For any values of $N$ and $\pi$ there is a value of $\rho$ below which OLS outperforms TSLS. 
As $N$ and $\pi$ increase this value approaches zero; as they decrease it approaches one.
Although the first two moments of the TSLS estimator exist in this simulation design, none of its higher moments do. 
This fact is clearly evident for small values of $N$ and $\pi$: even with 10,000 simulation replications, the RMSE of the TSLS estimator shows a noticable degree of simulation error unlike those of the OLS and post-FMSC estimators.

The FMSC represents a compromise between OLS and TSLS.
When the RMSE of TSLS is high, the FMSC behaves more like OLS; when the RMSE of OLS is high it behaves more like TSLS.
Because the FMSC is itself a random variable, however, it sometimes makes moment selection mistakes.\footnote{For more discussion of this point, see Section \ref{sec:avg}.} 
For this reason it does not attain an RMSE equal to the lower envelope of the OLS and TSLS estimators: moment selection is not a free lunch.
The larger the RMSE difference between OLS and TSLS, however, the closer the FMSC comes to this lower envelope: the more costly the mistake, the rarer.

The question remains: should one carry out moment selection via the FMSC or avoid selection altogether and simply use the correctly specified TSLS estimator?
As usual the answer depends where we happen to be in the parameter space.
The FMSC does not provide a uniform improvement over TSLS: with a scalar target parameter no procedure can.
The worst-case RMSE of the post-FMSC estimator, however, is never much higher than that of TSLS while the worst-case RMSE of TSLS can be dramatically higher than that of the FMSC, depending on parameter values.
Moreover, as discussed above, the FMSC is intended for situations in which an applied researcher fears that her ``baseline'' assumptions may be too weak and consequently considers adding one or more ``controversial'' assumptions. 
In this case, she fears that the exogenous instruments $z_1, z_2, z_3$ are not particularly strong, $\pi$ is small relative to $N$, and thus entertains the assumption that $x$ is exogenous.
It is precisely in these situations that the FMSC provides large performance gains over TSLS.

\begin{figure}
\centering
	\input{./SimulationOLSvsIV/Results/RMSE_coarse_pi_baseline.tex}
	\caption{RMSE values for the two-stage least squares (TSLS) estimator, the ordinary least squares (OLS) estimator, and the post-Focused Moment Selection Criterion (FMSC) estimator based on 10,000 simulation draws from the DGP given in Equations \ref{eq:OLSvsIVDGP1}--\ref{eq:OLSvsIVDGP3} using the formulas described in Section \ref{sec:OLSvsIVExample}.}
	\label{fig:OLSvsIV_RMSEbaseline}
\end{figure}

As shown above, the FMSC takes a very special form in this example: it is equivalent to a DHW test with $\alpha \approx 0.16$.
Accordingly, Figure \ref{fig:OLSvsIV_RMSEbaseline} compares the RMSE of the post-FMSC estimator to those of DHW pre-test estimators with significance levels $\alpha = 0.05$ and $\alpha = 0.1$, indicated in the legend by DHW95 and DHW90.
Since these three procedures differ only in their critical values, they show similar qualitative behavior.
When $\rho$ is sufficiently close to zero, we saw from Figure \ref{fig:OLSvsIV_RMSEbaseline} that OLS has a lower RMSE than TSLS.
Since DHW95 and DHW90 require a higher burden of proof to reject OLS in favor of TSLS, they outperform FMSC in this region of the parameter space.
When $\rho$ crosses the threshold beyond which TSLS has a lower RMSE than OLS, the tables are turned: FMSC outperforms DHW95 and DHW90.
As $\rho$ increases further, relative to sample size and $\pi$, the three procedures become indistinguishable in terms of RMSE.
In addition to comparing the FMSC to DHW pre-test estimators, Figure \ref{fig:OLSvsIV_AVG} also presents the finite-sample RMSE of the minimum-AMSE moment average estimator presented in Equations \ref{eq:OLSvsIV_AVG1} and \ref{eq:OLSvsIV_AVG2}.
The performance of the moment average estimator is strong: it provides the lowest worst-case RMSE and improves uniformly on the FMSC for all but the largest values of $\rho$.
While a complete examination of minimum-AMSE moment averaging is beyond the scope of the present paper, these results suggest that it could prove extremely useful in practice.

\begin{figure}
\centering
	\input{./SimulationOLSvsIV/Results/RMSE_coarse_pi_relative_all.tex}
	\caption{RMSE values for the post-Focused Moment Selection Criterion (FMSC) estimator, Durbin-Hausman-Wu pre-test estimators with $\alpha = 0.1$ (DWH90) and $\alpha = 0.05$ (DHW95), and the minmum-AMSE averaging estimator, based on 10,000 simulation draws from the DGP given in Equations \ref{eq:OLSvsIVDGP1}--\ref{eq:OLSvsIVDGP3} using the formulas described in Sections \ref{sec:OLSvsIVExample} and \ref{sec:momentavgexample}.}
	\label{fig:OLSvsIV_AVG}
\end{figure}