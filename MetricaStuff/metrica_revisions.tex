\documentclass[12pt]{article}
\usepackage{todonotes}
\usepackage[margin=1.5in]{geometry}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{enumerate}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]

\linespread{1.2}

\title{JMP Revisions Following Econometrica Comments}
\author{Francis J.\ DiTraglia}
\begin{document}

\maketitle

\begin{abstract}
This document collects all of the new material that I plan to add to my JMP following the referee reports from Econometrica. Once I've put this material together, I'll merge it with the existing version and send it to ReStud.
\end{abstract}

\section*{Referee Comments and My Responses}
The two main objections to my paper were as follows:
  \begin{enumerate}
    \item The FMSC provides an asymptotically unbiased estimate of MSE, but the FMSC itself is random, even in the limit. It is not true that the FMSC converges in probability to actual AMSE because of this additional randomness. 
    \item If the valid model is identified, why not just use it? If it's not identified, FMSC doesn't work. It's not clear that the gains from this procedure would be large in practice since the FMSC has a variance. See point \# 1. 
    \item This approach is not robust to weak identification.
    \item The procedure for correcting confidence intervals is too computationally intensive and aims to get the size correct without even looking at the width.
    \item Finite sample MSE doesn't always exist for these kinds of models. Maybe this isn't the best measure to look at.
    What about OLS versus IV?
    \item There are some missing references.
    \item Some objections to the simulation experiment. In particular, no widths are reported for confidence intervals.
  \end{enumerate}
The most important objections are 1 and 2, possibly followed by 3. The other points can be fairly easily dealt with and I will do so below. 

\paragraph{Objection \# 1} This comment boils down to ``I don't like efficient model selection: you should use consistent model selection.'' It is indeed true that the FMSC is random even in the limit and that it does not converge to actual AMSE: it is merely an asymptotically unbiased estimator of AMSE. However, this is the whole point: I explicitly want to \emph{avoid} doing consistent selection because the associated risk properties are so poor. I need to make this clearer by citing, for example, the Leeb and P\"{o}tscher paper from 2008 that Larry Wasserman discussed on his blog.

In a certain sense, this comment amounts to a critique of my use of local asymptotics. I should make very clear that this device is widely used in econometrics to study, among other things, local power, local-to-unit roots, etc. I should provide references for this, including Schorfheide and Moon. Furthermore, a great many model selection procedures are random in the limit: Mallows $C_p$ and AIC are two well-known examples, but many people have worked with selection in this framework. I should cite Bruce Hansen as well as Frank Schorfheide's VAR paper and Schorfheide and Moon.

I should also relate my framework to the idea of uniform asymptotic validity as discussed in Andrews and Guggenberger (2010) and Schorfheide and Moon.


\paragraph{Objection \# 2} This comment suggests that: (a) my simulations weren't convincing enough, and (b) I wasn't clear enough about the sort of situation for which the FMSC is designed. The solution is to do more and better simulations and to be clearer! The point of the FMSC is that we often find ourselves in a setting where there are various ``plausible'' assumptions we could use in estimation, some of which are weaker and some of which are stronger. Typically, we worry that the weaker assumptions might not provide sufficient information to study the question we're interested in, which we worry that the stronger assumptions might not quite be true. You can think of this as a kind of ``prior knowledge'' that violations of the stronger assumptions are ``small'' which is pretty much exactly what the local mis-specification idea encodes. I can relate this to the idea of ``plausibly exogenous'' as well as Schorfheide and Moon. 

I should be clear about the fact, and I need to find the citation for this, that model selection cannot uniformly beat the ``full'' model. (In this case, the full model is the set of moment conditions based on the weakest assumptions.) However, selection \emph{can} beat the ``full'' model over large regions of the parameter space, so it's really a questino of where you think you might be a priori. I should argue that the whole idea is to use my method when you consider it likely that you might be in the relevant region of the parameter space. I should also show in simulations that the cost you pay when you are \emph{not} in this region isn't too high.


\paragraph{Objection \# 3} This point is less important but also harder to handle. I'm pretty sure that there's no straightforward way to include weak identification in my framework directly although it's a very compelling idea: when you have weak identification it might make a lot of sense to use a slightly endogenous instrument. Even though I don't think it's possible to incorporate weak instrument \emph{asymptotics}, however, I can still evaluate how my proposed \emph{procedure} deals with weak instruments. There are at least two ways to do this. The first is by carrying out a simulation study. The second is by looking trying to relate the FMSC to some other well-known tests or procedures. In the IV versus OLS case, for example, I know that the result is a Hausman test with a non-standard critical value. I seem to recall that Dufour has a paper in which he argues that this is a good idea when you might have weak instruments. Basically the idea here is to look at how the FMSC implicitly trades off instrument strength and validity when we take the procedure \emph{outside} of its asymptotic framework.  

\paragraph{Overall Thoughts and To Do List}
Besides responding to the Objections 1--3, here are some other things I should do, roughly in order of importance:
  \begin{enumerate}
    \item Should defend my assumption that we have a minimal set of correct MCs by referencing the literature, including Chen etc.
    \item Change the notation to allow us to use arbitrary subsets of the moment conditions. This will accomodate the OLS/IV example.
    \item Allow the weighting matrix $W$ to be indexed by $S$.
    \item Fold in the IV/OLS example. This will allow me to discuss a number of interesting points, including optimal estimator averaging, weak instruments, relationship to testing, etc. Also allows a brief consideration of what happens when we have no valid moment conditions.
    \item Do a better job with the choosing instruments example.
    \item New and better simulation experiments. Look at median absolute deviation as well as trimmed MSE, etc. Try to cover more of the parameter space, etc. Pictures rather than tables. Everything needs to be replicable for ReStud!
    \item Re-do the empirical example with improved code for the confidence interval. Everything needs to be replicable for ReStud!
    \item Possibly add a second empirical example for OLS versus IV.
    \item Look at the proposed references and think about including them.
    \item Might want to be slightly more careful about regularity conditions. See for example Schorfheide and Moon.
  \end{enumerate}

\section{New Notation for Moment Selection Vector}
This is straightforward: just need to redefine $S$ and $\Xi_S$. See the GFIC paper.

\section{Two Running Examples in the Paper}
Two simple but empirically relevant examples we'll consider throughout the paper. Helpful because they make the intuition clear and also interesting in their own right. To be clear, FMSC applies to GMM in general, not just to linear models like these.


\paragraph{Example \#1: Choosing Instrumental Variables}
Consider the linear model
  \begin{equation}
  y_i = \mathbf{x}_i'\beta + \epsilon_i
  \end{equation}
where some or all of the regressors are endogenous. Suppose we have a vector of valid instruments $\mathbf{z}^{(1)}_i$ and another vector of ``suspect'' instruments $\mathbf{w}_i$ that are likely to be highly relevant but may well be slightly endogenous. These could be ``plausibly exogenous.'' Should we include $\mathbf{z}_i$ in the instrument set for use in estimation?  Arises in various settings. One concerns exogeneity assumptions in a panel data setting: strict exogeneity versus predeterminedness. 

\paragraph{Example \#2: Least Squares versus Instrumental Variables}
This example is similar to the first one, but illustrates that we don't have to structure the problem in terms of choosing over-identifying restrictions: we can select over fundamentally different estimators. Suppose we want to estimate the effect of an endogenous regressor $x$ in a linear model of the form
  \begin{equation}
    y_i = \mathbf{w}_i' \theta + \beta x_i + \epsilon_i
  \end{equation}
where $\mathbf{w}_i$ is a vector of endogenous control regressors. In this case the target parameter is $\beta$. Suppose we have a vector of valid instruments $\mathbf{z}_i$. Should we use OLS or IV? Dufour: ``IV is like Amputation; it should be a last resort to save the patient." Easily generalized to more than one endogenous regressor, but this example is very common in many settings, particularly treatment effects with microdata. Cite the Nevo and Rosen and Plausibly Exogenous papers. Another issue is weak instruments and whether it is possible to \emph{combine} OLS and IV.



\section{Local Mis-specification for the Examples}

\paragraph{Example \#1: Choosing Instrumental Variables}
 \begin{equation}
  E_n\left[\begin{array}{c}
    \mathbf{w}_i(y_i - \mathbf{x}_i \beta)\\
     \mathbf{z}_i(y_i - \mathbf{x}_i \beta)
  \end{array} \right] = \left[ \begin{array}{c}\mathbf{0} \\ \boldsymbol{\tau}/\sqrt{n} \end{array}\right] 
 \end{equation}
 
 \paragraph{Example \#2: Least Squares versus Instrumental Variables}
\begin{equation}
    E_n\left[\begin{array}{c}
    \mathbf{w}_i(y_i - \mathbf{w}_i' \theta  - \beta x_i)\\
    \mathbf{z}_i(y_i - \mathbf{w}_i' \theta  - \beta x_i)\\
    x_i(y_i - \mathbf{w}_i' \theta  - \beta x_i)
    \end{array}\right] = \left[\begin{array}{c} \mathbf{0} \\ \mathbf{0} \\ \tau/\sqrt{n}  \end{array} \right]
\end{equation}
In our consideration of this example in the remainder of the paper we will assume, without loss of generality, that there are no exogenous regressors $\mathbf{w}_i$. If there are any, they can always be ``projected out'' of $y$, $x$ and $\mathbf{z}$.

\section{Lemma: A CLT for Local Mis-specification}
Covers both of the examples from the paper in an iid setting (microdata, panel, etc.) Can be extended to handle dependence by using something other than Lindeberg-Feller.

\begin{lem}[CLT Under Local Mis-specification] 
\label{lem:CLT}
 Let $\{\mathbf{w}_i, \mathbf{z}_i, \epsilon_i\colon 1 \leq i \leq n, n = 1, 2, \hdots\}$ be a triangular array of random variables such that 
  \begin{enumerate}[(a)]
    \item $(\mathbf{w}_i, \mathbf{z}_i, \epsilon_i) \sim \mbox {iid}$ within each row of the array (i.e.\ for fixed $n$)
    \item $E_n\left[\epsilon_i \mathbf{w}_i \right] = \mathbf{0}$
    \item $E_n\left[\epsilon_i \mathbf{z}_i \right] = \boldsymbol{\tau}/\sqrt{n}$
    \item $E_n \left[\left|\epsilon_i \mathbf{w}_i\right|^{2+\eta} \right] < C$ and $E_n \left[\left|\epsilon_i \mathbf{z}_i\right|^{2+\eta} \right] < C$ for all $n$ and some $\eta > 0$ where $C < \infty$.
    \item $\displaystyle  \Omega = \lim_{n \rightarrow \infty} E_n \left[\begin{array}{cc}
    \epsilon_i^2 \mathbf{w}_i \mathbf{w}_i' &  \epsilon_i^2 \mathbf{w}_i\mathbf{z}_i '\\
    \epsilon_i^2 \mathbf{z}_i \mathbf{w}_i'& \epsilon_i^2 \mathbf{z}_i \mathbf{z}_i'
    \end{array}\right]$ exists and is both finite and positive definite.
  \end{enumerate}
  Then, 
    $$\frac{1}{\sqrt{n}} \sum_{i=1}^n \left[\begin{array}{c} \epsilon_i \mathbf{w}_i \\ \epsilon_i \mathbf{z}_i \end{array} \right] \overset{d}{\rightarrow} N\left(\left[\begin{array}{c} \mathbf{0} \\ \boldsymbol{\tau} \end{array} \right], \Omega\right)$$
\end{lem}

\begin{proof}
We proceed by verifying the conditions of the Lindeberg-Feller Central Limit Theorem (van der Vaart 2.27) for the sequence of random vectors $Y_{ni} = n^{-1/2}\left(\epsilon_i \mathbf{w}_i', \epsilon_i \mathbf{z}_i' \right)'$. Since each row of the triangular array contains $n$ iid observations, 
  $$\lim_{n\rightarrow \infty}\sum_{i=1}^{k_n} Var_n(Y_{ni}) = \lim_{n\rightarrow \infty} \frac{1}{n} \sum_{i=1^n} Var_n(Y_{ni})= \lim_{n\rightarrow \infty} Var_n(Y_{ni})$$
and under the local mis-specification assumption,
  \begin{eqnarray*}
  Var_n(Y_{ni}) = E_n \left[\begin{array}{cc}
    \epsilon_i^2 \mathbf{w}_i \mathbf{w}_i' &  \epsilon_i^2 \mathbf{w}_i\mathbf{z}_i '\\
    \epsilon_i^2 \mathbf{z}_i \mathbf{w}_i'& \epsilon_i^2 \mathbf{z}_i \mathbf{z}_i'
    \end{array}\right] - \left[\begin{array}{cc}
    \mathbf{0} &  \mathbf{0}\\
    \mathbf{0}& \boldsymbol{\tau}\boldsymbol{\tau}'/n
    \end{array}\right] \rightarrow \Omega.
  \end{eqnarray*}
Similarly, the sum from the Lindeberg Condition simplifies to 
  $$\sum_{i=1}^{k_n} E\left[ \left|Y_{ni} \right|^2 \mathbf{1}\left\{\left|Y_{ni} \right| >\xi \right\}\right]= E_n \left[\epsilon_i^2 \left(\left|\mathbf{w}_i \right|^2 + \left| \mathbf{z}_i\right|^2 \right) \mathbf{1} \left\{A_n\right\}\right] \rightarrow 0$$
as $n \rightarrow \infty$ for any $\xi>0$, where
$$A_n = \left\{\left|\epsilon_i\right| \left( \left|\mathbf{w}_i \right|^2 + \left| \mathbf{z}_i\right|^2\right)^{1/2}  > n^{1/2}\xi\right\}$$ 
Now, by H\"{o}lder's Inequality followed by Minkowski's Inequality (White 3.4, 3.11)
  \begin{eqnarray*}
    E_n \left[\epsilon_i^2 \left(\left|\mathbf{w}_i \right|^2 + \left| \mathbf{z}_i\right|^2 \right) \mathbf{1} \left\{A_n\right\}\right] &=& E_n \left[\left(\left|\epsilon_i\mathbf{w}_i \right|^2 + \left|\epsilon_i \mathbf{z}_i\right|^2 \right) \mathbf{1} \left\{A_n\right\}\right]\\
    &\leq&E_n \left[\left|\left|\epsilon_i\mathbf{w}_i \right|^2 + \left|\epsilon_i \mathbf{z}_i\right|^2 \right|^p\right]^{1/p} E_n\left[\left|\mathbf{1}\{A_n\} \right|^q \right]^{1/q}\\
    &=& E_n \left[\left|\left|\epsilon_i\mathbf{w}_i \right|^2 + \left|\epsilon_i \mathbf{z}_i\right|^2 \right|^p\right]^{1/p} P(A_n)^{1/q}\\
    &\leq& \left(E_n \left[\left|\epsilon_i \mathbf{w}_i\right|^{2p} \right]^{1/p} + E_n\left[ \left|\epsilon_i \mathbf{z}_i\right|^{2p}\right]^{1/p}\right)P(A_n)^{1/q}
  \end{eqnarray*}
provided that $p>1$, $1/p + 1/q = 1$ and all the relevant moments exist. By the Generalized Chebyshev Inequality (White 2.41) 
    \begin{eqnarray*}
      P(A_n) &\leq& \left( \frac{1}{n\xi^2}\right) E_n\left[\epsilon_i^2 \left(\left|\mathbf{w}_i \right|^2 + \left|\mathbf{z}_i \right|^2\right) \right]\\
      &=& \left( \frac{1}{n\xi^2}\right) E_n\left[\left|\epsilon_i \mathbf{w}_i \right|^2 + \left| \epsilon_i \mathbf{z}_i \right|^2\right] \leq \frac{2C}{n\xi^2}
    \end{eqnarray*}
Combining these and taking $p = 1+\eta$, $q = (1+\eta)/\eta$, we have
  \begin{eqnarray*}
  E_n \left[\epsilon_i^2 \left(\left|\mathbf{w}_i \right|^2 + \left| \mathbf{z}_i\right|^2 \right) \mathbf{1} \left\{A_n\right\}\right] &\leq& 2C^{\frac{1}{1+\eta}} \left(\frac{2C}{n\xi^2} \right)^{\frac{\eta}{\eta+1}} \rightarrow 0.
  \end{eqnarray*}
Therefore, by the Lindeberg-Feller Central Limit Theorem,
  $$\frac{1}{\sqrt{n}} \sum_{i=1}^n \left[\begin{array}{c} \epsilon_i \mathbf{w}_i \\ \epsilon_i \mathbf{z}_i - \boldsymbol{\tau}/\sqrt{n}\end{array} \right] = \left(\frac{1}{\sqrt{n}} \sum_{i=1}^n \left[\begin{array}{c} \epsilon_i \mathbf{w}_i \\ \epsilon_i \mathbf{z}_i \end{array} \right] - \left[\begin{array}{c}\mathbf{0}\\ \boldsymbol{\tau} \end{array} \right] \right)\overset{d}{\rightarrow} N(\mathbf{0}, \Omega).$$
\end{proof}


\section{Derivations for OLS vs.\ IV Example}
Without loss of generality, we may assume that there are no exogenous regressors or, equivalently, that they have been ``projected out.'' Thus, the DGP is: 
    	\begin{eqnarray}
			y_{i} &=& \beta x_{i}  + \epsilon_{i}\\
	x_{i} &=& \mathbf{z}_{i}' \boldsymbol{\pi} + v_{i}
		\end{eqnarray}
and the local mis-specification assumption becomes
  \begin{equation}
    E_n \left[\begin{array}{c} \mathbf{z}_i \epsilon_i \\ x_i \epsilon_i \end{array}\right] = \left[\begin{array}{c} \mathbf{0} \\ \tau/\sqrt{n} \end{array}\right].
  \end{equation}
Stacking observations,
\begin{eqnarray}
    \mathbf{y} &=& \mathbf{x}\beta + \boldsymbol{\epsilon}\\
    \mathbf{x} &=& Z\boldsymbol{\pi} + \mathbf{v}
\end{eqnarray}
where $Z' = (\mathbf{z}_{1}, \hdots, \mathbf{z}_{n}), \mathbf{x}' = (x_{1}, \hdots, x_{n})$ and so on. We consider two estimators of the scalar $\beta$: the ordinary least squares (OLS) estimator $\widehat{\beta}$ and the Generalized Instrumental Variable (GIV) estimator $\widetilde{\beta}$
  \begin{eqnarray}
		\widehat{\beta} &=& \left(\mathbf{x}'\mathbf{x}\right)^{-1}\mathbf{x}'\mathbf{y}\\
		\widetilde{\beta} &=& \left(\mathbf{x}'Z W_n Z'\mathbf{x}\right)^{-1}\mathbf{x}'Z W_nZ'\mathbf{y}
	\end{eqnarray}
where $W_n$ is a positive definite weighting matrix. To estimate $\tau$, we plug the GIV estimator into the OLS moment conditions and multiply by $\sqrt{n}$ as follows:
  \begin{equation}
     \widehat{\tau} = \sqrt{n} \left[\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})/n\right] = n^{-1/2}\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})
   \end{equation} 


\begin{thm}
\label{thm:OLSIV}
Suppose that
  \begin{enumerate}[(a)]
    \item The DGP with triangular array.
    \item Local Mis-specification.
    \item Fourth and a bit moments of everything.
    \item $W_n \overset{p}{\rightarrow} W$, a positive definite matrix.
    \item Limit of variance for $v_i$ exists and is positive.
    \item The limit $\lim_{n\rightarrow \infty} E_n[\mathbf{z}_i \mathbf{z}_i'] = Q_z$ exists and is positive definite
    \item The instruments $\mathbf{z}_i$ are relevant:  $|\boldsymbol{\pi}|>0$
  \end{enumerate}
  Then,
  $$
\left[
\begin{array}{c}
  \sqrt{n}\left(\widehat{\beta} - \beta\right) \\
  \sqrt{n}\left(\widetilde{\beta} - \beta\right)\\
  \widehat{\tau}
\end{array}
\right] \overset{d}{\rightarrow} 
\left[\begin{array}{cc}
\widehat{K} & \mathbf{0}'\\
  0 & \widetilde{K}\\
  1 & -\widehat{K}^{-1} \widetilde{K}
  \end{array}
  \right]\left( \left[\begin{array}{c} \tau \\ \mathbf{0}  \end{array} \right] + M\right)
$$
where $M \sim \mathcal{N}(0, \Omega)$ and
  \begin{eqnarray}
 \widehat{K}&=&   \left(\boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2\right)^{-1}\\
    \widetilde{K}&=& \left(\boldsymbol{\pi}'Q_z W Q_z \boldsymbol{\pi} \right)^{-1}\boldsymbol{\pi}'Q_z W\\
    \Omega &=& \lim_{n\rightarrow \infty} E_n \left[
    \begin{array}{cc}
      \epsilon_i^2 x_i^2 & \epsilon_i^2 x_i \mathbf{z}_i' \\
      \epsilon_i^2 x_i \mathbf{z}_i& \epsilon_i^2 \mathbf{z}_i \mathbf{z}_i'
    \end{array}
    \right]
  \end{eqnarray}
\end{thm}

\begin{proof}
Substituting the DGP and rearranging, $\sqrt{n}\left( \widehat{\beta} - \beta\right) = \widehat{K}_n \boldsymbol{m}_n$ and similarly
$\sqrt{n} \left( \widetilde{\beta} - \beta\right) = \widetilde{K}_n \boldsymbol{m}_n$, where
  \begin{eqnarray*}
    \widehat{K}_n&=&  \left(\mathbf{x}'\mathbf{x}/n\right)^{-1}\\
    \widetilde{K}_n&=&\left[\left(\mathbf{x}'Z/n\right) W_n \left(Z'\mathbf{x}/n\right)\right]^{-1}\left(\mathbf{x}'Z/n\right) W_n \\
  \boldsymbol{m}_n &=& \left[
\begin{array}{c}
\mathbf{x}'\boldsymbol{\epsilon}/\sqrt{n}\\
Z'\boldsymbol{\epsilon}/\sqrt{n}
\end{array}
\right]
\end{eqnarray*}
Moreover,
  \begin{eqnarray*}
   \widehat{\tau} &=& \sqrt{n} \left[\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})/n\right] =\mathbf{x}'\boldsymbol{\epsilon}/\sqrt{n} -  (\mathbf{x}'\mathbf{x}/n)\sqrt{n}(\widetilde{\beta} - \beta) \\
        &=& \left[\begin{array}{cc} 1 \quad &\widehat{K}_n^{-1}\widetilde{K}_n\end{array}\right]\boldsymbol{m}_n
  \end{eqnarray*}
Since
  \begin{eqnarray*}
    \frac{\mathbf{x}'\mathbf{x}}{n} &=& \boldsymbol{\pi}'\left(\frac{Z'Z}{n}\right) \boldsymbol{\pi} + \left(\frac{\mathbf{v}'Z}{n}\right)\boldsymbol{\pi} +\boldsymbol{\pi}' \left(\frac{Z'\mathbf{v}}{n} \right) + \frac{\mathbf{v}'\mathbf{v}}{n}\\
      \frac{\mathbf{x}'Z}{n}&=&  \boldsymbol{\pi}' \left(\frac{Z'Z}{n}\right) + \frac{\mathbf{v}'Z}{n}
  \end{eqnarray*}
it suffices to examine $Z'Z/n$, $Z'\mathbf{v}/n$ and $\mathbf{v}'\mathbf{v}/n$. Because the triangular array is iid in each row, uniformly bounded fourth moments are sufficient for $L_2$ convergence (see e.g.\ Davidson 19.1) which implies convergence in probability. Thus,
  \begin{eqnarray*}
    Z'Z/n &\overset{p}{\rightarrow}& \lim_{n\rightarrow \infty} E_n[\mathbf{z}_i \mathbf{z}_i'] = Q_z \\
    Z' \mathbf{v}/z &\overset{p}{\rightarrow}& \lim_{n\rightarrow \infty} E_n[\mathbf{z}_i v_i] = 0\\
    \mathbf{v}'\mathbf{v}/n &\overset{p}{\rightarrow}& \lim_{n \rightarrow \infty} E_n[v_i^2] = \sigma^2_v 
  \end{eqnarray*}  
It follows that $\widehat{K}_n\overset{p}{\rightarrow} \widehat{K}$ and $\widetilde{K}_n \overset{p}{\rightarrow} \widetilde{K}$. Finally, we apply Lemma \ref{lem:CLT} to $\boldsymbol{m}_n$. The required bounds $E_n\left[\left| \epsilon_i \mathbf{z}_i \right|^{2+\eta}\right] <C$ and $E_n\left[\left| \epsilon_i x_i \right|^{2+\eta}\right] < C$ follow
from our assumptions on the moments of $v_i$, $\epsilon_i$ and $\mathbf{z}_i$ by Minkowski's Inequality and the Cauchy-Schwarz Inequality.
\end{proof}

\subsection{A Simplification - 2SLS}
Preceding result covers any iid setting. To get more intuition and compare to well-known procedures consider a simplification so that 2SLS is the efficient GIV estimator and OLS is fully efficient. 

\begin{cor}
Suppose that 
  \begin{enumerate}[(a)]
    \item $W_n =(Z'Z/n)^{-1}$
    \item $\sigma_\epsilon^2 = \lim_{n \rightarrow \infty} E_n[\epsilon_i^2]$ exists and is positive
    \item $\displaystyle \Omega =\sigma_\epsilon^2 \left(\lim_{n\rightarrow \infty} E_n \left[
    \begin{array}{cc}
      x_i^2 & x_i \mathbf{z}_i' \\
       x_i \mathbf{z}_i& \mathbf{z}_i \mathbf{z}_i'
    \end{array}
    \right]\right)$
  \end{enumerate}
Then, under the conditions of Theorem \ref{thm:OLSIV}
$$
\left[
\begin{array}{c}
  \sqrt{n}\left(\widehat{\beta} - \beta\right) \\
  \sqrt{n}\left(\widetilde{\beta} - \beta\right)\\
  \widehat{\tau}
\end{array}
\right] \overset{d}{\rightarrow}
\mathcal{N}\left(
\left[
\begin{array}{c}
\tau/\sigma_x^2 \\ 
0\\
\tau
\end{array}
\right],
\sigma_\epsilon^2 \left[ \begin{array}{ccc}
  1/\sigma_x^2 & 1/\sigma_x^2 & 0\\
  1/\sigma_x^2 & 1/\gamma^2 & 1 - \sigma_x^2/\gamma^2\\
  0& 1 - \sigma_x^2/\gamma^2 & \sigma_x^2(\sigma_x^2/\gamma^2 - 1)
  \end{array}\right]
  \right)
$$
where $\sigma_x^2 = \gamma^2 + \sigma_v^2$ and $\gamma^2 = \boldsymbol{\pi}'Q_z \boldsymbol{\pi}$.
\end{cor}
Note that, since $\sigma_x^2/\gamma^2 = 1 + \sigma_v^2 /\gamma^2$, the covariance between $\sqrt{n}\left(\widetilde{\beta} - \beta\right)$ and $\widehat{\tau}$ in the limit is \emph{negative}. In particular, it equals $\sigma_v^2/\gamma^2$. Hence, the weaker the instruments, the more negative the covariance becomes.
\begin{proof}
First,
$$\Omega = \sigma_\epsilon^2\left[ \begin{array}{cc}
\sigma_x^2
&\boldsymbol{\pi}'Q_z\\
  Q_z \boldsymbol{\pi}&Q_z
  \end{array}\right] = \sigma_\epsilon^2 V$$
  Since $W_n = (Z'Z/n)^{-1}\overset{p}{\rightarrow} Q_z^{-1}$, we have $\widetilde{K} = \left(\boldsymbol{\pi}'Q_z \boldsymbol{\pi}\right)^{-1}\boldsymbol{\pi}'$. 
Defining
  $$\Sigma = \left[\begin{array}{cc}
\widehat{K} & \mathbf{0}'\\
  \mathbf{0} & \widetilde{K}\\
  1 & -\widehat{K}^{-1} \widetilde{K}
  \end{array}
  \right] = \left[\begin{array}
    {cc}
    1/\sigma_x^2 & \textbf{0}' \\
    0 & \boldsymbol{\pi}'/\gamma^2\\
    1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}\right]$$
we have,
  \begin{eqnarray*}
    \sigma_\epsilon^2 \Sigma V \Sigma' &=&
    \sigma_\epsilon^2 
    \left[\begin{array}
    {cc}
    1/\sigma_x^2 & \textbf{0}' \\
    0 & \boldsymbol{\pi}'/\gamma^2\\
    1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}\right]
    \left[ \begin{array}{cc}
\sigma_x^2
&\boldsymbol{\pi}'Q_z\\
  Q_z \boldsymbol{\pi}&Q_z
  \end{array}\right] 
    \left[
    \begin{array}{ccc}
      1/\sigma_x^2 & 0 & 1 \\
      \mathbf{0} & \boldsymbol{\pi}/\gamma^2 & (-\sigma_x^2/\gamma^2)\boldsymbol{\pi}
    \end{array}
    \right]\\
   &=&\sigma_\epsilon^2 
    \left[\begin{array}
    {cc}
    1/\sigma_x^2 & \textbf{0}' \\
    0 & \boldsymbol{\pi}'/\gamma^2\\
    1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}\right]\left[ 
    \begin{array}
      {ccc}
      1 & 1 & 0 \\
      Q_z \boldsymbol{\pi}/\sigma_x^2 & Q_z \boldsymbol{\pi}/\gamma^2 & Q_z \boldsymbol{\pi}(1 - \sigma_x^2/\gamma^2)
    \end{array}
  \right] \\
  &=& \sigma_\epsilon^2 \left[ \begin{array}{ccc}
  1/\sigma_x^2 & 1/\sigma_x^2 & 0\\
  1/\sigma_x^2 & 1/\gamma^2 & 1 - \sigma_x^2/\gamma^2\\
  0& 1 - \sigma_x^2/\gamma^2 & \sigma_x^2(\sigma_x^2/\gamma^2 - 1)
  \end{array}\right].
    \end{eqnarray*}
\end{proof}

\subsection{Infeasible FMSC: OLS versus 2SLS}
We see that the variance of the OLS estimator is always strictly lower than that of the 2SLS estimator since $\sigma^2_\epsilon/\sigma_x^2 = \sigma^2_\epsilon/(\gamma^2 + \sigma_v^2)$. The AMSE of the OLS and 2SLS estimators takes a particularly simple form:
  \begin{eqnarray}
  \mbox{AMSE(OLS)} &=& \frac{\tau^2}{\sigma_x^4} + \frac{\sigma_\epsilon^2}{\sigma_x^2}\\
  \mbox{AMSE(2SLS)} &=& \frac{\sigma_\epsilon^2}{\gamma^2}
  \end{eqnarray}
where $\sigma_x^2 = \gamma^2 + \sigma_v^2$ and $\gamma^2 = \boldsymbol{\pi}'Q_z \boldsymbol{\pi}$. The AMSE of the OLS estimator is lower than that of the OLS estimator when
  \begin{eqnarray*}
    \frac{\tau^2}{\sigma_x^4} + \frac{\sigma_\epsilon^2}{\sigma_x^2} &<& \frac{\sigma_\epsilon^2}{\gamma^2}\\
    \frac{\tau^2}{\sigma_\epsilon^2\sigma_x^2} + 1&<&\frac{\sigma_x^2}{\gamma^2}\\
    \tau^2 &<&\sigma_\epsilon^2\sigma_x^2\left(\frac{\sigma_x^2}{\gamma^2} - 1\right)\\
        \tau^2  &<& \sigma_x^2 \sigma_\epsilon^2\left(\frac{\sigma_x^2 - \gamma^2}{\gamma^2}\right)\\
        \tau^2  &<& \sigma_x^2 \sigma_\epsilon^2\left(\frac{\sigma_v^2}{\gamma^2}\right)\\
                \tau^2  &<& \sigma_v^2 \sigma_\epsilon^2\left(\frac{\sigma_x^2}{\gamma^2}\right)\\
              |\tau|  &<& \sigma_v \sigma_\epsilon\sqrt{\frac{\boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}}\\
              |\tau|  &<& \sigma_v \sigma_\epsilon \sqrt{1+ \frac{1}{\kappa^2}}
  \end{eqnarray*}
where $\kappa^2 = (\boldsymbol{\pi}'Q_z \boldsymbol{\pi})/\sigma_v^2$, is the signal-to-noise ratio in the first stage. We see that the FMSC trades the endogeneity of $x$, as measured by $\tau$, against the strength of the instruments, as measured by the concentration parameter $\kappa^2$. 


To get a better sense of the form of this trade-off, we can ``re-interpret'' this cutoff \emph{as though} it were finite-sample rule. Recall that we defined $E_n[x_i\epsilon_i]= \tau/\sqrt{n}$. Hence,
  $$\tau = \sqrt{n} E_n[x_i \epsilon_i] = \sqrt{n}E_n\left[\left(\mathbf{z}_i' \boldsymbol{\pi} + v_i\right)\epsilon_i \right] = \sqrt{n} E_n[v_i\epsilon_i] = \sqrt{n} Cor_n(v_i, \epsilon_i) \sigma_v \sigma_\epsilon$$
provided that we do not envision $\sigma_v$ and $\sigma_\epsilon$ changing with sample size. Thus, for a \emph{fixed} sample size $n$, we can think of $\tau$ as the correlation between $\epsilon_i$ and $v_i$ scaled by $\sqrt{n}$ and their respective standard deviations. Dropping the $n$ subscript on the correlation and substituting into the preceding inequality, we have
  $$|\rho|  < \sqrt{\frac{1}{n}\left(1 + \frac{1}{\kappa^2}\right)}$$
where $\rho$ is the finite sample correlation between $v_i$ and $\epsilon_i$. Thus, interpreted as a finite sample rule, the FMSC tells us to weigh the endogeneity of $x_i$ as measured by $\rho$ against the sample size and the concentration parameter $\kappa^2$. We should only use the 2SLS estimator when the concentration parameter is sufficiently large, the sample size is sufficiently large, or $x$ is sufficiently endogenous. The following figure depicts the region in which OLS is favored for different sample sizes.

\begin{figure}
  \caption{OLS and IV Figure goes here. R code commented out in this document.}
\end{figure}
% <<OLSboundary, dev = "tikz", echo=FALSE, cache=TRUE, fig.width=5, fig.height=5>>=
% boundary.plot <- function(n, x.max = 2, y.max = 1){
%   step <- x.max/400
%   start <- 0 + step
%   x <- seq(from = start, to = x.max, by = step)
%   y <- sqrt((1/n) * (1 + 1/x))
%   plot(x,y, type = 'l', xlim = c(0,x.max), ylim = c(0, y.max) ,xaxs = "i", yaxs = "i", pty = "s", ylab = "$|\\rho_{\\epsilon v}|$", xlab =  "$\\kappa^2$")
  
%   poly <- cbind(c(0, x, x.max), c(0, y, 0))
%   polygon(poly, density = 20)
          
%   text(x.max/2, 0.8 * y.max, cex = 1.5, bquote("$n$" == .(n)))
% }

% par(mfrow = c(2,2), mgp=c(2.2,0.45,0), tcl=-0.4, mar=c(3.3,3.6,1.1,1.1))

% boundary.plot(10, x.max = 2, y.max = 1)
% boundary.plot(100, x.max = 1, y.max = 1)
% boundary.plot(500, x.max = 0.5, y.max = 1)
% boundary.plot(1000, x.max = 0.25, y.max = 1)
% @


As the sample size grows, the boundary moves towards the origin, meaning that we are more likely to use 2SLS. However, each of regions asymptotes at the origin so that when instruments are weak, we choose OLS. This makes intuitive sense. Notice that we change the horizontal axis limits to make it easier to see the threshold for weak instruments depending on sample size.

\paragraph{Alternative Parameterization}
Above we used $\kappa^2 = (\boldsymbol{\pi}'Q_z \boldsymbol{\pi})/\sigma_v^2$ to encode the strength of the instruments. The boundary region between OLS and IV turned out to depend on $1 + 1/\kappa^2$. Another way to express this is as follows:
  \begin{eqnarray*}
    1 + \frac{1}{\kappa^2} &=& 1 + \frac{\sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}} = \frac{ \boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2}{\boldsymbol{\pi'}Q_z \boldsymbol{\pi}} = \frac{1}{R^2}
  \end{eqnarray*}
where $R^2$ is the population coefficient of determination in the first-stage regression. Using this convention, the finite-sample version of the boundary from above becomes
    $$|\rho|  < \sqrt{\frac{1}{nR^2} }$$
which is a much simpler expression. The only potential problem with expressing things in this fashion is that we cannot really vary the population $R^2$ completely independently from the correlation between first and second stage errors. I have to think about this some more...

\paragraph{Yet Another Parameterization} Should we parameterize things in terms of the correlation between $x$ and $\epsilon$ or in terms of $\epsilon$ and $v$ as above?

\todo[inline]{Need to think carefully about the finite sample version of this.}


\subsection{Feasible FMSC} % (fold)
\label{sub:feasible_fmsc}
In the previous section we found that, under local mis-specification, the AMSE of the OLS estimator is lower than that of the IV estimator whenever 
    $$\frac{\tau^2}{\sigma_v^2 \sigma_\epsilon^2}  < \frac{\boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}} = \frac{\sigma_x^2}{\gamma^2} $$
% subsection feasible_fmsc (end)
To use the FMSC we substitute estimators of all the quantities in the preceding expression. For the right-hand side we use the familiar estimators which remain consistent under local mis-specification
  \begin{eqnarray*}
     \widehat{\sigma}_x^2 &=& n^{-1}\mathbf{x}' \mathbf{x} \overset{p}{\rightarrow} \boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2\\
      \widehat{\gamma}^2 &=& n^{-1}\mathbf{x}' Z (Z'Z)^{-1}Z' \mathbf{x} \overset{p}{\rightarrow} \boldsymbol{\pi}'Q_z \boldsymbol{\pi} 
  \end{eqnarray*}
as we saw in the proofs above. The difference of the two provides a consistent estimator of $\sigma_v^2$ and we see that this is equivalent to using the residuals from the first-stage regression:
  \begin{eqnarray*}
     \widehat{\sigma}_v^2 &=& \widehat{\sigma}_x^2 - \widehat{\gamma}^2 \\
     &=&n^{-1}\mathbf{x}' \mathbf{x}  - n^{-1}\mathbf{x}' Z (Z'Z)^{-1}Z' \mathbf{x}\\
      &=& n^{-1} \mathbf{x}' \left(I - Z(Z'Z)^{-1}Z' \right)\mathbf{x}\\
    &=& n^{-1} \mathbf{x}' (I - P_Z)\mathbf{x}\\
    &=& n^{-1} \mathbf{x}' (I-P_Z)'(I - P_Z)\mathbf{x}\\
    &=& (M_Z \mathbf{x})'(M_Z \mathbf{x})/n\\
    &=& \widehat{v}'\widehat{v}/n \overset{p}{\rightarrow} \sigma_v^2
 \end{eqnarray*}
 since $P_Z$ is an orthogonal projection, hence symmetric and idempotent. (Remember that we're talking about the first-stage here: $\mathbf{x}$ is the dependent variable.) We could also divide by $(n - p)$ rather than $n$ where $p$ is the number of instrumental variables. 

 There are two possible ways to estimate $\sigma_\epsilon^2$. We can either use the redsiduals from the OLS estimator or those from the IV estimator. Under local mis-specification, either choice gives a consistent estimator. Unless the instruments are extremely weak, however, we would expect the IV residuals to be more robust. (We could always try both and see if it makes a difference.)  

 The only remaining quantity to be estimated is $\tau^2$. From above we know that $\widehat{\tau} \overset{d}{\rightarrow} N(\tau, V)$ where
    $$V = \sigma^2_\epsilon \sigma^2_x \left(\frac{\sigma^2_x}{\gamma^2} - 1 \right)  = \sigma^2_\epsilon \sigma^2_x \left(\frac{\sigma^2_v + \gamma^2}{\gamma^2} - 1 \right) =  \sigma^2_\epsilon \sigma^2_x \left( \frac{\sigma^2_v}{\gamma^2}\right)$$
Therefore,
  $$\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} - 1 \right)= \widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2 \left(\frac{\widehat{\sigma}_v^2}{\widehat{\gamma}^2}\right)$$
is an asymptotically unbiased estimator of $\tau^2$ and accordingly
  $$\frac{\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2 \widehat{\sigma}_v^2/\widehat{\gamma}^2}{\widehat{\sigma}_\epsilon^2 \widehat{\sigma}_v^2} = \frac{\widehat{\tau}^2}{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2} - \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} =  \frac{\widehat{\tau}^2}{(\widehat{\sigma}_x^2 - \widehat{\gamma}^2) \widehat{\sigma}_\epsilon^2} - \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2}$$
is an asymptotically unbiased estimator of $\tau^2/(\sigma_\epsilon^2 \sigma_v^2)$.

Now we can construct the FMSC for the OLS versus IV example. From above we know that the AMSE of OLS is lower than that of IV whenever $\tau^2/(\sigma_\epsilon^2 \sigma_v^2)$ is less than $\sigma_x^2/\gamma^2$. Substituting our estimators of these quantities into the inequality, the FMSC tells us to use OLS whenever
  \begin{eqnarray*}
       \frac{\widehat{\tau}^2}{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2} - \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} &<& \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2}\\
       \frac{\widehat{\tau}^2}{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2} &<& 2 \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} \\
        \widehat{T}_{FMSC} = \frac{\widehat{\tau}^2 \widehat{\gamma}^2}{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2} &<& 2
  \end{eqnarray*}
Now, the question is what is the limiting behavior of $\widehat{T}_{FMSC}$? Note that
  \begin{eqnarray*}
    \widehat{T}_{FMSC} &=& \frac{\widehat{\tau}^2}{\widehat{V}} = \frac{\widehat{\tau}^2 }{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2/\widehat{\gamma}^2} = \frac{\widehat{\tau}^2}{ \widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right)}
  \end{eqnarray*}
Hence, if $\tau = 0$ then $\widehat{T}_{FMSC} \overset{d}{\rightarrow} \chi^2(1)$. This means that we can interpret the FMSC as a test of the null hypothesis $H\colon \tau = 0$ against the two-sided alternative based on a critical value of $2$. This corresponds to a test with size  \texttt{1 - pchisq(2, df = 1)}$\approx 0.16$. Rather than leaving us free to choose the size of our test, the FMSC tells us precisely what size to use to obtain a favorable bias-variance tradeoff. We should use IV only when we reject $H_0\colon \tau = 0$ at the 16\% level. 

\paragraph{Relationship to Durbin-Hausman-Wu Test} In applied work researchers often use a Hausman test to choose between OLS and IV. The test is based on a quadratic form in the difference between the OLS and IV estimators. Using the notation of the present example, 
  \begin{eqnarray*}
    \sqrt{n}\left(\widehat{\beta} - \widetilde{\beta}\right) &=&\left[\begin{array}{cc}
    1 & -1
    \end{array}\right] \sqrt{n} \left(\begin{array}{c}
    \widehat{\beta} - \beta  \\ \widetilde{\beta} - \beta
    \end{array} \right)\\
     &\overset{d}{\rightarrow}& \left[\begin{array}{cc}
    1 & -1
    \end{array}\right]
    N\left(\left[
      \begin{array}{c}
        \tau/\sigma_x^2 \\ 0
      \end{array}\right],\sigma_\epsilon^2\left[
        \begin{array}{cc}
          1/\sigma_x^2 & 1/\sigma_x^2 \\
          1/\sigma_x^2 & 1/\gamma^2
        \end{array}
      \right]
    \right)\\
    &\sim& N\left(\tau/\sigma_x^2, \Sigma
    \right)
  \end{eqnarray*}
where
    \begin{eqnarray*}
      \Sigma &=& (\sigma_\epsilon^2/\sigma_x^2) \left[\begin{array}{cc}
    1 & -1
    \end{array}\right]
      \left[
        \begin{array}{cc}
          1&1\\
          1&\sigma_x^2/\gamma^2
        \end{array}
      \right]
    \left[\begin{array}{c}
    1 \\ -1
    \end{array}\right]\\
        &=& (\sigma_\epsilon^2/\sigma_x^2)(\sigma_x^2/\gamma^2 - 1) = \sigma_\epsilon^2/\gamma^2 - \sigma_\epsilon^2/\sigma_x^2 \\
        &=& \mbox{AVAR(2SLS)} - \mbox{AVAR(OLS)}\\
        &=& \sigma_\epsilon^2 \left(1/\gamma^2 - 1/\sigma_x^2 \right)
    \end{eqnarray*}
which agrees with the standard textbook formula in the case where OLS is efficient under the null (see e.g.\ Cameron \& Trivedi). Hence,
  \begin{eqnarray*}
    \widehat{T}_{Hausman} &=& \frac{n(\widehat{\beta} - \widetilde{\beta})^2}{\widehat{\Sigma}} = \frac{n(\widehat{\beta} - \widetilde{\beta})^2}{\widehat{\sigma}_\epsilon^2\left(1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2\right)}\overset{d}{\rightarrow} \chi^2(1)
  \end{eqnarray*}
under $H_0\colon \tau = 0$. So long as we use the same estimator of $\sigma_\epsilon^2$ for each term in the denominator, as we have here, we can be sure that the difference is positive since $\widehat{\sigma}_x^2 = \widehat{\gamma}^2 + \widehat{\sigma}_v^2$ and we showed above that $\widehat{\sigma}_v^2 = (M_Z \mathbf{x})'(M_Z \mathbf{x})/n$.


Now we will show that the test statistic for the Durbin-Hausman-Wu test is \emph{numerically identical} to the ``test statistic'' view of the FMSC for this problem. From above,
  $$\widehat{T}_{FMSC} =  \frac{\widehat{\tau}^2}{ \widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right)} = \left(\frac{\widehat{\tau}}{\widehat{\sigma}_x^4}\right)\left[\frac{1}{\widehat{\sigma}_\epsilon^2\left(1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2\right)} \right]$$
whereas,
  $$\widehat{T}_{Hausman} =\frac{n(\widehat{\beta} - \widetilde{\beta})^2}{\widehat{\sigma}_\epsilon^2\left(1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2\right)}$$
  Thus, we only need to show that $\widehat{\tau}^2/\widehat{\sigma}_x^4 = n(\widehat{\beta} - \widetilde{\beta})^2$. First notice that
\begin{eqnarray*}
   \mathbf{x}' \left(\mathbf{y} - \mathbf{x}\widetilde{\beta}\right)&=& \mathbf{x}'\mathbf{y} - \mathbf{x}'\mathbf{x}\widetilde{\beta} =\mathbf{x}'\mathbf{x}\left(\mathbf{x}'\mathbf{x}\right)^{-1} \mathbf{x}'\mathbf{y} - \mathbf{x}'\mathbf{x}\widetilde{\beta}\\
    &=&\mathbf{x}'\mathbf{x}\left[\left(\mathbf{x}'\mathbf{x}\right)^{-1} \mathbf{x}'\mathbf{y} -  \widetilde{\beta}\right] = \mathbf{x}'\mathbf{x} \left( \widehat{\beta} - \widetilde{\beta}\right)
\end{eqnarray*}
Now, from above
  $$\widehat{\tau} = \sqrt{n} \left[\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})/n\right] = n^{-1/2} \textbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})$$
so we have,
  \begin{eqnarray*}
    \widehat{\tau}^2/\widehat{\sigma}_x^4 &=&  \left[n^{-1/2} \textbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})\right]^2 = n^{-1}\left[\mathbf{x}'\mathbf{x} \left( \widehat{\beta} - \widetilde{\beta}\right) \right]^2/\widehat{\sigma}_x^4 \\
      &=& n^{-1}\left[n \widehat{\sigma}_x^2 \left( \widehat{\beta} - \widetilde{\beta}\right) \right]^2/\widehat{\sigma}_x^4  = n\left( \widehat{\beta} - \widetilde{\beta}\right) ^2
  \end{eqnarray*}
  Therefore $\widehat{T}_{FMSC} = \widehat{T}_{Hausman}$ as claimed. In other words, using the FMSC to choose between OLS and 2SLS is numerically equivalent to carrying out a DHW test at the 16\% level.


\paragraph{Implementing the FMSC via a Hausman Test}
We can use the relationship from above to implement the FMSC for this example directly. There is a potential issue involving the denominator, since this is a difference of asymptotic variance estimators. As formulated above, it is guaranteed to be positive since we use the \emph{same} sample estimator of $\sigma^2_\epsilon$ for each term of the difference in the denominator piece. (See 8.3.2 of Cameron \& Trivedi for more on this). 

There are several possible estimators for $\sigma_\epsilon^2$ as mentioned above. One is to use the OLS estimator, another is to use the 2SLS estimator. (See Dufour et al. for more possibilities.) It seems like the easiest thing to do would be to use the 2SLS estimator. I could also try the OLS estimator. Another idea would be to take the average of the two. It seems like much ink has been spilled on this topic but it's a bit of a distraction from this paper so I shouldn't spend too much time on it.

The point is this: once we have chosen an estimator of $\sigma_\epsilon^2$ we only need the following quantities to construct the DHW test statistic:
  \begin{eqnarray*}
    \widehat{\beta} &=& (\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y}\\
    \widehat{\sigma}_x^2 &=& n^{-1}\mathbf{x}'\mathbf{x} \\
    \widetilde{\beta} &=& (\mathbf{x}'Z(Z'Z)^{-1} Z'\mathbf{x})^{-1}\mathbf{x}'Z (Z'Z)^{-1} Z'\mathbf{y}\\
    \widehat{\gamma}^2 &=& n^{-1} \mathbf{x}'Z(Z'Z)^{-1}Z'\mathbf{x}
  \end{eqnarray*}
The FMSC is equivalent to carrying out this test at a particular significance level. Thus, we can easily compare it to some other significance levels and see how the performance differs. Coding this will be very straightforward: we don't even need to calculate $\widehat{\tau}$ explicitly. 

\todo[inline]{Should be fairly easy to write a function to implement this, along with the corrected confidence interval. Could give option to use various estimators for $\sigma_\epsilon^2$. Implement via model interface in R? Compare to \texttt{sem} or something like that? Just need to be careful about projecting out any exogenous regressors. This is numerically equivalent to doing things the hard way.}

\subsection{Robust Version of FMSC and Hausman Test?} % (fold)
\label{sub:robust_version_of_fmsc_and_hausman_test_}
\todo[inline]{Does the relationship continue to hold without the simplifying assumptions we used for the variance matrix?}
% subsection robust_version_of_fmsc_and_hausman_test_ (end)





\subsection{AMSE-Optimal Averaging} % (fold)
\label{sub:amse_optimal_averaging}
We showed above that the infeasible version of the FMSC selects OLS when 
  $$|\tau|  < \sigma_v \sigma_\epsilon\sqrt{\frac{\boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}}$$
But selection is a blunt instrument. A better idea is to \emph{combine} the OLS and 2SLS estimator by taking a weighted average and choose the weights to minimize AMSE. The problem is 
  $$\omega^* = \underset{\omega \in [0,1]}{\mbox{argmin }} \mbox{AMSE}\left(\omega \widehat{\beta} + (1-\omega)\widetilde{\beta}\right)  $$
  Let $\widehat{\beta}(\omega) = \omega \widehat{\beta} + (1-\omega) \widetilde{\beta}$. Then,
    \begin{eqnarray*}
      \mbox{Bias}\left(\widehat{\beta}(\omega)\right) &=& \mbox{Bias}(\widehat{\beta}) + \mbox{Bias}(\widetilde{\beta})=\omega\left( \frac{\tau}{\sigma_x^2}\right)
    \end{eqnarray*}
  and
  \begin{eqnarray*}
    Var\left(\widehat{\beta}(\omega)\right) &=& 
      \left[ \begin{array}{cc}
        \omega & 1-\omega
      \end{array}\right]
       \sigma_\epsilon^2 \left[
      \begin{array}{cc}
       1/\sigma_x^2 & 1/\sigma_x^2 \\
       1/\sigma_x^2 & 1/\gamma^2 
      \end{array}\right]\left[
      \begin{array}{c}
          \omega \\ 1- \omega
      \end{array}\right]\\
      &=& \sigma_\epsilon^2/\sigma_x^2 
      \left[ \begin{array}{cc}
        \omega & 1-\omega
      \end{array}\right]
          \left[\begin{array}{cc} 
          1& 1\\ 1& \sigma_x^2/\gamma^2
          \end{array} \right]
      \left[
      \begin{array}{c}
          \omega \\ 1- \omega
      \end{array}\right]\\
        &=&\frac{\sigma_\epsilon^2}{\sigma_x^2}\left\{ \omega + (1- \omega)\left[\omega + (1-\omega)\sigma_x^2/\gamma^2 \right]\right\} \\
        &=&\frac{\sigma_\epsilon^2}{\sigma_x^2}\left[ \omega + (1- \omega)\omega + (1-\omega)^2\sigma_x^2/\gamma^2 \right] \\
          &=& \frac{\sigma_\epsilon^2}{\sigma_x^2}\left[2\omega - \omega^2 + \frac{\sigma_x^2}{\gamma^2} \left(1 - 2\omega + \omega^2 \right) \right]\\
          &=& \frac{\sigma_\epsilon^2}{\sigma_x^2} \left[(2\omega - \omega^2)\left( 1 - \frac{\sigma_x^2}{\gamma^2}\right)+\frac{\sigma_x^2}{\gamma^2} \right]\\
          &=& \frac{\sigma_\epsilon^2}{\sigma_x^2} \left[(2\omega - \omega^2)\left( 1 - \frac{\sigma_x^2}{\gamma^2}\right)+\frac{\sigma_x^2}{\gamma^2} \right]
  \end{eqnarray*}
  and accordingly
    \begin{eqnarray*}
      \mbox{AMSE}\left(\widehat{\beta}(\omega)\right) &=& \omega^2 \left(\frac{\tau^2}{\sigma_x^4} \right) + (2\omega - \omega^2)\left(\frac{\sigma_\epsilon^2}{\sigma_x^2}\right)\left( 1 - \frac{\sigma_x^2}{\gamma^2}\right) + \frac{\sigma_\epsilon^2}{\gamma^2}\\
      &=& \omega^2 \left(\frac{\tau^2}{\sigma_x^4} \right) + (2\omega - \omega^2)\left(\frac{\sigma_\epsilon^2}{\sigma_x^2}\right)\left(\frac{\gamma^2 - \sigma_x^2}{\gamma^2}\right) + \frac{\sigma_\epsilon^2}{\gamma^2}\\
      &=& \omega^2 \left(\frac{\tau^2}{\sigma_x^4} \right) + (2\omega - \omega^2)\left(\frac{\sigma_\epsilon^2}{\sigma_x^2}\right)\left(\frac{-\sigma_v^2}{\boldsymbol{\pi}'Q_z\boldsymbol{\pi}}\right) + \frac{\sigma_\epsilon^2}{\gamma^2}\\
      &=& \omega^2 \left(\frac{\tau^2}{\sigma_x^4} \right) + (2\omega - \omega^2)\left(\frac{-\sigma_\epsilon^2}{\sigma_x^2\kappa^2}\right)+ \frac{\sigma_\epsilon^2}{\gamma^2}\\
        &=& a\omega^2 + b(2\omega - \omega^2) + c
    \end{eqnarray*}
  so the first order condition for a minimum is
    \begin{eqnarray*}
      2a\omega + b(2 - 2\omega) &=&0\\
      2(a-b) \omega + 2b &=& 0\\
      \omega^* &=& b/(b-a)\\
      \omega^* &=& \frac{1}{1 - a/b}
    \end{eqnarray*}
    Now, 
    \begin{eqnarray*}
       a &=&\frac{\tau^2}{\sigma_x^4} = \mbox{ABIAS(OLS)}^2 \\
       b &=& \frac{-\sigma_\epsilon^2}{\kappa^2 \sigma_x^2} = \sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(\frac{1}{\kappa^2}\right)\right] = - \sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(\frac{\sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}\right)\right] \\
          &=& - \sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(\frac{\sigma_x^2 - \boldsymbol{\pi}'Q_z \boldsymbol{\pi}}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}\right)\right]=-\sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(\frac{\sigma_x^2 }{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}-1\right)\right]=\sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(1-\frac{\sigma_x^2 }{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}\right)\right]\\
          &=&\sigma_\epsilon^2\left[  \frac{1}{\sigma_x^2}-\frac{1}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}\right] =  \frac{\sigma_\epsilon^2}{\sigma_x^2}-\frac{\sigma_\epsilon^2}{\gamma^2}  = \mbox{AVAR(OLS)}-\mbox{AVAR(2SLS)}
     \end{eqnarray*} 

  Substituting the definitions of $a$ and $b$ we have
    \begin{eqnarray*}
      \omega^* &=& \left[ 1 - \frac{a}{b}\right]^{-1}\\
      &=&\left[1 - \frac{\mbox{ABIAS(OLS)}^2}{\mbox{AVAR(OLS)}- \mbox{AVAR(2SLS)}} \right]^{-1}\\
      &=&\left[1 + \frac{\mbox{ABIAS(OLS)}^2}{\mbox{AVAR(2SLS)}-\mbox{AVAR(OLS)}} \right]^{-1}\\
      &=& \left[1 + \frac{\tau^2/\sigma_x^4}{\sigma_\epsilon^2(1/\gamma^2 - 1/\sigma_x^2)}\right]^{-1}
      = \left[1 + \frac{\tau^2}{\sigma_\epsilon^2 \sigma_x^4 (1/\gamma^2 - 1/\sigma_x^2)}\right]^{-1}\\
      &=& \left[1 + \frac{\tau^2}{\sigma_\epsilon^2 \sigma_x^2}\left( \frac{1}{\sigma_x^2(1/\gamma^2 - 1/\sigma_x^2)}\right)\right]^{-1}
      = \left[1 + \frac{\tau^2}{\sigma_\epsilon^2 \sigma_x^2}\left( \frac{1}{\sigma_x^2/\gamma^2 - 1}\right)\right]^{-1}\\
      &=& \left[1 + \frac{\tau^2}{\sigma_\epsilon^2 \sigma_x^2}\left( \frac{\gamma^2}{\sigma_x^2 - \gamma^2}\right)\right]^{-1}
      = \left[1 + \left|\frac{\tau}{\sigma_\epsilon \sigma_x}\right|^2\left( \frac{\gamma^2}{\sigma_v^2}\right)\right]^{-1}
      = \left[1 +\kappa^2 \left|\frac{\tau}{\sigma_\epsilon \sigma_x}\right|^2\right]^{-1}\\
      &=& \left[1 + \left|\frac{\tau}{\sigma_\epsilon \sigma_v}\right|^2\left( \frac{\gamma^2}{\sigma_x^2}\right)\right]^{-1}
      = \left[1 + R^2 \left|\frac{\tau}{\sigma_\epsilon \sigma_v}\right|^2\right]^{-1} = \frac{1}{1 + R^2 \rho_{\epsilon v}^2}\\
      &=& \left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-1}
    \end{eqnarray*}
    Notice that this agrees with my earlier derivations for the special case in which there is only one instrument since $\kappa^2 = R^2/(1 - R^2)$. The second order condition for a minimum is is $2(a-b) > 0$ which is equivalent to $a > b$. From above, $a$ is the squared asymptotic bias of the OLS estimator: $\tau^2/\sigma_x^4$. This is strictly postitive so long as $\tau \neq 0$. Also from above, $b$ is the difference between the asymptotic variance of the OLS estimator and the 2SLS estimator: $\sigma_\epsilon^2(1/\sigma_x^2 - 1/\gamma^2)$. Since $\sigma_x^2 = \gamma^2 + \sigma_v^2$, this quantity is strictly negative so long as $\sigma_v^2 \neq 0$. Thus, the second order condition holds except in the trivial case where the instruments are perfect predictors of $x$.
  \todo[inline]{The finite sample version of this equates $\tau^2/(\sigma_v^2 \sigma_\epsilon^2)$ with $\rho_{\epsilon v}^2$. There remains the question of which parameters can be varied independently and hence what the best way to express this is.}

Differentiating,
  \begin{eqnarray*}
    \frac{\partial \omega^*}{\partial \kappa^2} &=& -\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2} \left( \frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2} \right) \frac{d}{d\kappa^2} \left(\frac{\kappa^2}{1 + \kappa^2} \right)\\
      &=&  -\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2} \left( \frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2} \right)  \left(\frac{(1+ \kappa^2) - \kappa^2}{(1 + \kappa^2)^2} \right)\\
      &=& -\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2} \left( \frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2} \right)  \left(\frac{1}{(1 + \kappa^2)^2} \right) < 0\\
    \frac{\partial \omega^*}{\partial \tau^2} &=&-\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2}\left(\frac{\kappa^2}{1 + \kappa^2} \right)\frac{d}{d\tau^2}\left( \frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2} \right) \\
    &=&-\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2}\left(\frac{\kappa^2}{1 + \kappa^2} \right)\left( \frac{1}{\sigma_\epsilon^2 \sigma_v^2} \right) < 0 
  \end{eqnarray*}
The result makes intuitive sense. As $\kappa^2$ increases, the instruments $z$ become stronger so we give \emph{less} weight to OLS and more to 2SLS. As the $\kappa^2$ decreases, the instruments become weaker so we give \emph{more} weight to OLS and less to 2SLS. Similarly, as $\tau^2$ increases the regressor $x$ becomes more endogenous so we give \emph{less} weight to OLS and more to 2SLS. As $\tau^2$ decreases the regressor $x$ becomes less endogenous, so we give \emph{more} weight to OLS and less to 2SLS.

\paragraph{Is there an interior optimum?} To solve for $\omega^*$ we minimized the function 
    $$g(\omega) = a\omega^2 + b(2\omega - \omega^2) + c = (a - b)\omega^2 + 2b\omega + c$$ 
where $a = \mbox{ABIAS(OLS)}^2$ and $b = \mbox{AVAR(OLS) - AVAR(2SLS)}$. Since it is a squared bias, $a \geq 0$ and since OLS has a strictly lower asymptotic variance than 2SLS so long as the instruments aren't perfect predictors of $x$, $b \leq 0$. This implies that $(a - b)$ is \emph{positive}. Hence, the second derivative is strictly positive \emph{for all $x$} meaning that we are minimizing a \emph{globally convex function}, namely a quadratic. This means that the first order condition is both necessary and sufficient for a minimum. The only remaining question is whether the solution $\omega^* = 1/(1 - a/b)$ lies inside the interval $[0,1]$. Now, we'll assume that the instruments are \emph{not} perfect predictors of $x$ so the inequality for $b$ becomes strict, i.e.\ $b < 0$. Since $a \geq 0$, it follows that $-a/b \geq 0$ so that $1 - a/b \geq 1$. Hence,
$\omega^* = 1/(1 - a/b) \leq 1$. Since the denominator cannot be negative, we have $0 < \omega^* \leq 1$. Hence the optimum is \emph{always interior}.
 


% subsection amse_optimal_averaging (end)

\subsection{Feasible AMSE-Optimal Weighting} % (fold)
\label{sub:feasible_amse_optimal_weighting}
Since $\omega^*$ depends on unknowns, we need to estimate it. Recall from above that
  $$\omega^* = \left[1 + \frac{\mbox{ABIAS(OLS)}^2}{\mbox{AVAR(2SLS)}-\mbox{AVAR(OLS)}} \right]^{-1} = \left[1 + \frac{\tau^2/\sigma_x^4}{\sigma_\epsilon^2 (1/\gamma^2 - 1/\sigma_x^2)}\right]^{-1}$$ 
For $\sigma_x^2$, $\gamma^2$ and $\sigma_\epsilon^2$ we will use the obvious consistent estimators $\widehat{\sigma}_x^2$, $\widehat{\gamma}^2$ and $\widehat{\sigma}_\epsilon^2$. 
\todo[inline]{There is a question of whether $\sigma_\epsilon^2$ should be estimated based on the residuals constrcted from the OLS estimator or those constructed from the 2SLS estimator.}

For $\tau^2$ the situation is more complicated. As explained above, no consisted estimator of $\tau$ exsists although we can construct an asymptotically unbiased estimator of $\widehat{\tau}^2$. In the FMSC expression this yields an asymptotically unbiased estimator of AMSE. However here we will be substituting our estimator of $\tau^2$ into a \emph{highly nonlinear} function so there is no obvious way to preserve the asymptotic unbiasedness. Nevertheless, the most obvious idea is to substitute our asymptotically unbiased estimator of $\tau^2$ from above: 
$$\widehat{\tau}^2 - \widehat{V} = \widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} - 1 \right)$$
The problem is that, while $\tau^2$ is always greater than or equal to zero as is $\widehat{\tau}^2$, the difference $\widehat{\tau}^2 - \widehat{V}$ \emph{can easily be negative}. This gives a \emph{negative} estimate of $\mbox{ABIAS(OLS)}^2$ The obvious solution to this problem, borrowing an idea from James-Stein estimation, is to work with the positive part instead, namely $\max\left\{0, \; \widehat{\tau}^2 - \widehat{V}\right\}$. This will ensure that our estimate of $a$, using the notation of the preceding section, will satisfy the same inequalities as the population quantity ensuring that our estimate of $\omega^*$ is interior to $[0,1]$. (The estimator of $b$ automatically satisfies the corresponding inequalities for the population quantity). Our sample estimate of the AMSE-optimal weight on the OLS estimator is thus
  $$\widehat{\omega }^* = \left[1 + \frac{\max \left\{0, \; \left(\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right) \right)/\;\widehat{\sigma}_x^4 \right\}}{\widehat{\sigma}_\epsilon^2 (1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2)}\right]^{-1}$$

% subsection feasible_amse_optimal_weighting (end)

\subsection{Post-Selection/Averaging Inference} % (fold)
\label{sub:valid_confidence_interval_for_ols_vs_2sls_example}

\subsubsection{``Myopic'' Confidence Intervals}
These intervals assume that the selected model is correctly specified and ignore any uncertainty from the model selection step. They are constructed from the following asymptotic results:
  \begin{eqnarray*}
    \mbox{OLS:} \quad \sqrt{n}(\widehat{\beta} - \beta) \overset{d}{\rightarrow} N(0, \sigma_\epsilon^2/\sigma_x^2)\\
    \mbox{IV:} \quad \sqrt{n}(\widetilde{\beta} - \beta) \overset{d}{\rightarrow} N(0, \sigma_\epsilon^2/\gamma^2)\\
  \end{eqnarray*}
The reason the OLS estimator has a zero mean is that we are assuming here that, conditional on it being selected, it is correctly specified so that $\tau = 0$. To turn these results in to a procedure for constructing confidence interval, we'll plug in the estimators
 \begin{eqnarray*}
    \widehat{\beta} &=& (\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y}\\
    \widehat{\sigma}_x^2 &=& n^{-1}\mathbf{x}'\mathbf{x} \\
    \widetilde{\beta} &=& (\mathbf{x}'Z(Z'Z)^{-1} Z'\mathbf{x})^{-1}\mathbf{x}'Z (Z'Z)^{-1} Z'\mathbf{y}\\
    \widehat{\gamma}^2 &=& n^{-1} \mathbf{x}'Z(Z'Z)^{-1}Z'\mathbf{x}
  \end{eqnarray*}
from above. For $\sigma_\epsilon^2$, we'll use the error variance estimator that \emph{corresponds} to each estimator. In other words, 
  \begin{eqnarray*}
    \mbox{OLS:} \quad \widehat{\sigma}_\epsilon^2 = n^{-1} \left(\mathbf{y} - \mathbf{x}\widehat{\beta}\right)'\left(\mathbf{y} - \mathbf{x}\widehat{\beta}\right)\\
    \mbox{IV:} \quad \widetilde{\sigma}_\epsilon^2 = n^{-1} \left(\mathbf{y} - \mathbf{x}\widetilde{\beta}\right)'\left(\mathbf{y} - \mathbf{x}\widetilde{\beta}\right)
  \end{eqnarray*}
Note that for each we use the MLE rather than the unbiased variance estimator. It shouldn't make a big difference either way, of course. The resulting intervals are as follows:
  \begin{eqnarray*}
    \mbox{OLS:} \quad \widehat{\beta} \pm z_{\alpha/2} \cdot \frac{1}{\sqrt{n}} \left(\frac{\widehat{\sigma}_\epsilon^2}{\widehat{\sigma}_x^2} \right)^{1/2}\\
    \mbox{IV:} \quad \widetilde{\beta} \pm z_{\alpha/2} \cdot \frac{1}{\sqrt{n}} \left(\frac{\widetilde{\sigma}_\epsilon^2}{\widehat{\gamma}^2} \right)^{1/2}
  \end{eqnarray*}
\todo[inline]{Should I be using heteroscedasticity-consistent standard errors here?}

\subsubsection{Post-FMSC Corrected CIs}
The post-FMSC estimator is
  $$\beta^*_{FMSC} = \textbf{1}\left\{\widehat{T}_{FMSC} < 2 \right\} \widehat{\beta} + \textbf{1}\left\{\widehat{T}_{FMSC} \geq 2 \right\} \widetilde{\beta}$$
where
  $$\widehat{T}_{FMSC} = \frac{\widehat{\tau}^2 }{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2/\widehat{\gamma}^2}$$    
Since the weights sum to one, we can write
  $$\sqrt{n}(\beta^*_{FMSC} - \beta) = \textbf{1}\left\{\widehat{T}_{FMSC} < 2 \right\} \sqrt{n}(\widehat{\beta} - \beta) + \textbf{1}\left\{\widehat{T}_{FMSC} \geq 2 \right\}\sqrt{n}( \widetilde{\beta}-\beta)$$
The limit of $\sqrt{n}(\beta^*_{FMSC} - \beta)$ is $\Lambda(\tau)$ in the notation of the paper. From above we have
  $$
\left[
\begin{array}{c}
  \sqrt{n}\left(\widehat{\beta} - \beta\right) \\
  \sqrt{n}\left(\widetilde{\beta} - \beta\right)\\
  \widehat{\tau}
\end{array}
\right] \overset{d}{\rightarrow} 
\left[\begin{array}{cc}
1/\sigma_x^2 & \mathbf{0}'\\
  0 & \boldsymbol{\pi}'/\gamma^2\\
  1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}
  \right]\left( \left[\begin{array}{c} \tau \\ \mathbf{0}  \end{array} \right] + M\right)
$$
where $M \sim \mathcal{N}(0, \Omega)$ and
  \begin{eqnarray}
 \sigma_x^2&=&\boldsymbol{\pi}'Q_z \boldsymbol{\pi}  +\sigma^2_v \\
    \gamma^2 &=& \boldsymbol{\pi}'Q_z \boldsymbol{\pi}\\
    \Omega &=& \sigma_\epsilon^2\left[ \begin{array}{cc}
\sigma_x^2
&\boldsymbol{\pi}'Q_z\\
  Q_z \boldsymbol{\pi}&Q_z
  \end{array}\right]
  \end{eqnarray}
Following the general procedure from the paper, we'll simulate draws $M_j$, substituting consistent estimators of all unknowns except for $\tau$, for which we'll construct a confidence interval.

\paragraph{First Step: Confidence Region for $\tau$}
We'll use the fact, shown above, that $\widehat{\tau} \overset{d}{\rightarrow} N(\tau,V)$ where 
$$V = \sigma_\epsilon^2 \frac{\sigma_x^2\sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}} = \sigma^2_\epsilon \sigma^2_x \left(\sigma_x^2/\gamma^2 - 1 \right)$$
Define a consistent estimator $\widehat{V}$ of $V$ as follows
  $$\widehat{V} = \widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right)$$
For $\widehat{\sigma}^2_x$ and $\widehat{\gamma}^2$, we can simply use the estimators from above:
 \begin{eqnarray*}
    \widehat{\sigma}_x^2 &=& n^{-1}\mathbf{x}'\mathbf{x} \\
    \widehat{\gamma}^2 &=& n^{-1} \mathbf{x}'Z(Z'Z)^{-1}Z'\mathbf{x}
  \end{eqnarray*}
To estimate $\sigma_\epsilon^2$, there are two options: using the resdiduals formed from $\widetilde{\beta}$ or those formed from $\widehat{\beta}$. 
\todo[inline]{Don't worry about making a decision about this for the moment, but make sure to come back to it.}

Letting $z_{\delta/2}$ denote the $\alpha/2$ quantile of a standard normal distribution,
  $$\widehat{\tau} \pm z_{\delta/2} \; \sqrt{\widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} - 1 \right)}$$
is a $(1-\delta) \times 100\%$ asymptotic confidence interval for $\tau$.

\paragraph{Second Step: Simulation-Based Interval Conditional on $\tau$}
Let $\tau^*$ be an arbitrary value inside the confidence interval for $\tau$ from above. Now we will \emph{condition} on this value and calculate a confidence interval for $\beta^*_{FMSC}$ by simulation. To make the notation a little simpler, we'll assign names to each of the limiting random variables. Define
$$
\left[\begin{array}
  {c}
  A(\tau) \\ B \\ C(\tau) 
\end{array}\right] = \left[\begin{array}
  {c}
  \tau/\sigma_x^2  \\ 0 \\ \tau
\end{array}\right] + \left[\begin{array}{cc}
1/\sigma_x^2 & \mathbf{0}'\\
  0 & \boldsymbol{\pi}'/\gamma^2\\
  1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}
  \right]\left[ \begin{array}
    {c} M_1 \\ M_2
  \end{array}\right]$$
where 
  $$\left[\begin{array}
    {c} M_1 \\ M_2
  \end{array} \right] \sim N(0, \Omega)$$
and 
  $$\Omega = \sigma_\epsilon^2\left[ \begin{array}{cc}
\sigma_x^2
&\boldsymbol{\pi}'Q_z\\
  Q_z \boldsymbol{\pi}&Q_z
  \end{array}\right]$$
Using this notation, we have
 $$\left[
\begin{array}{c}
  \sqrt{n}\left(\widehat{\beta} - \beta\right) \\
  \sqrt{n}\left(\widetilde{\beta} - \beta\right)\\
  \widehat{\tau}
\end{array}
\right] \overset{d}{\rightarrow} \left[\begin{array}
  {c}
  A(\tau) \\ B \\ C(\tau) 
\end{array}\right]$$
Now let $\Lambda(\tau)$ denote the limiting random variable to which $\sqrt{n}\left(\beta^*_{FMSC} - \beta\right)$ converges. Then we have
  \begin{eqnarray*}
    \Lambda(\tau) &=& \mathbf{1}\left\{ \frac{C^2(\tau)}{V} < 2 \right\} A(\tau) +  \mathbf{1}\left\{ \frac{C^2(\tau)}{V} \geq 2 \right\} B\\
    &=& \mathbf{1}\left\{ C^2(\tau)< 2 V \right\} A(\tau) +  \mathbf{1}\left\{ C^2(\tau)\geq 2 V \right\} B
  \end{eqnarray*}
by the continuous mapping theorem since the indicator functions are almost-surely continuous. 


\begin{enumerate}
  \item For $j = 1, \hdots, J$ generate
    $$\left[ \begin{array}
      {c} M_1^{(j)} \\ M_2^{(j)}
    \end{array}\right] \overset{\mbox{iid}}{\sim} N\left( \left[ \begin{array}
      {c} 0 \\ \textbf{0}\\
    \end{array}\right], \;\widehat{\sigma}_\epsilon^2\left[ \begin{array}
      {cc}
      \widehat{\sigma}_x^2& \widehat{\boldsymbol{\pi}}' \widehat{Q}_z\\
      \widehat{Q}_z\widehat{\boldsymbol{\pi}} & \widehat{Q}_z
    \end{array}\right] \right) $$
  \item Set
    \begin{eqnarray*}
        A_j(\tau) &=&  \left(\tau + M_1^{(j)} \right)/\; \widehat{\sigma}_x^2\\    
        B_j &=& \widehat{\boldsymbol{\pi}'}M_2^{(j)}/\; \widehat{\gamma}^2\\
        C_j(\tau)&=& \tau + M_1^{(j)} - (\widehat{\sigma}_x^2/\widehat{\gamma}^2) \widehat{\boldsymbol{\pi}}' M_2^{(j)}
    \end{eqnarray*}
  \item Set 
    $$\Lambda_j(\tau) = \mathbf{1}\left\{ C_j^2(\tau)< 2 \widehat{V} \right\} A_j(\tau) +  \mathbf{1}\left\{ C_j^2(\tau)\geq 2 \widehat{V} \right\} B_j$$
    where $\widehat{V} = \widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right)$.
\end{enumerate}
Now, \emph{conditional on $\tau$} we can use the draws $\Lambda_j(\tau)$ to construct a confidence interval for $\beta$ post-FMSC selection as follows. Let $\widehat{a}(\tau)$ be the $\alpha/2$ quantile of the draws $\Lambda_j(\tau)$ and $\widehat{b}(\tau)$ be the $1- \alpha/2$ quantile. To get rid of the conditioning on $\tau$ we simply minimize $\widehat{a}(\tau)$ and maximize $\widehat{b}(\tau)$ over all values of $\tau$ in the confidence interval 
  $$\mathbf{T} = \left[ \widehat{\tau} - z_{\delta/2} \; \sqrt{\widehat{V}},  \; \widehat{\tau} + z_{\delta/2} \; \sqrt{\widehat{V}} \right]$$
Specifically, define
    \begin{eqnarray*}
      \widehat{a}_{min} &=& \min_{\tau \in \mathbf{T}} \widehat{a}(\tau)\\
      \widehat{b}_{max} &=& \max_{\tau \in \mathbf{T}} \widehat{b}(\tau)
    \end{eqnarray*}
Then $[\widehat{a}_{min} , \widehat{b}_{max}]$ is a confidence interval for $\Lambda(\tau)$ with asymptotic coverage of at least $1 - (\alpha + \delta)$ that \emph{does not depend on $\tau$}. Since $\Lambda(\tau)$ is the limit random variable to which $\sqrt{n}(\beta^*_{FMSC} - \beta)$ converges, we can use convert this into a confidence interval for $\beta$ based on $\beta^*_{FMSC}$ as follows. First substitute $\sqrt{n}(\beta^*_{FMSC} - \beta)$ for $\Lambda(\tau)$
$$P\left\{ \widehat{a}_{min} \leq  \sqrt{n}(\beta^*_{FMSC} - \beta) \leq \widehat{b}_{max}\right\} \geq 1 - (\alpha + \delta)$$
Rearranging,
\begin{eqnarray*}
   P\left\{ \widehat{a}_{min}/\sqrt{n} \leq  (\beta^*_{FMSC} - \beta) \leq \widehat{b}_{max}/\sqrt{n}\right\} &\geq& 1 - (\alpha + \delta)\\ 
   P\left\{ \widehat{a}_{min}/\sqrt{n} - \beta^*_{FMSC}  \leq - \beta \leq \widehat{b}_{max}/\sqrt{n} - \beta^*_{FMSC}  \right\} &\geq& 1 - (\alpha + \delta)\\
P\left\{\beta^*_{FMSC} -  \widehat{b}_{max}/\sqrt{n}  \leq  \beta \leq \beta^*_{FMSC} - \widehat{a}_{min}/\sqrt{n} \right\} &\geq& 1 - (\alpha + \delta)\\
 \end{eqnarray*} 
Thus, 
  $$\left[ \beta^*_{FMSC}  - \frac{\widehat{b}_{max}}{\sqrt{n}}, \; \beta^*_{FMSC}  - \frac{\widehat{a}_{min}}{\sqrt{n}} \right]$$
is a confidence interval for $\beta$ post-FMSC moment selection with asymptotic coverage of at least $1 - (\alpha + \delta)$.
% subsection valid_confidence_interval_for_ols_vs_2sls_example (end)


\subsubsection{Corrected CIs for DHW pre-test Estimators}
This is almost exactly the same as for post-FMSC described above. Simply replace 2 with the critical value for the desired test.


\subsubsection{Corrected CIs for Averaging Estimator}
The feasible version of the AMSE-optimal averaging estimator is
  $$\beta^*_{Avg} = \widehat{\omega}^* \widehat{\beta} + (1 - \widehat{\omega}^*)\widetilde{\beta}$$
where
$$\widehat{\omega }^* = \left[1 + \frac{\max \left\{0, \; \left(\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right) \right)/\;\widehat{\sigma}_x^4 \right\}}{\widehat{\sigma}_\epsilon^2 (1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2)}\right]^{-1}$$
Since the weights sum to one, we can write
  $$\sqrt{n}\left(\beta^*_{Avg}  - \beta \right) =\widehat{\omega}^* \sqrt{n}\left(\widehat{\beta} - \beta \right) +(1 - \widehat{\omega}^*) \sqrt{n}\left(\widetilde{\beta}  - \beta \right)$$
Now we can proceed to describe a simulation-based procedure to construct valid confidence intervals by direct analogy with the post-FMSC selection estimator. The only difference here is the form of the weight function which affects only the third step of the algorithm, the step in which we construct the draws $\Lambda_j(\tau)$. For the case of the averaging estimator, we set
  $$\Lambda_j(\tau) = \widehat{\omega}^*(C_j^2(\tau)) A_j(\tau) + \left[1 -  \widehat{\omega}^*(C_j^2(\tau)) \right]B_j$$
where
  $$\widehat{\omega}^*(C_j^2(\tau)) = \left[1 + \frac{\max \left\{0, \; \left(C^2_j(\tau) - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right) \right)/\;\widehat{\sigma}_x^4 \right\}}{\widehat{\sigma}_\epsilon^2 (1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2)}\right]^{-1}$$
All other steps of the procedure are identical.

\section{Derivations for Choosing Instruments Example}

\begin{eqnarray}
    y_i &=& \mathbf{x}_i' \boldsymbol{\beta} +  \epsilon_i\\
    \mathbf{x}_i &=& \Pi_w \mathbf{w}_i + \Pi_z \mathbf{z}_i + \mathbf{v}_i
\end{eqnarray}

$$E_n[\mathbf{z}_i(y_i - \mathbf{x}_i' \boldsymbol{\beta})] = E_n[\mathbf{z}_i\epsilon_i] = \boldsymbol{\tau}/\sqrt{n}$$

$$E_n[\mathbf{w}_i (y_i - \mathbf{x}_i' \boldsymbol{\beta})]= E_n[\mathbf{w}_i\epsilon_i] = \mathbf{0}$$

$$E_n[\mathbf{x}_i \epsilon_i] = \Pi_z \boldsymbol{\tau}/\sqrt{n} + E_n[\epsilon_i \mathbf{v}_i]$$
Need to think about how to control the endogeneity of $\mathbf{x}$ as I vary other parameters. Perhaps set
  $$E_n[\epsilon_i \mathbf{v}_i] = \boldsymbol{\gamma^2} - \Pi_z \boldsymbol{\tau}/\sqrt{n}$$ so that the endogeneity of $\mathbf{x}$ is held constant at $\boldsymbol{\gamma^2}$? Does this have any effect on the FMSC or does it only matter in the simulation?
  
  Should be able to write this down in terms of a tradeoff...





\end{document}