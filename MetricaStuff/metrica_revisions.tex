\documentclass[12pt]{article}
\usepackage{todonotes}
\usepackage[margin=1.5in]{geometry}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{enumerate}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]

\linespread{1.2}

\title{JMP Revisions Following Econometrica Comments}
\author{Francis J.\ DiTraglia}
\begin{document}

\maketitle

\begin{abstract}
This document collects all of the new material that I plan to add to my JMP following the referee reports from Econometrica. Once I've put this material together, I'll merge it with the existing version and send it to ReStud.
\end{abstract}

\section*{Referee Comments and My Responses}
The two main objections to my paper were as follows:
  \begin{enumerate}
    \item The FMSC provides an asymptotically unbiased estimate of MSE, but the FMSC itself is random, even in the limit. It is not true that the FMSC converges in probability to actual AMSE because of this additional randomness. 
    \item If the valid model is identified, why not just use it? If it's not identified, FMSC doesn't work. It's not clear that the gains from this procedure would be large in practice since the FMSC has a variance. See point \# 1. 
    \item This approach is not robust to weak identification.
    \item The procedure for correcting confidence intervals is too computationally intensive and aims to get the size correct without even looking at the width.
    \item Finite sample MSE doesn't always exist for these kinds of models. Maybe this isn't the best measure to look at.
    What about OLS versus IV?
    \item There are some missing references.
    \item Some objections to the simulation experiment. In particular, no widths are reported for confidence intervals.
  \end{enumerate}
The most important objections are 1 and 2, possibly followed by 3. The other points can be fairly easily dealt with and I will do so below. 

\paragraph{Objection \# 1} This comment boils down to ``I don't like efficient model selection: you should use consistent model selection.'' It is indeed true that the FMSC is random even in the limit and that it does not converge to actual AMSE: it is merely an asymptotically unbiased estimator of AMSE. However, this is the whole point: I explicitly want to \emph{avoid} doing consistent selection because the associated risk properties are so poor. I need to make this clearer by citing, for example, the Leeb and P\"{o}tscher paper from 2008 that Larry Wasserman discussed on his blog.

In a certain sense, this comment amounts to a critique of my use of local asymptotics. I should make very clear that this device is widely used in econometrics to study, among other things, local power, local-to-unit roots, etc. I should provide references for this, including Schorfheide and Moon. Furthermore, a great many model selection procedures are random in the limit: Mallows $C_p$ and AIC are two well-known examples, but many people have worked with selection in this framework. I should cite Bruce Hansen as well as Frank Schorfheide's VAR paper and Schorfheide and Moon.

I should also relate my framework to the idea of uniform asymptotic validity as discussed in Andrews and Guggenberger (2010) and Schorfheide and Moon.


\paragraph{Objection \# 2} This comment suggests that: (a) my simulations weren't convincing enough, and (b) I wasn't clear enough about the sort of situation for which the FMSC is designed. The solution is to do more and better simulations and to be clearer! The point of the FMSC is that we often find ourselves in a setting where there are various ``plausible'' assumptions we could use in estimation, some of which are weaker and some of which are stronger. Typically, we worry that the weaker assumptions might not provide sufficient information to study the question we're interested in, which we worry that the stronger assumptions might not quite be true. You can think of this as a kind of ``prior knowledge'' that violations of the stronger assumptions are ``small'' which is pretty much exactly what the local mis-specification idea encodes. I can relate this to the idea of ``plausibly exogenous'' as well as Schorfheide and Moon. 

I should be clear about the fact, and I need to find the citation for this, that model selection cannot uniformly beat the ``full'' model. (In this case, the full model is the set of moment conditions based on the weakest assumptions.) However, selection \emph{can} beat the ``full'' model over large regions of the parameter space, so it's really a questino of where you think you might be a priori. I should argue that the whole idea is to use my method when you consider it likely that you might be in the relevant region of the parameter space. I should also show in simulations that the cost you pay when you are \emph{not} in this region isn't too high.


\paragraph{Objection \# 3} This point is less important but also harder to handle. I'm pretty sure that there's no straightforward way to include weak identification in my framework directly although it's a very compelling idea: when you have weak identification it might make a lot of sense to use a slightly endogenous instrument. Even though I don't think it's possible to incorporate weak instrument \emph{asymptotics}, however, I can still evaluate how my proposed \emph{procedure} deals with weak instruments. There are at least two ways to do this. The first is by carrying out a simulation study. The second is by looking trying to relate the FMSC to some other well-known tests or procedures. In the IV versus OLS case, for example, I know that the result is a Hausman test with a non-standard critical value. I seem to recall that Dufour has a paper in which he argues that this is a good idea when you might have weak instruments. Basically the idea here is to look at how the FMSC implicitly trades off instrument strength and validity when we take the procedure \emph{outside} of its asymptotic framework.  

\paragraph{Overall Thoughts and To Do List}
Besides responding to the Objections 1--3, here are some other things I should do, roughly in order of importance:
  \begin{enumerate}
    \item Should defend my assumption that we have a minimal set of correct MCs by referencing the literature, including Chen etc.
    \item Change the notation to allow us to use arbitrary subsets of the moment conditions. This will accomodate the OLS/IV example.
    \item Allow the weighting matrix $W$ to be indexed by $S$.
    \item Fold in the IV/OLS example. This will allow me to discuss a number of interesting points, including optimal estimator averaging, weak instruments, relationship to testing, etc. Also allows a brief consideration of what happens when we have no valid moment conditions.
    \item Do a better job with the choosing instruments example.
    \item New and better simulation experiments. Look at median absolute deviation as well as trimmed MSE, etc. Try to cover more of the parameter space, etc. Pictures rather than tables. Everything needs to be replicable for ReStud!
    \item Re-do the empirical example with improved code for the confidence interval. Everything needs to be replicable for ReStud!
    \item Possibly add a second empirical example for OLS versus IV.
    \item Look at the proposed references and think about including them.
    \item Might want to be slightly more careful about regularity conditions. See for example Schorfheide and Moon.
  \end{enumerate}

\section{New Notation for Moment Selection Vector}
This is straightforward: just need to redefine $S$ and $\Xi_S$. See the GFIC paper.

\section{Two Running Examples in the Paper}
Two simple but empirically relevant examples we'll consider throughout the paper. Helpful because they make the intuition clear and also interesting in their own right. To be clear, FMSC applies to GMM in general, not just to linear models like these.


\paragraph{Example \#1: Choosing Instrumental Variables}
Consider the linear model
  \begin{equation}
  y_i = \mathbf{x}_i'\beta + \epsilon_i
  \end{equation}
where some or all of the regressors are endogenous. Suppose we have a vector of valid instruments $\mathbf{z}^{(1)}_i$ and another vector of ``suspect'' instruments $\mathbf{w}_i$ that are likely to be highly relevant but may well be slightly endogenous. These could be ``plausibly exogenous.'' Should we include $\mathbf{z}_i$ in the instrument set for use in estimation?  Arises in various settings. One concerns exogeneity assumptions in a panel data setting: strict exogeneity versus predeterminedness. 

\paragraph{Example \#2: Least Squares versus Instrumental Variables}
This example is similar to the first one, but illustrates that we don't have to structure the problem in terms of choosing over-identifying restrictions: we can select over fundamentally different estimators. Suppose we want to estimate the effect of an endogenous regressor $x$ in a linear model of the form
  \begin{equation}
    y_i = \mathbf{w}_i' \theta + \beta x_i + \epsilon_i
  \end{equation}
where $\mathbf{w}_i$ is a vector of endogenous control regressors. In this case the target parameter is $\beta$. Suppose we have a vector of valid instruments $\mathbf{z}_i$. Should we use OLS or IV? Dufour: ``IV is like Amputation; it should be a last resort to save the patient." Easily generalized to more than one endogenous regressor, but this example is very common in many settings, particularly treatment effects with microdata. Cite the Nevo and Rosen and Plausibly Exogenous papers. Another issue is weak instruments and whether it is possible to \emph{combine} OLS and IV.



\section{Local Mis-specification for the Examples}

\paragraph{Example \#1: Choosing Instrumental Variables}
 \begin{equation}
  E_n\left[\begin{array}{c}
    \mathbf{w}_i(y_i - \mathbf{x}_i \beta)\\
     \mathbf{z}_i(y_i - \mathbf{x}_i \beta)
  \end{array} \right] = \left[ \begin{array}{c}\mathbf{0} \\ \boldsymbol{\tau}/\sqrt{n} \end{array}\right] 
 \end{equation}
 
 \paragraph{Example \#2: Least Squares versus Instrumental Variables}
\begin{equation}
    E_n\left[\begin{array}{c}
    \mathbf{w}_i(y_i - \mathbf{w}_i' \theta  - \beta x_i)\\
    \mathbf{z}_i(y_i - \mathbf{w}_i' \theta  - \beta x_i)\\
    x_i(y_i - \mathbf{w}_i' \theta  - \beta x_i)
    \end{array}\right] = \left[\begin{array}{c} \mathbf{0} \\ \mathbf{0} \\ \tau/\sqrt{n}  \end{array} \right]
\end{equation}
In our consideration of this example in the remainder of the paper we will assume, without loss of generality, that there are no exogenous regressors $\mathbf{w}_i$. If there are any, they can always be ``projected out'' of $y$, $x$ and $\mathbf{z}$.

\section{Lemma: A CLT for Local Mis-specification}
Covers both of the examples from the paper in an iid setting (microdata, panel, etc.) Can be extended to handle dependence by using something other than Lindeberg-Feller.

\begin{lem}[CLT Under Local Mis-specification] 
\label{lem:CLT}
 Let $\{\mathbf{w}_i, \mathbf{z}_i, \epsilon_i\colon 1 \leq i \leq n, n = 1, 2, \hdots\}$ be a triangular array of random variables such that 
  \begin{enumerate}[(a)]
    \item $(\mathbf{w}_i, \mathbf{z}_i, \epsilon_i) \sim \mbox {iid}$ within each row of the array (i.e.\ for fixed $n$)
    \item $E_n\left[\epsilon_i \mathbf{w}_i \right] = \mathbf{0}$
    \item $E_n\left[\epsilon_i \mathbf{z}_i \right] = \boldsymbol{\tau}/\sqrt{n}$
    \item $E_n \left[\left|\epsilon_i \mathbf{w}_i\right|^{2+\eta} \right] < C$ and $E_n \left[\left|\epsilon_i \mathbf{z}_i\right|^{2+\eta} \right] < C$ for all $n$ and some $\eta > 0$ where $C < \infty$.
    \item $\displaystyle  \Omega = \lim_{n \rightarrow \infty} E_n \left[\begin{array}{cc}
    \epsilon_i^2 \mathbf{w}_i \mathbf{w}_i' &  \epsilon_i^2 \mathbf{w}_i\mathbf{z}_i '\\
    \epsilon_i^2 \mathbf{z}_i \mathbf{w}_i'& \epsilon_i^2 \mathbf{z}_i \mathbf{z}_i'
    \end{array}\right]$ exists and is both finite and positive definite.
  \end{enumerate}
  Then, 
    $$\frac{1}{\sqrt{n}} \sum_{i=1}^n \left[\begin{array}{c} \epsilon_i \mathbf{w}_i \\ \epsilon_i \mathbf{z}_i \end{array} \right] \overset{d}{\rightarrow} N\left(\left[\begin{array}{c} \mathbf{0} \\ \boldsymbol{\tau} \end{array} \right], \Omega\right)$$
\end{lem}

\begin{proof}
We proceed by verifying the conditions of the Lindeberg-Feller Central Limit Theorem (van der Vaart 2.27) for the sequence of random vectors $Y_{ni} = n^{-1/2}\left(\epsilon_i \mathbf{w}_i', \epsilon_i \mathbf{z}_i' \right)'$. Since each row of the triangular array contains $n$ iid observations, 
  $$\lim_{n\rightarrow \infty}\sum_{i=1}^{k_n} Var_n(Y_{ni}) = \lim_{n\rightarrow \infty} \frac{1}{n} \sum_{i=1^n} Var_n(Y_{ni})= \lim_{n\rightarrow \infty} Var_n(Y_{ni})$$
and under the local mis-specification assumption,
  \begin{eqnarray*}
  Var_n(Y_{ni}) = E_n \left[\begin{array}{cc}
    \epsilon_i^2 \mathbf{w}_i \mathbf{w}_i' &  \epsilon_i^2 \mathbf{w}_i\mathbf{z}_i '\\
    \epsilon_i^2 \mathbf{z}_i \mathbf{w}_i'& \epsilon_i^2 \mathbf{z}_i \mathbf{z}_i'
    \end{array}\right] - \left[\begin{array}{cc}
    \mathbf{0} &  \mathbf{0}\\
    \mathbf{0}& \boldsymbol{\tau}\boldsymbol{\tau}'/n
    \end{array}\right] \rightarrow \Omega.
  \end{eqnarray*}
Similarly, the sum from the Lindeberg Condition simplifies to 
  $$\sum_{i=1}^{k_n} E\left[ \left|Y_{ni} \right|^2 \mathbf{1}\left\{\left|Y_{ni} \right| >\xi \right\}\right]= E_n \left[\epsilon_i^2 \left(\left|\mathbf{w}_i \right|^2 + \left| \mathbf{z}_i\right|^2 \right) \mathbf{1} \left\{A_n\right\}\right] \rightarrow 0$$
as $n \rightarrow \infty$ for any $\xi>0$, where
$$A_n = \left\{\left|\epsilon_i\right| \left( \left|\mathbf{w}_i \right|^2 + \left| \mathbf{z}_i\right|^2\right)^{1/2}  > n^{1/2}\xi\right\}$$ 
Now, by H\"{o}lder's Inequality followed by Minkowski's Inequality (White 3.4, 3.11)
  \begin{eqnarray*}
    E_n \left[\epsilon_i^2 \left(\left|\mathbf{w}_i \right|^2 + \left| \mathbf{z}_i\right|^2 \right) \mathbf{1} \left\{A_n\right\}\right] &=& E_n \left[\left(\left|\epsilon_i\mathbf{w}_i \right|^2 + \left|\epsilon_i \mathbf{z}_i\right|^2 \right) \mathbf{1} \left\{A_n\right\}\right]\\
    &\leq&E_n \left[\left|\left|\epsilon_i\mathbf{w}_i \right|^2 + \left|\epsilon_i \mathbf{z}_i\right|^2 \right|^p\right]^{1/p} E_n\left[\left|\mathbf{1}\{A_n\} \right|^q \right]^{1/q}\\
    &=& E_n \left[\left|\left|\epsilon_i\mathbf{w}_i \right|^2 + \left|\epsilon_i \mathbf{z}_i\right|^2 \right|^p\right]^{1/p} P(A_n)^{1/q}\\
    &\leq& \left(E_n \left[\left|\epsilon_i \mathbf{w}_i\right|^{2p} \right]^{1/p} + E_n\left[ \left|\epsilon_i \mathbf{z}_i\right|^{2p}\right]^{1/p}\right)P(A_n)^{1/q}
  \end{eqnarray*}
provided that $p>1$, $1/p + 1/q = 1$ and all the relevant moments exist. By the Generalized Chebyshev Inequality (White 2.41) 
    \begin{eqnarray*}
      P(A_n) &\leq& \left( \frac{1}{n\xi^2}\right) E_n\left[\epsilon_i^2 \left(\left|\mathbf{w}_i \right|^2 + \left|\mathbf{z}_i \right|^2\right) \right]\\
      &=& \left( \frac{1}{n\xi^2}\right) E_n\left[\left|\epsilon_i \mathbf{w}_i \right|^2 + \left| \epsilon_i \mathbf{z}_i \right|^2\right] \leq \frac{2C}{n\xi^2}
    \end{eqnarray*}
Combining these and taking $p = 1+\eta$, $q = (1+\eta)/\eta$, we have
  \begin{eqnarray*}
  E_n \left[\epsilon_i^2 \left(\left|\mathbf{w}_i \right|^2 + \left| \mathbf{z}_i\right|^2 \right) \mathbf{1} \left\{A_n\right\}\right] &\leq& 2C^{\frac{1}{1+\eta}} \left(\frac{2C}{n\xi^2} \right)^{\frac{\eta}{\eta+1}} \rightarrow 0.
  \end{eqnarray*}
Therefore, by the Lindeberg-Feller Central Limit Theorem,
  $$\frac{1}{\sqrt{n}} \sum_{i=1}^n \left[\begin{array}{c} \epsilon_i \mathbf{w}_i \\ \epsilon_i \mathbf{z}_i - \boldsymbol{\tau}/\sqrt{n}\end{array} \right] = \left(\frac{1}{\sqrt{n}} \sum_{i=1}^n \left[\begin{array}{c} \epsilon_i \mathbf{w}_i \\ \epsilon_i \mathbf{z}_i \end{array} \right] - \left[\begin{array}{c}\mathbf{0}\\ \boldsymbol{\tau} \end{array} \right] \right)\overset{d}{\rightarrow} N(\mathbf{0}, \Omega).$$
\end{proof}


\section{OLS versus IV Example}
Without loss of generality, we may assume that there are no exogenous regressors or, equivalently, that they have been ``projected out.'' Thus, the DGP is: 
    	\begin{eqnarray}
			y_{i} &=& \beta x_{i}  + \epsilon_{i}\\
	x_{i} &=& \mathbf{z}_{i}' \boldsymbol{\pi} + v_{i}
		\end{eqnarray}
and the local mis-specification assumption becomes
  \begin{equation}
    E_n \left[\begin{array}{c} \mathbf{z}_i \epsilon_i \\ x_i \epsilon_i \end{array}\right] = \left[\begin{array}{c} \mathbf{0} \\ \tau/\sqrt{n} \end{array}\right].
  \end{equation}
Stacking observations,
\begin{eqnarray}
    \mathbf{y} &=& \mathbf{x}\beta + \boldsymbol{\epsilon}\\
    \mathbf{x} &=& Z\boldsymbol{\pi} + \mathbf{v}
\end{eqnarray}
where $Z' = (\mathbf{z}_{1}, \hdots, \mathbf{z}_{n}), \mathbf{x}' = (x_{1}, \hdots, x_{n})$ and so on. We consider two estimators of the scalar $\beta$: the ordinary least squares (OLS) estimator $\widehat{\beta}$ and the Generalized Instrumental Variable (GIV) estimator $\widetilde{\beta}$
  \begin{eqnarray}
		\widehat{\beta} &=& \left(\mathbf{x}'\mathbf{x}\right)^{-1}\mathbf{x}'\mathbf{y}\\
		\widetilde{\beta} &=& \left(\mathbf{x}'Z W_n Z'\mathbf{x}\right)^{-1}\mathbf{x}'Z W_nZ'\mathbf{y}
	\end{eqnarray}
where $W_n$ is a positive definite weighting matrix. To estimate $\tau$, we plug the GIV estimator into the OLS moment conditions and multiply by $\sqrt{n}$ as follows:
  \begin{equation}
     \widehat{\tau} = \sqrt{n} \left[\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})/n\right] = n^{-1/2}\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})
   \end{equation} 


\begin{thm}
\label{thm:OLSIV}
Suppose that
  \begin{enumerate}[(a)]
    \item The DGP with triangular array.
    \item Local Mis-specification.
    \item Fourth and a bit moments of everything.
    \item $W_n \overset{p}{\rightarrow} W$, a positive definite matrix.
    \item Limit of variance for $v_i$ exists and is positive.
    \item The limit $\lim_{n\rightarrow \infty} E_n[\mathbf{z}_i \mathbf{z}_i'] = Q_z$ exists and is positive definite
    \item The instruments $\mathbf{z}_i$ are relevant:  $|\boldsymbol{\pi}|>0$
  \end{enumerate}
  Then,
  $$
\left[
\begin{array}{c}
  \sqrt{n}\left(\widehat{\beta} - \beta\right) \\
  \sqrt{n}\left(\widetilde{\beta} - \beta\right)\\
  \widehat{\tau}
\end{array}
\right] \overset{d}{\rightarrow} 
\left[\begin{array}{cc}
\widehat{K} & \mathbf{0}'\\
  0 & \widetilde{K}\\
  1 & -\widehat{K}^{-1} \widetilde{K}
  \end{array}
  \right]\left( \left[\begin{array}{c} \tau \\ \mathbf{0}  \end{array} \right] + M\right)
$$
where $M \sim \mathcal{N}(0, \Omega)$ and
  \begin{eqnarray}
 \widehat{K}&=&   \left(\boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2\right)^{-1}\\
    \widetilde{K}&=& \left(\boldsymbol{\pi}'Q_z W Q_z \boldsymbol{\pi} \right)^{-1}\boldsymbol{\pi}'Q_z W\\
    \Omega &=& \lim_{n\rightarrow \infty} E_n \left[
    \begin{array}{cc}
      \epsilon_i^2 x_i^2 & \epsilon_i^2 x_i \mathbf{z}_i' \\
      \epsilon_i^2 x_i \mathbf{z}_i& \epsilon_i^2 \mathbf{z}_i \mathbf{z}_i'
    \end{array}
    \right]
  \end{eqnarray}
\end{thm}

\begin{proof}
Substituting the DGP and rearranging, $\sqrt{n}\left( \widehat{\beta} - \beta\right) = \widehat{K}_n \boldsymbol{m}_n$ and similarly
$\sqrt{n} \left( \widetilde{\beta} - \beta\right) = \widetilde{K}_n \boldsymbol{m}_n$, where
  \begin{eqnarray*}
    \widehat{K}_n&=&  \left(\mathbf{x}'\mathbf{x}/n\right)^{-1}\\
    \widetilde{K}_n&=&\left[\left(\mathbf{x}'Z/n\right) W_n \left(Z'\mathbf{x}/n\right)\right]^{-1}\left(\mathbf{x}'Z/n\right) W_n \\
  \boldsymbol{m}_n &=& \left[
\begin{array}{c}
\mathbf{x}'\boldsymbol{\epsilon}/\sqrt{n}\\
Z'\boldsymbol{\epsilon}/\sqrt{n}
\end{array}
\right]
\end{eqnarray*}
Moreover,
  \begin{eqnarray*}
   \widehat{\tau} &=& \sqrt{n} \left[\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})/n\right] =\mathbf{x}'\boldsymbol{\epsilon}/\sqrt{n} -  (\mathbf{x}'\mathbf{x}/n)\sqrt{n}(\widetilde{\beta} - \beta) \\
        &=& \left[\begin{array}{cc} 1 \quad &\widehat{K}_n^{-1}\widetilde{K}_n\end{array}\right]\boldsymbol{m}_n
  \end{eqnarray*}
Since
  \begin{eqnarray*}
    \frac{\mathbf{x}'\mathbf{x}}{n} &=& \boldsymbol{\pi}'\left(\frac{Z'Z}{n}\right) \boldsymbol{\pi} + \left(\frac{\mathbf{v}'Z}{n}\right)\boldsymbol{\pi} +\boldsymbol{\pi}' \left(\frac{Z'\mathbf{v}}{n} \right) + \frac{\mathbf{v}'\mathbf{v}}{n}\\
      \frac{\mathbf{x}'Z}{n}&=&  \boldsymbol{\pi}' \left(\frac{Z'Z}{n}\right) + \frac{\mathbf{v}'Z}{n}
  \end{eqnarray*}
it suffices to examine $Z'Z/n$, $Z'\mathbf{v}/n$ and $\mathbf{v}'\mathbf{v}/n$. Because the triangular array is iid in each row, uniformly bounded fourth moments are sufficient for $L_2$ convergence (see e.g.\ Davidson 19.1) which implies convergence in probability. Thus,
  \begin{eqnarray*}
    Z'Z/n &\overset{p}{\rightarrow}& \lim_{n\rightarrow \infty} E_n[\mathbf{z}_i \mathbf{z}_i'] = Q_z \\
    Z' \mathbf{v}/z &\overset{p}{\rightarrow}& \lim_{n\rightarrow \infty} E_n[\mathbf{z}_i v_i] = 0\\
    \mathbf{v}'\mathbf{v}/n &\overset{p}{\rightarrow}& \lim_{n \rightarrow \infty} E_n[v_i^2] = \sigma^2_v 
  \end{eqnarray*}  
It follows that $\widehat{K}_n\overset{p}{\rightarrow} \widehat{K}$ and $\widetilde{K}_n \overset{p}{\rightarrow} \widetilde{K}$. Finally, we apply Lemma \ref{lem:CLT} to $\boldsymbol{m}_n$. The required bounds $E_n\left[\left| \epsilon_i \mathbf{z}_i \right|^{2+\eta}\right] <C$ and $E_n\left[\left| \epsilon_i x_i \right|^{2+\eta}\right] < C$ follow
from our assumptions on the moments of $v_i$, $\epsilon_i$ and $\mathbf{z}_i$ by Minkowski's Inequality and the Cauchy-Schwarz Inequality.
\end{proof}

\subsection{A Simplification - 2SLS}
Preceding result covers any iid setting. To get more intuition and compare to well-known procedures consider a simplification so that 2SLS is the efficient GIV estimator and OLS is fully efficient. 

\begin{cor}
Suppose that 
  \begin{enumerate}[(a)]
    \item $W_n =(Z'Z/n)^{-1}$
    \item $\sigma_\epsilon^2 = \lim_{n \rightarrow \infty} E_n[\epsilon_i^2]$ exists and is positive
    \item $\displaystyle \Omega =\sigma_\epsilon^2 \left(\lim_{n\rightarrow \infty} E_n \left[
    \begin{array}{cc}
      x_i^2 & x_i \mathbf{z}_i' \\
       x_i \mathbf{z}_i& \mathbf{z}_i \mathbf{z}_i'
    \end{array}
    \right]\right)$
  \end{enumerate}
Then, under the conditions of Theorem \ref{thm:OLSIV}
$$
\left[
\begin{array}{c}
  \sqrt{n}\left(\widehat{\beta} - \beta\right) \\
  \sqrt{n}\left(\widetilde{\beta} - \beta\right)\\
  \widehat{\tau}
\end{array}
\right] \overset{d}{\rightarrow}
\mathcal{N}\left(
\left[
\begin{array}{c}
\tau/\sigma_x^2 \\ 
0\\
\tau
\end{array}
\right],
\sigma_\epsilon^2 \left[ \begin{array}{ccc}
  1/\sigma_x^2 & 1/\sigma_x^2 & 0\\
  1/\sigma_x^2 & 1/\gamma^2 & 1 - \sigma_x^2/\gamma^2\\
  0& 1 - \sigma_x^2/\gamma^2 & \sigma_x^2(\sigma_x^2/\gamma^2 - 1)
  \end{array}\right]
  \right)
$$
where $\sigma_x^2 = \gamma^2 + \sigma_v^2$ and $\gamma^2 = \boldsymbol{\pi}'Q_z \boldsymbol{\pi}$.
\end{cor}
Note that, since $\sigma_x^2/\gamma^2 = 1 + \sigma_v^2 /\gamma^2$, the covariance between $\sqrt{n}\left(\widetilde{\beta} - \beta\right)$ and $\widehat{\tau}$ in the limit is \emph{negative}. In particular, it equals $\sigma_v^2/\gamma^2$. Hence, the weaker the instruments, the more negative the covariance becomes.
\begin{proof}
First,
$$\Omega = \sigma_\epsilon^2\left[ \begin{array}{cc}
\sigma_x^2
&\boldsymbol{\pi}'Q_z\\
  Q_z \boldsymbol{\pi}&Q_z
  \end{array}\right] = \sigma_\epsilon^2 V$$
  Since $W_n = (Z'Z/n)^{-1}\overset{p}{\rightarrow} Q_z^{-1}$, we have $\widetilde{K} = \left(\boldsymbol{\pi}'Q_z \boldsymbol{\pi}\right)^{-1}\boldsymbol{\pi}'$. 
Defining
  $$\Sigma = \left[\begin{array}{cc}
\widehat{K} & \mathbf{0}'\\
  \mathbf{0} & \widetilde{K}\\
  1 & -\widehat{K}^{-1} \widetilde{K}
  \end{array}
  \right] = \left[\begin{array}
    {cc}
    1/\sigma_x^2 & \textbf{0}' \\
    0 & \boldsymbol{\pi}'/\gamma^2\\
    1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}\right]$$
we have,
  \begin{eqnarray*}
    \sigma_\epsilon^2 \Sigma V \Sigma' &=&
    \sigma_\epsilon^2 
    \left[\begin{array}
    {cc}
    1/\sigma_x^2 & \textbf{0}' \\
    0 & \boldsymbol{\pi}'/\gamma^2\\
    1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}\right]
    \left[ \begin{array}{cc}
\sigma_x^2
&\boldsymbol{\pi}'Q_z\\
  Q_z \boldsymbol{\pi}&Q_z
  \end{array}\right] 
    \left[
    \begin{array}{ccc}
      1/\sigma_x^2 & 0 & 1 \\
      \mathbf{0} & \boldsymbol{\pi}/\gamma^2 & (-\sigma_x^2/\gamma^2)\boldsymbol{\pi}
    \end{array}
    \right]\\
   &=&\sigma_\epsilon^2 
    \left[\begin{array}
    {cc}
    1/\sigma_x^2 & \textbf{0}' \\
    0 & \boldsymbol{\pi}'/\gamma^2\\
    1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}\right]\left[ 
    \begin{array}
      {ccc}
      1 & 1 & 0 \\
      Q_z \boldsymbol{\pi}/\sigma_x^2 & Q_z \boldsymbol{\pi}/\gamma^2 & Q_z \boldsymbol{\pi}(1 - \sigma_x^2/\gamma^2)
    \end{array}
  \right] \\
  &=& \sigma_\epsilon^2 \left[ \begin{array}{ccc}
  1/\sigma_x^2 & 1/\sigma_x^2 & 0\\
  1/\sigma_x^2 & 1/\gamma^2 & 1 - \sigma_x^2/\gamma^2\\
  0& 1 - \sigma_x^2/\gamma^2 & \sigma_x^2(\sigma_x^2/\gamma^2 - 1)
  \end{array}\right].
    \end{eqnarray*}
\end{proof}

\paragraph{Unpacking the Homoskedasticity Assumption}
The assumption
  $$\Omega \equiv \lim_{n\rightarrow \infty} E_n \left[
    \begin{array}{cc}
      \epsilon_i^2 x_i^2 & \epsilon_i^2 x_i \mathbf{z}_i' \\
      \epsilon_i^2 x_i \mathbf{z}_i& \epsilon_i^2 \mathbf{z}_i \mathbf{z}_i'
    \end{array}
    \right] =\sigma_\epsilon^2 \left(\lim_{n\rightarrow \infty} E_n \left[
    \begin{array}{cc}
      x_i^2 & x_i \mathbf{z}_i' \\
       x_i \mathbf{z}_i& \mathbf{z}_i \mathbf{z}_i'
    \end{array}
    \right]\right)$$
places asymptotic restrictions on the joint distribution of $\epsilon, v, \mathbf{z}$. First write:
\begin{eqnarray*}
  \epsilon_i^2 x_i^2 &=& \epsilon_i^2\left(\mathbf{z}_i'\boldsymbol{\pi} + v_i\right)^2 = \epsilon_i^2\left(\boldsymbol{\pi}'\mathbf{z}_i \mathbf{z}_i' \boldsymbol{\pi} + 2 v_i\mathbf{z}_i'\boldsymbol{\pi} + v_i^2\right) \\
  \epsilon_i^2 x_i \mathbf{z}_i'&=& \epsilon_i^2 \left(\mathbf{z}_i'\boldsymbol{\pi} + v_i\right)\mathbf{z}_i' = \epsilon_i^2 \left(\boldsymbol{\pi}'\mathbf{z}_i\mathbf{z}_i' + v_i\mathbf{z}_i'\right)
\end{eqnarray*}
Thus, we see that the following conditions are sufficient:
\begin{eqnarray*}
  E[\epsilon_i^2 \mathbf{z}_i \mathbf{z}_i']- E[\epsilon_i^2]E[ \mathbf{z}_i \mathbf{z}_i'] \rightarrow 0\\
  E[\epsilon_i^2 v_i \mathbf{z}_i'] - E[\epsilon_i^2]E[v_i \mathbf{z}_i'] \rightarrow 0\\
  E[\epsilon_i^2 v_i^2] - E[\epsilon_i^2]E[v_i^2] \rightarrow 0
\end{eqnarray*}

\subsection{Infeasible FMSC: OLS versus 2SLS}
We see that the variance of the OLS estimator is always strictly lower than that of the 2SLS estimator since $\sigma^2_\epsilon/\sigma_x^2 = \sigma^2_\epsilon/(\gamma^2 + \sigma_v^2)$. The AMSE of the OLS and 2SLS estimators takes a particularly simple form:
  \begin{eqnarray}
  \mbox{AMSE(OLS)} &=& \frac{\tau^2}{\sigma_x^4} + \frac{\sigma_\epsilon^2}{\sigma_x^2}\\
  \mbox{AMSE(2SLS)} &=& \frac{\sigma_\epsilon^2}{\gamma^2}
  \end{eqnarray}
where $\sigma_x^2 = \gamma^2 + \sigma_v^2$ and $\gamma^2 = \boldsymbol{\pi}'Q_z \boldsymbol{\pi}$. The AMSE of the OLS estimator is lower than that of the 2SLS estimator when
  \begin{eqnarray*}
    \frac{\tau^2}{\sigma_x^4} + \frac{\sigma_\epsilon^2}{\sigma_x^2} &<& \frac{\sigma_\epsilon^2}{\gamma^2}\\
    \frac{\tau^2}{\sigma_\epsilon^2\sigma_x^2} + 1&<&\frac{\sigma_x^2}{\gamma^2}\\
    \tau^2 &<&\sigma_\epsilon^2\sigma_x^2\left(\frac{\sigma_x^2}{\gamma^2} - 1\right)\\
        \tau^2  &<& \sigma_x^2 \sigma_\epsilon^2\left(\frac{\sigma_x^2 - \gamma^2}{\gamma^2}\right)\\
        \tau^2  &<& \sigma_x^2 \sigma_\epsilon^2\left(\frac{\sigma_v^2}{\gamma^2}\right)\\
                \tau^2  &<& \sigma_v^2 \sigma_\epsilon^2\left(\frac{\sigma_x^2}{\gamma^2}\right)\\
              |\tau|  &<& \sigma_v \sigma_\epsilon\sqrt{\frac{\boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}}\\
              |\tau|  &<& \sigma_v \sigma_\epsilon \sqrt{1+ \frac{1}{\kappa^2}}
  \end{eqnarray*}
where $\kappa^2 = (\boldsymbol{\pi}'Q_z \boldsymbol{\pi})/\sigma_v^2$, is the signal-to-noise ratio in the first stage. We see that the FMSC trades the endogeneity of $x$, as measured by $\tau$, against the strength of the instruments, as measured by the concentration parameter $\kappa^2$. 


To get a better sense of the form of this trade-off, we can ``re-interpret'' this cutoff \emph{as though} it were finite-sample rule. Recall that we defined $E_n[x_i\epsilon_i]= \tau/\sqrt{n}$. Hence,
  $$\tau = \sqrt{n} E_n[x_i \epsilon_i] = \sqrt{n}E_n\left[\left(\mathbf{z}_i' \boldsymbol{\pi} + v_i\right)\epsilon_i \right] = \sqrt{n} E_n[v_i\epsilon_i] = \sqrt{n} Cor_n(v_i, \epsilon_i) \sigma_v \sigma_\epsilon$$
provided that we do not envision $\sigma_v$ and $\sigma_\epsilon$ changing with sample size. Thus, for a \emph{fixed} sample size $n$, we can think of $\tau$ as the correlation between $\epsilon_i$ and $v_i$ scaled by $\sqrt{n}$ and their respective standard deviations. Dropping the $n$ subscript on the correlation and substituting into the preceding inequality, we have
  $$|\rho|  < \sqrt{\frac{1}{n}\left(1 + \frac{1}{\kappa^2}\right)}$$
where $\rho$ is the finite sample correlation between $v_i$ and $\epsilon_i$. Thus, interpreted as a finite sample rule, the FMSC tells us to weigh the endogeneity of $x_i$ as measured by $\rho$ against the sample size and the concentration parameter $\kappa^2$. We should only use the 2SLS estimator when the concentration parameter is sufficiently large, the sample size is sufficiently large, or $x$ is sufficiently endogenous. The following figure depicts the region in which OLS is favored for different sample sizes.

\begin{figure}
  \caption{OLS and IV Figure goes here. R code commented out in this document.}
\end{figure}
% <<OLSboundary, dev = "tikz", echo=FALSE, cache=TRUE, fig.width=5, fig.height=5>>=
% boundary.plot <- function(n, x.max = 2, y.max = 1){
%   step <- x.max/400
%   start <- 0 + step
%   x <- seq(from = start, to = x.max, by = step)
%   y <- sqrt((1/n) * (1 + 1/x))
%   plot(x,y, type = 'l', xlim = c(0,x.max), ylim = c(0, y.max) ,xaxs = "i", yaxs = "i", pty = "s", ylab = "$|\\rho_{\\epsilon v}|$", xlab =  "$\\kappa^2$")
  
%   poly <- cbind(c(0, x, x.max), c(0, y, 0))
%   polygon(poly, density = 20)
          
%   text(x.max/2, 0.8 * y.max, cex = 1.5, bquote("$n$" == .(n)))
% }

% par(mfrow = c(2,2), mgp=c(2.2,0.45,0), tcl=-0.4, mar=c(3.3,3.6,1.1,1.1))

% boundary.plot(10, x.max = 2, y.max = 1)
% boundary.plot(100, x.max = 1, y.max = 1)
% boundary.plot(500, x.max = 0.5, y.max = 1)
% boundary.plot(1000, x.max = 0.25, y.max = 1)
% @


As the sample size grows, the boundary moves towards the origin, meaning that we are more likely to use 2SLS. However, each of regions asymptotes at the origin so that when instruments are weak, we choose OLS. This makes intuitive sense. Notice that we change the horizontal axis limits to make it easier to see the threshold for weak instruments depending on sample size.

\paragraph{Alternative Parameterization}
Above we used $\kappa^2 = (\boldsymbol{\pi}'Q_z \boldsymbol{\pi})/\sigma_v^2$ to encode the strength of the instruments. The boundary region between OLS and IV turned out to depend on $1 + 1/\kappa^2$. Another way to express this is as follows:
  \begin{eqnarray*}
    1 + \frac{1}{\kappa^2} &=& 1 + \frac{\sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}} = \frac{ \boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2}{\boldsymbol{\pi'}Q_z \boldsymbol{\pi}} = \frac{1}{R^2}
  \end{eqnarray*}
where $R^2$ is the population coefficient of determination in the first-stage regression. Using this convention, the finite-sample version of the boundary from above becomes
    $$|\rho|  < \sqrt{\frac{1}{nR^2} }$$
which is a much simpler expression. The only potential problem with expressing things in this fashion is that we cannot really vary the population $R^2$ completely independently from the correlation between first and second stage errors. I have to think about this some more...

\paragraph{Yet Another Parameterization} Should we parameterize things in terms of the correlation between $x$ and $\epsilon$ or in terms of $\epsilon$ and $v$ as above?

\todo[inline]{Need to think carefully about the finite sample version of this.}


\subsection{Feasible FMSC} % (fold)
\label{sub:feasible_fmsc}
In the previous section we found that, under local mis-specification, the AMSE of the OLS estimator is lower than that of the IV estimator whenever 
    $$\frac{\tau^2}{\sigma_v^2 \sigma_\epsilon^2}  < \frac{\boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}} = \frac{\sigma_x^2}{\gamma^2} $$
% subsection feasible_fmsc (end)
To use the FMSC we substitute estimators of all the quantities in the preceding expression. For the right-hand side we use the familiar estimators which remain consistent under local mis-specification
  \begin{eqnarray*}
     \widehat{\sigma}_x^2 &=& n^{-1}\mathbf{x}' \mathbf{x} \overset{p}{\rightarrow} \boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2\\
      \widehat{\gamma}^2 &=& n^{-1}\mathbf{x}' Z (Z'Z)^{-1}Z' \mathbf{x} \overset{p}{\rightarrow} \boldsymbol{\pi}'Q_z \boldsymbol{\pi} 
  \end{eqnarray*}
as we saw in the proofs above. The difference of the two provides a consistent estimator of $\sigma_v^2$ and we see that this is equivalent to using the residuals from the first-stage regression:
  \begin{eqnarray*}
     \widehat{\sigma}_v^2 &=& \widehat{\sigma}_x^2 - \widehat{\gamma}^2 \\
     &=&n^{-1}\mathbf{x}' \mathbf{x}  - n^{-1}\mathbf{x}' Z (Z'Z)^{-1}Z' \mathbf{x}\\
      &=& n^{-1} \mathbf{x}' \left(I - Z(Z'Z)^{-1}Z' \right)\mathbf{x}\\
    &=& n^{-1} \mathbf{x}' (I - P_Z)\mathbf{x}\\
    &=& n^{-1} \mathbf{x}' (I-P_Z)'(I - P_Z)\mathbf{x}\\
    &=& (M_Z \mathbf{x})'(M_Z \mathbf{x})/n\\
    &=& \widehat{v}'\widehat{v}/n \overset{p}{\rightarrow} \sigma_v^2
 \end{eqnarray*}
 since $P_Z$ is an orthogonal projection, hence symmetric and idempotent. (Remember that we're talking about the first-stage here: $\mathbf{x}$ is the dependent variable.) We could also divide by $(n - p)$ rather than $n$ where $p$ is the number of instrumental variables. 

 There are two possible ways to estimate $\sigma_\epsilon^2$. We can either use the redsiduals from the OLS estimator or those from the IV estimator. Under local mis-specification, either choice gives a consistent estimator. Unless the instruments are extremely weak, however, we would expect the IV residuals to be more robust. (We could always try both and see if it makes a difference.)  

 The only remaining quantity to be estimated is $\tau^2$. From above we know that $\widehat{\tau} \overset{d}{\rightarrow} N(\tau, V)$ where
    $$V = \sigma^2_\epsilon \sigma^2_x \left(\frac{\sigma^2_x}{\gamma^2} - 1 \right)  = \sigma^2_\epsilon \sigma^2_x \left(\frac{\sigma^2_v + \gamma^2}{\gamma^2} - 1 \right) =  \sigma^2_\epsilon \sigma^2_x \left( \frac{\sigma^2_v}{\gamma^2}\right)$$
Therefore,
  $$\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} - 1 \right)= \widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2 \left(\frac{\widehat{\sigma}_v^2}{\widehat{\gamma}^2}\right)$$
is an asymptotically unbiased estimator of $\tau^2$ and accordingly
  $$\frac{\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2 \widehat{\sigma}_v^2/\widehat{\gamma}^2}{\widehat{\sigma}_\epsilon^2 \widehat{\sigma}_v^2} = \frac{\widehat{\tau}^2}{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2} - \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} =  \frac{\widehat{\tau}^2}{(\widehat{\sigma}_x^2 - \widehat{\gamma}^2) \widehat{\sigma}_\epsilon^2} - \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2}$$
is an asymptotically unbiased estimator of $\tau^2/(\sigma_\epsilon^2 \sigma_v^2)$.

Now we can construct the FMSC for the OLS versus IV example. From above we know that the AMSE of OLS is lower than that of IV whenever $\tau^2/(\sigma_\epsilon^2 \sigma_v^2)$ is less than $\sigma_x^2/\gamma^2$. Substituting our estimators of these quantities into the inequality, the FMSC tells us to use OLS whenever
  \begin{eqnarray*}
       \frac{\widehat{\tau}^2}{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2} - \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} &<& \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2}\\
       \frac{\widehat{\tau}^2}{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2} &<& 2 \frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} \\
        \widehat{T}_{FMSC} = \frac{\widehat{\tau}^2 \widehat{\gamma}^2}{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2} &<& 2
  \end{eqnarray*}
Now, the question is what is the limiting behavior of $\widehat{T}_{FMSC}$? Note that
  \begin{eqnarray*}
    \widehat{T}_{FMSC} &=& \frac{\widehat{\tau}^2}{\widehat{V}} = \frac{\widehat{\tau}^2 }{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2/\widehat{\gamma}^2} = \frac{\widehat{\tau}^2}{ \widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right)}
  \end{eqnarray*}
Hence, if $\tau = 0$ then $\widehat{T}_{FMSC} \overset{d}{\rightarrow} \chi^2(1)$. This means that we can interpret the FMSC as a test of the null hypothesis $H\colon \tau = 0$ against the two-sided alternative based on a critical value of $2$. This corresponds to a test with size  \texttt{1 - pchisq(2, df = 1)}$\approx 0.16$. Rather than leaving us free to choose the size of our test, the FMSC tells us precisely what size to use to obtain a favorable bias-variance tradeoff. We should use IV only when we reject $H_0\colon \tau = 0$ at the 16\% level. 

\paragraph{Relationship to Durbin-Hausman-Wu Test} In applied work researchers often use a Hausman test to choose between OLS and IV. The test is based on a quadratic form in the difference between the OLS and IV estimators. Using the notation of the present example, 
  \begin{eqnarray*}
    \sqrt{n}\left(\widehat{\beta} - \widetilde{\beta}\right) &=&\left[\begin{array}{cc}
    1 & -1
    \end{array}\right] \sqrt{n} \left(\begin{array}{c}
    \widehat{\beta} - \beta  \\ \widetilde{\beta} - \beta
    \end{array} \right)\\
     &\overset{d}{\rightarrow}& \left[\begin{array}{cc}
    1 & -1
    \end{array}\right]
    N\left(\left[
      \begin{array}{c}
        \tau/\sigma_x^2 \\ 0
      \end{array}\right],\sigma_\epsilon^2\left[
        \begin{array}{cc}
          1/\sigma_x^2 & 1/\sigma_x^2 \\
          1/\sigma_x^2 & 1/\gamma^2
        \end{array}
      \right]
    \right)\\
    &\sim& N\left(\tau/\sigma_x^2, \Sigma
    \right)
  \end{eqnarray*}
where
    \begin{eqnarray*}
      \Sigma &=& (\sigma_\epsilon^2/\sigma_x^2) \left[\begin{array}{cc}
    1 & -1
    \end{array}\right]
      \left[
        \begin{array}{cc}
          1&1\\
          1&\sigma_x^2/\gamma^2
        \end{array}
      \right]
    \left[\begin{array}{c}
    1 \\ -1
    \end{array}\right]\\
        &=& (\sigma_\epsilon^2/\sigma_x^2)(\sigma_x^2/\gamma^2 - 1) = \sigma_\epsilon^2/\gamma^2 - \sigma_\epsilon^2/\sigma_x^2 \\
        &=& \mbox{AVAR(2SLS)} - \mbox{AVAR(OLS)}\\
        &=& \sigma_\epsilon^2 \left(1/\gamma^2 - 1/\sigma_x^2 \right)
    \end{eqnarray*}
which agrees with the standard textbook formula in the case where OLS is efficient under the null (see e.g.\ Cameron \& Trivedi). Hence,
  \begin{eqnarray*}
    \widehat{T}_{Hausman} &=& \frac{n(\widehat{\beta} - \widetilde{\beta})^2}{\widehat{\Sigma}} = \frac{n(\widehat{\beta} - \widetilde{\beta})^2}{\widehat{\sigma}_\epsilon^2\left(1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2\right)}\overset{d}{\rightarrow} \chi^2(1)
  \end{eqnarray*}
under $H_0\colon \tau = 0$. So long as we use the same estimator of $\sigma_\epsilon^2$ for each term in the denominator, as we have here, we can be sure that the difference is positive since $\widehat{\sigma}_x^2 = \widehat{\gamma}^2 + \widehat{\sigma}_v^2$ and we showed above that $\widehat{\sigma}_v^2 = (M_Z \mathbf{x})'(M_Z \mathbf{x})/n$.


Now we will show that the test statistic for the Durbin-Hausman-Wu test is \emph{numerically identical} to the ``test statistic'' view of the FMSC for this problem. From above,
  $$\widehat{T}_{FMSC} =  \frac{\widehat{\tau}^2}{ \widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right)} = \left(\frac{\widehat{\tau}}{\widehat{\sigma}_x^4}\right)\left[\frac{1}{\widehat{\sigma}_\epsilon^2\left(1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2\right)} \right]$$
whereas,
  $$\widehat{T}_{Hausman} =\frac{n(\widehat{\beta} - \widetilde{\beta})^2}{\widehat{\sigma}_\epsilon^2\left(1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2\right)}$$
  Thus, we only need to show that $\widehat{\tau}^2/\widehat{\sigma}_x^4 = n(\widehat{\beta} - \widetilde{\beta})^2$. First notice that
\begin{eqnarray*}
   \mathbf{x}' \left(\mathbf{y} - \mathbf{x}\widetilde{\beta}\right)&=& \mathbf{x}'\mathbf{y} - \mathbf{x}'\mathbf{x}\widetilde{\beta} =\mathbf{x}'\mathbf{x}\left(\mathbf{x}'\mathbf{x}\right)^{-1} \mathbf{x}'\mathbf{y} - \mathbf{x}'\mathbf{x}\widetilde{\beta}\\
    &=&\mathbf{x}'\mathbf{x}\left[\left(\mathbf{x}'\mathbf{x}\right)^{-1} \mathbf{x}'\mathbf{y} -  \widetilde{\beta}\right] = \mathbf{x}'\mathbf{x} \left( \widehat{\beta} - \widetilde{\beta}\right)
\end{eqnarray*}
Now, from above
  $$\widehat{\tau} = \sqrt{n} \left[\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})/n\right] = n^{-1/2} \textbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})$$
so we have,
  \begin{eqnarray*}
    \widehat{\tau}^2/\widehat{\sigma}_x^4 &=&  \left[n^{-1/2} \textbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})\right]^2 = n^{-1}\left[\mathbf{x}'\mathbf{x} \left( \widehat{\beta} - \widetilde{\beta}\right) \right]^2/\widehat{\sigma}_x^4 \\
      &=& n^{-1}\left[n \widehat{\sigma}_x^2 \left( \widehat{\beta} - \widetilde{\beta}\right) \right]^2/\widehat{\sigma}_x^4  = n\left( \widehat{\beta} - \widetilde{\beta}\right) ^2
  \end{eqnarray*}
  Therefore $\widehat{T}_{FMSC} = \widehat{T}_{Hausman}$ as claimed. In other words, using the FMSC to choose between OLS and 2SLS is numerically equivalent to carrying out a DHW test at the 16\% level.


\paragraph{Implementing the FMSC via a Hausman Test}
We can use the relationship from above to implement the FMSC for this example directly. There is a potential issue involving the denominator, since this is a difference of asymptotic variance estimators. As formulated above, it is guaranteed to be positive since we use the \emph{same} sample estimator of $\sigma^2_\epsilon$ for each term of the difference in the denominator piece. (See 8.3.2 of Cameron \& Trivedi for more on this). 

There are several possible estimators for $\sigma_\epsilon^2$ as mentioned above. One is to use the OLS estimator, another is to use the 2SLS estimator. (See Dufour et al. for more possibilities.) It seems like the easiest thing to do would be to use the 2SLS estimator. I could also try the OLS estimator. Another idea would be to take the average of the two. It seems like much ink has been spilled on this topic but it's a bit of a distraction from this paper so I shouldn't spend too much time on it.

The point is this: once we have chosen an estimator of $\sigma_\epsilon^2$ we only need the following quantities to construct the DHW test statistic:
  \begin{eqnarray*}
    \widehat{\beta} &=& (\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y}\\
    \widehat{\sigma}_x^2 &=& n^{-1}\mathbf{x}'\mathbf{x} \\
    \widetilde{\beta} &=& (\mathbf{x}'Z(Z'Z)^{-1} Z'\mathbf{x})^{-1}\mathbf{x}'Z (Z'Z)^{-1} Z'\mathbf{y}\\
    \widehat{\gamma}^2 &=& n^{-1} \mathbf{x}'Z(Z'Z)^{-1}Z'\mathbf{x}
  \end{eqnarray*}
The FMSC is equivalent to carrying out this test at a particular significance level. Thus, we can easily compare it to some other significance levels and see how the performance differs. Coding this will be very straightforward: we don't even need to calculate $\widehat{\tau}$ explicitly. 

\todo[inline]{Should be fairly easy to write a function to implement this, along with the corrected confidence interval. Could give option to use various estimators for $\sigma_\epsilon^2$. Implement via model interface in R? Compare to \texttt{sem} or something like that? Just need to be careful about projecting out any exogenous regressors. This is numerically equivalent to doing things the hard way.}

\subsection{Robust Version of FMSC and Hausman Test?} % (fold)
\label{sub:robust_version_of_fmsc_and_hausman_test_}
\todo[inline]{Does the relationship continue to hold without the simplifying assumptions we used for the variance matrix?}
% subsection robust_version_of_fmsc_and_hausman_test_ (end)





\subsection{AMSE-Optimal Averaging} % (fold)
\label{sub:amse_optimal_averaging}
We showed above that the infeasible version of the FMSC selects OLS when 
  $$|\tau|  < \sigma_v \sigma_\epsilon\sqrt{\frac{\boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}}$$
But selection is a blunt instrument. A better idea is to \emph{combine} the OLS and 2SLS estimator by taking a weighted average and choose the weights to minimize AMSE. The problem is 
  $$\omega^* = \underset{\omega \in [0,1]}{\mbox{argmin }} \mbox{AMSE}\left(\omega \widehat{\beta} + (1-\omega)\widetilde{\beta}\right)  $$
  Let $\widehat{\beta}(\omega) = \omega \widehat{\beta} + (1-\omega) \widetilde{\beta}$. Then,
    \begin{eqnarray*}
      \mbox{Bias}\left(\widehat{\beta}(\omega)\right) &=& \mbox{Bias}(\widehat{\beta}) + \mbox{Bias}(\widetilde{\beta})=\omega\left( \frac{\tau}{\sigma_x^2}\right)
    \end{eqnarray*}
  and
  \begin{eqnarray*}
    Var\left(\widehat{\beta}(\omega)\right) &=& 
      \left[ \begin{array}{cc}
        \omega & 1-\omega
      \end{array}\right]
       \sigma_\epsilon^2 \left[
      \begin{array}{cc}
       1/\sigma_x^2 & 1/\sigma_x^2 \\
       1/\sigma_x^2 & 1/\gamma^2 
      \end{array}\right]\left[
      \begin{array}{c}
          \omega \\ 1- \omega
      \end{array}\right]\\
      &=& \sigma_\epsilon^2/\sigma_x^2 
      \left[ \begin{array}{cc}
        \omega & 1-\omega
      \end{array}\right]
          \left[\begin{array}{cc} 
          1& 1\\ 1& \sigma_x^2/\gamma^2
          \end{array} \right]
      \left[
      \begin{array}{c}
          \omega \\ 1- \omega
      \end{array}\right]\\
        &=&\frac{\sigma_\epsilon^2}{\sigma_x^2}\left\{ \omega + (1- \omega)\left[\omega + (1-\omega)\sigma_x^2/\gamma^2 \right]\right\} \\
        &=&\frac{\sigma_\epsilon^2}{\sigma_x^2}\left[ \omega + (1- \omega)\omega + (1-\omega)^2\sigma_x^2/\gamma^2 \right] \\
          &=& \frac{\sigma_\epsilon^2}{\sigma_x^2}\left[2\omega - \omega^2 + \frac{\sigma_x^2}{\gamma^2} \left(1 - 2\omega + \omega^2 \right) \right]\\
          &=& \frac{\sigma_\epsilon^2}{\sigma_x^2} \left[(2\omega - \omega^2)\left( 1 - \frac{\sigma_x^2}{\gamma^2}\right)+\frac{\sigma_x^2}{\gamma^2} \right]\\
          &=& \frac{\sigma_\epsilon^2}{\sigma_x^2} \left[(2\omega - \omega^2)\left( 1 - \frac{\sigma_x^2}{\gamma^2}\right)+\frac{\sigma_x^2}{\gamma^2} \right]
  \end{eqnarray*}
  and accordingly
    \begin{eqnarray*}
      \mbox{AMSE}\left(\widehat{\beta}(\omega)\right) &=& \omega^2 \left(\frac{\tau^2}{\sigma_x^4} \right) + (2\omega - \omega^2)\left(\frac{\sigma_\epsilon^2}{\sigma_x^2}\right)\left( 1 - \frac{\sigma_x^2}{\gamma^2}\right) + \frac{\sigma_\epsilon^2}{\gamma^2}\\
      &=& \omega^2 \left(\frac{\tau^2}{\sigma_x^4} \right) + (2\omega - \omega^2)\left(\frac{\sigma_\epsilon^2}{\sigma_x^2}\right)\left(\frac{\gamma^2 - \sigma_x^2}{\gamma^2}\right) + \frac{\sigma_\epsilon^2}{\gamma^2}\\
      &=& \omega^2 \left(\frac{\tau^2}{\sigma_x^4} \right) + (2\omega - \omega^2)\left(\frac{\sigma_\epsilon^2}{\sigma_x^2}\right)\left(\frac{-\sigma_v^2}{\boldsymbol{\pi}'Q_z\boldsymbol{\pi}}\right) + \frac{\sigma_\epsilon^2}{\gamma^2}\\
      &=& \omega^2 \left(\frac{\tau^2}{\sigma_x^4} \right) + (2\omega - \omega^2)\left(\frac{-\sigma_\epsilon^2}{\sigma_x^2\kappa^2}\right)+ \frac{\sigma_\epsilon^2}{\gamma^2}\\
        &=& a\omega^2 + b(2\omega - \omega^2) + c
    \end{eqnarray*}
  so the first order condition for a minimum is
    \begin{eqnarray*}
      2a\omega + b(2 - 2\omega) &=&0\\
      2(a-b) \omega + 2b &=& 0\\
      \omega^* &=& b/(b-a)\\
      \omega^* &=& \frac{1}{1 - a/b}
    \end{eqnarray*}
    Now, 
    \begin{eqnarray*}
       a &=&\frac{\tau^2}{\sigma_x^4} = \mbox{ABIAS(OLS)}^2 \\
       b &=& \frac{-\sigma_\epsilon^2}{\kappa^2 \sigma_x^2} = \sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(\frac{1}{\kappa^2}\right)\right] = - \sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(\frac{\sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}\right)\right] \\
          &=& - \sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(\frac{\sigma_x^2 - \boldsymbol{\pi}'Q_z \boldsymbol{\pi}}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}\right)\right]=-\sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(\frac{\sigma_x^2 }{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}-1\right)\right]=\sigma_\epsilon^2\left[ \frac{1}{\sigma_x^2} \left(1-\frac{\sigma_x^2 }{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}\right)\right]\\
          &=&\sigma_\epsilon^2\left[  \frac{1}{\sigma_x^2}-\frac{1}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}}\right] =  \frac{\sigma_\epsilon^2}{\sigma_x^2}-\frac{\sigma_\epsilon^2}{\gamma^2}  = \mbox{AVAR(OLS)}-\mbox{AVAR(2SLS)}
     \end{eqnarray*} 

  Substituting the definitions of $a$ and $b$ we have
    \begin{eqnarray*}
      \omega^* &=& \left[ 1 - \frac{a}{b}\right]^{-1}\\
      &=&\left[1 - \frac{\mbox{ABIAS(OLS)}^2}{\mbox{AVAR(OLS)}- \mbox{AVAR(2SLS)}} \right]^{-1}\\
      &=&\left[1 + \frac{\mbox{ABIAS(OLS)}^2}{\mbox{AVAR(2SLS)}-\mbox{AVAR(OLS)}} \right]^{-1}\\
      &=& \left[1 + \frac{\tau^2/\sigma_x^4}{\sigma_\epsilon^2(1/\gamma^2 - 1/\sigma_x^2)}\right]^{-1}
      = \left[1 + \frac{\tau^2}{\sigma_\epsilon^2 \sigma_x^4 (1/\gamma^2 - 1/\sigma_x^2)}\right]^{-1}\\
      &=& \left[1 + \frac{\tau^2}{\sigma_\epsilon^2 \sigma_x^2}\left( \frac{1}{\sigma_x^2(1/\gamma^2 - 1/\sigma_x^2)}\right)\right]^{-1}
      = \left[1 + \frac{\tau^2}{\sigma_\epsilon^2 \sigma_x^2}\left( \frac{1}{\sigma_x^2/\gamma^2 - 1}\right)\right]^{-1}\\
      &=& \left[1 + \frac{\tau^2}{\sigma_\epsilon^2 \sigma_x^2}\left( \frac{\gamma^2}{\sigma_x^2 - \gamma^2}\right)\right]^{-1}
      = \left[1 + \left|\frac{\tau}{\sigma_\epsilon \sigma_x}\right|^2\left( \frac{\gamma^2}{\sigma_v^2}\right)\right]^{-1}
      = \left[1 +\kappa^2 \left|\frac{\tau}{\sigma_\epsilon \sigma_x}\right|^2\right]^{-1}\\
      &=& \left[1 + \left|\frac{\tau}{\sigma_\epsilon \sigma_v}\right|^2\left( \frac{\gamma^2}{\sigma_x^2}\right)\right]^{-1}
      = \left[1 + R^2 \left|\frac{\tau}{\sigma_\epsilon \sigma_v}\right|^2\right]^{-1} = \frac{1}{1 + R^2 \rho_{\epsilon v}^2}\\
      &=& \left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-1}
    \end{eqnarray*}
    Notice that this agrees with my earlier derivations for the special case in which there is only one instrument since $\kappa^2 = R^2/(1 - R^2)$. The second order condition for a minimum is is $2(a-b) > 0$ which is equivalent to $a > b$. From above, $a$ is the squared asymptotic bias of the OLS estimator: $\tau^2/\sigma_x^4$. This is strictly postitive so long as $\tau \neq 0$. Also from above, $b$ is the difference between the asymptotic variance of the OLS estimator and the 2SLS estimator: $\sigma_\epsilon^2(1/\sigma_x^2 - 1/\gamma^2)$. Since $\sigma_x^2 = \gamma^2 + \sigma_v^2$, this quantity is strictly negative so long as $\sigma_v^2 \neq 0$. Thus, the second order condition holds except in the trivial case where the instruments are perfect predictors of $x$.
  \todo[inline]{The finite sample version of this equates $\tau^2/(\sigma_v^2 \sigma_\epsilon^2)$ with $\rho_{\epsilon v}^2$. There remains the question of which parameters can be varied independently and hence what the best way to express this is.}

Differentiating,
  \begin{eqnarray*}
    \frac{\partial \omega^*}{\partial \kappa^2} &=& -\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2} \left( \frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2} \right) \frac{d}{d\kappa^2} \left(\frac{\kappa^2}{1 + \kappa^2} \right)\\
      &=&  -\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2} \left( \frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2} \right)  \left(\frac{(1+ \kappa^2) - \kappa^2}{(1 + \kappa^2)^2} \right)\\
      &=& -\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2} \left( \frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2} \right)  \left(\frac{1}{(1 + \kappa^2)^2} \right) < 0\\
    \frac{\partial \omega^*}{\partial \tau^2} &=&-\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2}\left(\frac{\kappa^2}{1 + \kappa^2} \right)\frac{d}{d\tau^2}\left( \frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2} \right) \\
    &=&-\left[1 + \left(\frac{\tau^2}{\sigma_\epsilon^2 \sigma_v^2}\right)\left( \frac{\kappa^2}{1 + \kappa^2}\right)\right]^{-2}\left(\frac{\kappa^2}{1 + \kappa^2} \right)\left( \frac{1}{\sigma_\epsilon^2 \sigma_v^2} \right) < 0 
  \end{eqnarray*}
The result makes intuitive sense. As $\kappa^2$ increases, the instruments $z$ become stronger so we give \emph{less} weight to OLS and more to 2SLS. As the $\kappa^2$ decreases, the instruments become weaker so we give \emph{more} weight to OLS and less to 2SLS. Similarly, as $\tau^2$ increases the regressor $x$ becomes more endogenous so we give \emph{less} weight to OLS and more to 2SLS. As $\tau^2$ decreases the regressor $x$ becomes less endogenous, so we give \emph{more} weight to OLS and less to 2SLS.

\paragraph{Is there an interior optimum?} To solve for $\omega^*$ we minimized the function 
    $$g(\omega) = a\omega^2 + b(2\omega - \omega^2) + c = (a - b)\omega^2 + 2b\omega + c$$ 
where $a = \mbox{ABIAS(OLS)}^2$ and $b = \mbox{AVAR(OLS) - AVAR(2SLS)}$. Since it is a squared bias, $a \geq 0$ and since OLS has a strictly lower asymptotic variance than 2SLS so long as the instruments aren't perfect predictors of $x$, $b \leq 0$. This implies that $(a - b)$ is \emph{positive}. Hence, the second derivative is strictly positive \emph{for all $x$} meaning that we are minimizing a \emph{globally convex function}, namely a quadratic. This means that the first order condition is both necessary and sufficient for a minimum. The only remaining question is whether the solution $\omega^* = 1/(1 - a/b)$ lies inside the interval $[0,1]$. Now, we'll assume that the instruments are \emph{not} perfect predictors of $x$ so the inequality for $b$ becomes strict, i.e.\ $b < 0$. Since $a \geq 0$, it follows that $-a/b \geq 0$ so that $1 - a/b \geq 1$. Hence,
$\omega^* = 1/(1 - a/b) \leq 1$. Since the denominator cannot be negative, we have $0 < \omega^* \leq 1$. Hence the optimum is \emph{always interior}.
 


% subsection amse_optimal_averaging (end)

\subsection{Feasible AMSE-Optimal Weighting} % (fold)
\label{sub:feasible_amse_optimal_weighting}
Since $\omega^*$ depends on unknowns, we need to estimate it. Recall from above that
  $$\omega^* = \left[1 + \frac{\mbox{ABIAS(OLS)}^2}{\mbox{AVAR(2SLS)}-\mbox{AVAR(OLS)}} \right]^{-1} = \left[1 + \frac{\tau^2/\sigma_x^4}{\sigma_\epsilon^2 (1/\gamma^2 - 1/\sigma_x^2)}\right]^{-1}$$ 
For $\sigma_x^2$, $\gamma^2$ and $\sigma_\epsilon^2$ we will use the obvious consistent estimators $\widehat{\sigma}_x^2$, $\widehat{\gamma}^2$ and $\widehat{\sigma}_\epsilon^2$. 
\todo[inline]{There is a question of whether $\sigma_\epsilon^2$ should be estimated based on the residuals constrcted from the OLS estimator or those constructed from the 2SLS estimator.}

For $\tau^2$ the situation is more complicated. As explained above, no consisted estimator of $\tau$ exsists although we can construct an asymptotically unbiased estimator of $\widehat{\tau}^2$. In the FMSC expression this yields an asymptotically unbiased estimator of AMSE. However here we will be substituting our estimator of $\tau^2$ into a \emph{highly nonlinear} function so there is no obvious way to preserve the asymptotic unbiasedness. Nevertheless, the most obvious idea is to substitute our asymptotically unbiased estimator of $\tau^2$ from above: 
$$\widehat{\tau}^2 - \widehat{V} = \widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} - 1 \right)$$
The problem is that, while $\tau^2$ is always greater than or equal to zero as is $\widehat{\tau}^2$, the difference $\widehat{\tau}^2 - \widehat{V}$ \emph{can easily be negative}. This gives a \emph{negative} estimate of $\mbox{ABIAS(OLS)}^2$ The obvious solution to this problem, borrowing an idea from James-Stein estimation, is to work with the positive part instead, namely $\max\left\{0, \; \widehat{\tau}^2 - \widehat{V}\right\}$. This will ensure that our estimate of $a$, using the notation of the preceding section, will satisfy the same inequalities as the population quantity ensuring that our estimate of $\omega^*$ is interior to $[0,1]$. (The estimator of $b$ automatically satisfies the corresponding inequalities for the population quantity). Our sample estimate of the AMSE-optimal weight on the OLS estimator is thus
  $$\widehat{\omega }^* = \left[1 + \frac{\max \left\{0, \; \left(\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right) \right)/\;\widehat{\sigma}_x^4 \right\}}{\widehat{\sigma}_\epsilon^2 (1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2)}\right]^{-1}$$

% subsection feasible_amse_optimal_weighting (end)

\subsection{Post-Selection/Averaging Inference} % (fold)
\label{sub:valid_confidence_interval_for_ols_vs_2sls_example}

\subsubsection{``Myopic'' Confidence Intervals}
These intervals assume that the selected model is correctly specified and ignore any uncertainty from the model selection step. They are constructed from the following asymptotic results:
  \begin{eqnarray*}
    \mbox{OLS:} \quad \sqrt{n}(\widehat{\beta} - \beta) \overset{d}{\rightarrow} N(0, \sigma_\epsilon^2/\sigma_x^2)\\
    \mbox{IV:} \quad \sqrt{n}(\widetilde{\beta} - \beta) \overset{d}{\rightarrow} N(0, \sigma_\epsilon^2/\gamma^2)\\
  \end{eqnarray*}
The reason the OLS estimator has a zero mean is that we are assuming here that, conditional on it being selected, it is correctly specified so that $\tau = 0$. To turn these results in to a procedure for constructing confidence interval, we'll plug in the estimators
 \begin{eqnarray*}
    \widehat{\beta} &=& (\mathbf{x}'\mathbf{x})^{-1}\mathbf{x}'\mathbf{y}\\
    \widehat{\sigma}_x^2 &=& n^{-1}\mathbf{x}'\mathbf{x} \\
    \widetilde{\beta} &=& (\mathbf{x}'Z(Z'Z)^{-1} Z'\mathbf{x})^{-1}\mathbf{x}'Z (Z'Z)^{-1} Z'\mathbf{y}\\
    \widehat{\gamma}^2 &=& n^{-1} \mathbf{x}'Z(Z'Z)^{-1}Z'\mathbf{x}
  \end{eqnarray*}
from above. For $\sigma_\epsilon^2$, we'll use the error variance estimator that \emph{corresponds} to each estimator. In other words, 
  \begin{eqnarray*}
    \mbox{OLS:} \quad \widehat{\sigma}_\epsilon^2 = n^{-1} \left(\mathbf{y} - \mathbf{x}\widehat{\beta}\right)'\left(\mathbf{y} - \mathbf{x}\widehat{\beta}\right)\\
    \mbox{IV:} \quad \widetilde{\sigma}_\epsilon^2 = n^{-1} \left(\mathbf{y} - \mathbf{x}\widetilde{\beta}\right)'\left(\mathbf{y} - \mathbf{x}\widetilde{\beta}\right)
  \end{eqnarray*}
Note that for each we use the MLE rather than the unbiased variance estimator. It shouldn't make a big difference either way, of course. The resulting intervals are as follows:
  \begin{eqnarray*}
    \mbox{OLS:} \quad \widehat{\beta} \pm z_{\alpha/2} \cdot \frac{1}{\sqrt{n}} \left(\frac{\widehat{\sigma}_\epsilon^2}{\widehat{\sigma}_x^2} \right)^{1/2}\\
    \mbox{IV:} \quad \widetilde{\beta} \pm z_{\alpha/2} \cdot \frac{1}{\sqrt{n}} \left(\frac{\widetilde{\sigma}_\epsilon^2}{\widehat{\gamma}^2} \right)^{1/2}
  \end{eqnarray*}
\todo[inline]{Should I be using heteroscedasticity-consistent standard errors here?}

\subsubsection{Post-FMSC Corrected CIs}
The post-FMSC estimator is
  $$\beta^*_{FMSC} = \textbf{1}\left\{\widehat{T}_{FMSC} < 2 \right\} \widehat{\beta} + \textbf{1}\left\{\widehat{T}_{FMSC} \geq 2 \right\} \widetilde{\beta}$$
where
  $$\widehat{T}_{FMSC} = \frac{\widehat{\tau}^2 }{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2/\widehat{\gamma}^2}$$    
Since the weights sum to one, we can write
  $$\sqrt{n}(\beta^*_{FMSC} - \beta) = \textbf{1}\left\{\widehat{T}_{FMSC} < 2 \right\} \sqrt{n}(\widehat{\beta} - \beta) + \textbf{1}\left\{\widehat{T}_{FMSC} \geq 2 \right\}\sqrt{n}( \widetilde{\beta}-\beta)$$
The limit of $\sqrt{n}(\beta^*_{FMSC} - \beta)$ is $\Lambda(\tau)$ in the notation of the paper. From above we have
  $$
\left[
\begin{array}{c}
  \sqrt{n}\left(\widehat{\beta} - \beta\right) \\
  \sqrt{n}\left(\widetilde{\beta} - \beta\right)\\
  \widehat{\tau}
\end{array}
\right] \overset{d}{\rightarrow} 
\left[\begin{array}{cc}
1/\sigma_x^2 & \mathbf{0}'\\
  0 & \boldsymbol{\pi}'/\gamma^2\\
  1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}
  \right]\left( \left[\begin{array}{c} \tau \\ \mathbf{0}  \end{array} \right] + M\right)
$$
where $M \sim \mathcal{N}(0, \Omega)$ and
  \begin{eqnarray}
 \sigma_x^2&=&\boldsymbol{\pi}'Q_z \boldsymbol{\pi}  +\sigma^2_v \\
    \gamma^2 &=& \boldsymbol{\pi}'Q_z \boldsymbol{\pi}\\
    \Omega &=& \sigma_\epsilon^2\left[ \begin{array}{cc}
\sigma_x^2
&\boldsymbol{\pi}'Q_z\\
  Q_z \boldsymbol{\pi}&Q_z
  \end{array}\right]
  \end{eqnarray}
Following the general procedure from the paper, we'll simulate draws $M_j$, substituting consistent estimators of all unknowns except for $\tau$, for which we'll construct a confidence interval.

\paragraph{First Step: Confidence Region for $\tau$}
We'll use the fact, shown above, that $\widehat{\tau} \overset{d}{\rightarrow} N(\tau,V)$ where 
$$V = \sigma_\epsilon^2 \frac{\sigma_x^2\sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}} = \sigma^2_\epsilon \sigma^2_x \left(\sigma_x^2/\gamma^2 - 1 \right)$$
Define a consistent estimator $\widehat{V}$ of $V$ as follows
  $$\widehat{V} = \widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right)$$
For $\widehat{\sigma}^2_x$ and $\widehat{\gamma}^2$, we can simply use the estimators from above:
 \begin{eqnarray*}
    \widehat{\sigma}_x^2 &=& n^{-1}\mathbf{x}'\mathbf{x} \\
    \widehat{\gamma}^2 &=& n^{-1} \mathbf{x}'Z(Z'Z)^{-1}Z'\mathbf{x}
  \end{eqnarray*}
To estimate $\sigma_\epsilon^2$, there are two options: using the resdiduals formed from $\widetilde{\beta}$ or those formed from $\widehat{\beta}$. 
\todo[inline]{Don't worry about making a decision about this for the moment, but make sure to come back to it.}

Letting $z_{\delta/2}$ denote the $\alpha/2$ quantile of a standard normal distribution,
  $$\widehat{\tau} \pm z_{\delta/2} \; \sqrt{\widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\frac{\widehat{\sigma}_x^2}{\widehat{\gamma}^2} - 1 \right)}$$
is a $(1-\delta) \times 100\%$ asymptotic confidence interval for $\tau$.

\paragraph{Second Step: Simulation-Based Interval Conditional on $\tau$}
Let $\tau^*$ be an arbitrary value inside the confidence interval for $\tau$ from above. Now we will \emph{condition} on this value and calculate a confidence interval for $\beta^*_{FMSC}$ by simulation. To make the notation a little simpler, we'll assign names to each of the limiting random variables. Define
$$
\left[\begin{array}
  {c}
  A(\tau) \\ B \\ C(\tau) 
\end{array}\right] = \left[\begin{array}
  {c}
  \tau/\sigma_x^2  \\ 0 \\ \tau
\end{array}\right] + \left[\begin{array}{cc}
1/\sigma_x^2 & \mathbf{0}'\\
  0 & \boldsymbol{\pi}'/\gamma^2\\
  1 & (-\sigma_x^2/\gamma^2) \boldsymbol{\pi}'
  \end{array}
  \right]\left[ \begin{array}
    {c} M_1 \\ M_2
  \end{array}\right]$$
where 
  $$\left[\begin{array}
    {c} M_1 \\ M_2
  \end{array} \right] \sim N(0, \Omega)$$
and 
  $$\Omega = \sigma_\epsilon^2\left[ \begin{array}{cc}
\sigma_x^2
&\boldsymbol{\pi}'Q_z\\
  Q_z \boldsymbol{\pi}&Q_z
  \end{array}\right]$$
Using this notation, we have
 $$\left[
\begin{array}{c}
  \sqrt{n}\left(\widehat{\beta} - \beta\right) \\
  \sqrt{n}\left(\widetilde{\beta} - \beta\right)\\
  \widehat{\tau}
\end{array}
\right] \overset{d}{\rightarrow} \left[\begin{array}
  {c}
  A(\tau) \\ B \\ C(\tau) 
\end{array}\right]$$
Now let $\Lambda(\tau)$ denote the limiting random variable to which $\sqrt{n}\left(\beta^*_{FMSC} - \beta\right)$ converges. Then we have
  \begin{eqnarray*}
    \Lambda(\tau) &=& \mathbf{1}\left\{ \frac{C^2(\tau)}{V} < 2 \right\} A(\tau) +  \mathbf{1}\left\{ \frac{C^2(\tau)}{V} \geq 2 \right\} B\\
    &=& \mathbf{1}\left\{ C^2(\tau)< 2 V \right\} A(\tau) +  \mathbf{1}\left\{ C^2(\tau)\geq 2 V \right\} B
  \end{eqnarray*}
by the continuous mapping theorem since the indicator functions are almost-surely continuous. 


\begin{enumerate}
  \item For $j = 1, \hdots, J$ generate
    $$\left[ \begin{array}
      {c} M_1^{(j)} \\ M_2^{(j)}
    \end{array}\right] \overset{\mbox{iid}}{\sim} N\left( \left[ \begin{array}
      {c} 0 \\ \textbf{0}\\
    \end{array}\right], \;\widehat{\sigma}_\epsilon^2\left[ \begin{array}
      {cc}
      \widehat{\sigma}_x^2& \widehat{\boldsymbol{\pi}}' \widehat{Q}_z\\
      \widehat{Q}_z\widehat{\boldsymbol{\pi}} & \widehat{Q}_z
    \end{array}\right] \right) $$
  \item Set
    \begin{eqnarray*}
        A_j(\tau) &=&  \left(\tau + M_1^{(j)} \right)/\; \widehat{\sigma}_x^2\\    
        B_j &=& \widehat{\boldsymbol{\pi}'}M_2^{(j)}/\; \widehat{\gamma}^2\\
        C_j(\tau)&=& \tau + M_1^{(j)} - (\widehat{\sigma}_x^2/\widehat{\gamma}^2) \widehat{\boldsymbol{\pi}}' M_2^{(j)}
    \end{eqnarray*}
  \item Set 
    $$\Lambda_j(\tau) = \mathbf{1}\left\{ C_j^2(\tau)< 2 \widehat{V} \right\} A_j(\tau) +  \mathbf{1}\left\{ C_j^2(\tau)\geq 2 \widehat{V} \right\} B_j$$
    where $\widehat{V} = \widehat{\sigma}^2_\epsilon \widehat{\sigma}^2_x \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right)$.
\end{enumerate}
Now, \emph{conditional on $\tau$} we can use the draws $\Lambda_j(\tau)$ to construct a confidence interval for $\beta$ post-FMSC selection as follows. Let $\widehat{a}(\tau)$ be the $\alpha/2$ quantile of the draws $\Lambda_j(\tau)$ and $\widehat{b}(\tau)$ be the $1- \alpha/2$ quantile. To get rid of the conditioning on $\tau$ we simply minimize $\widehat{a}(\tau)$ and maximize $\widehat{b}(\tau)$ over all values of $\tau$ in the confidence interval 
  $$\mathbf{T} = \left[ \widehat{\tau} - z_{\delta/2} \; \sqrt{\widehat{V}},  \; \widehat{\tau} + z_{\delta/2} \; \sqrt{\widehat{V}} \right]$$
Specifically, define
    \begin{eqnarray*}
      \widehat{a}_{min} &=& \min_{\tau \in \mathbf{T}} \widehat{a}(\tau)\\
      \widehat{b}_{max} &=& \max_{\tau \in \mathbf{T}} \widehat{b}(\tau)
    \end{eqnarray*}
Then $[\widehat{a}_{min} , \widehat{b}_{max}]$ is a confidence interval for $\Lambda(\tau)$ with asymptotic coverage of at least $1 - (\alpha + \delta)$ that \emph{does not depend on $\tau$}. Since $\Lambda(\tau)$ is the limit random variable to which $\sqrt{n}(\beta^*_{FMSC} - \beta)$ converges, we can use convert this into a confidence interval for $\beta$ based on $\beta^*_{FMSC}$ as follows. First substitute $\sqrt{n}(\beta^*_{FMSC} - \beta)$ for $\Lambda(\tau)$
$$P\left\{ \widehat{a}_{min} \leq  \sqrt{n}(\beta^*_{FMSC} - \beta) \leq \widehat{b}_{max}\right\} \geq 1 - (\alpha + \delta)$$
Rearranging,
\begin{eqnarray*}
   P\left\{ \widehat{a}_{min}/\sqrt{n} \leq  (\beta^*_{FMSC} - \beta) \leq \widehat{b}_{max}/\sqrt{n}\right\} &\geq& 1 - (\alpha + \delta)\\ 
   P\left\{ \widehat{a}_{min}/\sqrt{n} - \beta^*_{FMSC}  \leq - \beta \leq \widehat{b}_{max}/\sqrt{n} - \beta^*_{FMSC}  \right\} &\geq& 1 - (\alpha + \delta)\\
P\left\{\beta^*_{FMSC} -  \widehat{b}_{max}/\sqrt{n}  \leq  \beta \leq \beta^*_{FMSC} - \widehat{a}_{min}/\sqrt{n} \right\} &\geq& 1 - (\alpha + \delta)\\
 \end{eqnarray*} 
Thus, 
  $$\left[ \beta^*_{FMSC}  - \frac{\widehat{b}_{max}}{\sqrt{n}}, \; \beta^*_{FMSC}  - \frac{\widehat{a}_{min}}{\sqrt{n}} \right]$$
is a confidence interval for $\beta$ post-FMSC moment selection with asymptotic coverage of at least $1 - (\alpha + \delta)$.
% subsection valid_confidence_interval_for_ols_vs_2sls_example (end)


\subsubsection{Corrected CIs for DHW pre-test Estimators}
This is almost exactly the same as for post-FMSC described above. Simply replace 2 with the critical value for the desired test.


\subsubsection{Corrected CIs for Averaging Estimator}
The feasible version of the AMSE-optimal averaging estimator is
  $$\beta^*_{Avg} = \widehat{\omega}^* \widehat{\beta} + (1 - \widehat{\omega}^*)\widetilde{\beta}$$
where
$$\widehat{\omega }^* = \left[1 + \frac{\max \left\{0, \; \left(\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right) \right)/\;\widehat{\sigma}_x^4 \right\}}{\widehat{\sigma}_\epsilon^2 (1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2)}\right]^{-1}$$
Since the weights sum to one, we can write
  $$\sqrt{n}\left(\beta^*_{Avg}  - \beta \right) =\widehat{\omega}^* \sqrt{n}\left(\widehat{\beta} - \beta \right) +(1 - \widehat{\omega}^*) \sqrt{n}\left(\widetilde{\beta}  - \beta \right)$$
Now we can proceed to describe a simulation-based procedure to construct valid confidence intervals by direct analogy with the post-FMSC selection estimator. The only difference here is the form of the weight function which affects only the third step of the algorithm, the step in which we construct the draws $\Lambda_j(\tau)$. For the case of the averaging estimator, we set
  $$\Lambda_j(\tau) = \widehat{\omega}^*(C_j^2(\tau)) A_j(\tau) + \left[1 -  \widehat{\omega}^*(C_j^2(\tau)) \right]B_j$$
where
  $$\widehat{\omega}^*(C_j^2(\tau)) = \left[1 + \frac{\max \left\{0, \; \left(C^2_j(\tau) - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2  \left(\widehat{\sigma}_x^2/\widehat{\gamma}^2 - 1 \right) \right)/\;\widehat{\sigma}_x^4 \right\}}{\widehat{\sigma}_\epsilon^2 (1/\widehat{\gamma}^2 - 1/\widehat{\sigma}_x^2)}\right]^{-1}$$
All other steps of the procedure are identical.

\section{Choosing Instruments Example}
\todo[inline]{For this example it's probably worth considering what happens if we constrain the squared bias estimator to be non-negative. There wasn't any need to do this when using the FMSC for the other example since the ``test statistic'' view of the criterion didn't directly involve comparisons of AMSE values. Could try it both ways in the simulation.}

\subsection{Overview}
In this example we suppose that we have a minimal set of valid instruments that identifies the model. We have another set of instruments that we expect to be stronger and, as a consequence, are worried might be slightly endogenous. The question is whether to include the extra instruments. This is the situation examined in the empirical example.

Without loss of generality suppose that there are no exogenous regressors (they can always be projected out)
\begin{eqnarray}
    y_i &=& \mathbf{x}_i' \beta +  \epsilon_i\\
    \mathbf{x}_i' &=& \mathbf{z}_{i1}' \Pi_1 + \mathbf{z}_{i2}' \Pi_2 + \mathbf{v}_i' 
\end{eqnarray}
Stacking observations in the usual way,
\begin{eqnarray}
  \mathbf{y} &=& X\beta +\boldsymbol{\epsilon}\\
  X &=&  Z \Pi + \mathbf{v}
\end{eqnarray}
where $Z = (Z_1, Z_2)$ and $\Pi = (\Pi_1', \Pi_2')'$. Let $n$ be the number of observations, $p$ be the number of (endogenous)regressors $\textbf{x}$, and $q$ be the total number of instruments. Then, $X$ is an $n\times p$ matrix, $Z$ is an $n\times q$ matrix, and $\Pi$ is a $q\times p$ matrix. Let $q_1$ be the number of instruments in $Z_1$ and $q_2$ be the number of instruments in $Z_2$ so that $q = q_1 + q_2$. The local mis-specification assumption is given by
\begin{equation}
  E_n\left[\begin{array}
    {c}
    \textbf{z}_{i1} (y_i - \textbf{x}_i\beta) \\
    \textbf{z}_{i2} (y_i - \textbf{x}_i \beta)
\end{array}\right] = \left[
  \begin{array}
    {c}
    \textbf{0} \\ \boldsymbol{\tau}/\sqrt{n}
  \end{array}
\right]
\end{equation}
and we can calculate the endogeneity of $x$ as follows
\begin{eqnarray*}
  E_n[\mathbf{x}_i \epsilon_i] &=& E_n[(\Pi_1'\mathbf{z}_{i1} +  \Pi_2'\mathbf{z}_{i2} + \mathbf{v}_i)\epsilon_i]\\
    &=&\Pi_1' E_n[\mathbf{z}_{i1}\epsilon_i] + \Pi_2' E_n[\mathbf{z}_{i2}\epsilon_i] + E_n[\epsilon_i v_i]\\
    &=& \Pi_2' \boldsymbol{\tau}/\sqrt{n} + \sigma_{\epsilon v}
\end{eqnarray*}
where we are assuming that $Cov(\epsilon_i v_i) = \sigma_{\epsilon v}$ does not change with sample size.
\todo[inline]{We see from this that the endogenity of $x$ changes with both the strength and endogeneity of $\mathbf{z}_{i2}$. If we want to compensate for this in the simulation design, we could change $\sigma_{\epsilon v}$ to keep the endogeneity of $x$ fixed. No sure if it makes sense to do this or not.}


\subsection{Limit Distributions of the Estimators}
We'll consider generalized instrumental variables estimators of the following form
$$\widehat{\beta}_S = (X'Z_S W^S_n Z_S' X)^{-1}X'Z_S W^S_n Z_S' \mathbf{y}$$
where $S$ indexes the instrument set under consideration and we always include $Z_1$ in the instrument set. Expanding,
  \begin{eqnarray*}
    \widehat{\beta}_S &=& (X'Z_S W^S_n Z_S' X)^{-1}X'Z_S W^S_n Z_S' \mathbf{y}\\
      &=& (X'Z_S W^S_n Z_S' X)^{-1}X'Z_S W^S_nZ_S'(X\beta + \boldsymbol{\epsilon})\\
      &=& \beta +(X'Z_S W^S_n Z_S' X)^{-1}X'Z_S W^S_n Z_S'\boldsymbol{\epsilon}
  \end{eqnarray*}
and hence, 
\begin{eqnarray*}
  \sqrt{n}(\widehat{\beta}_S -\beta) &=& \sqrt{n}(X'Z_S W^S_n Z_S' X)^{-1}X'Z_S W^S_n Z_S'\boldsymbol{\epsilon}\\
      &=& \left[(X'Z_S/n) W^S_n (Z_S' X/n)\right]^{-1}(X'Z_S/n) W^S_n  \left( \Xi_S \; n^{-1/2} Z' \boldsymbol{\epsilon}\right)\\
      &=& \widehat{K}_n^S \Xi_S \left(  n^{-1/2} Z' \boldsymbol{\epsilon}\right)
\end{eqnarray*}
where we have used the fact that $Z_S'  = \Xi_S Z'$ and have defined
  \begin{eqnarray*}
    \widehat{K}_n^S &=& n(X'Z \Xi_S' W^S_n \Xi_S Z X)^{-1}X' Z \Xi_S' W^S_n \\
                    &=& n(X'Z_S W^S_n Z_S' X)^{-1}X'Z_S W^S_n\\
        &=&\left[(X'Z_S/n) W^S_n (Z_S' X/n)\right]^{-1}(X'Z_S/n) W^S_n
  \end{eqnarray*}
Note that $\Xi_S$ is an $|S|\times q$ matrix. Now, we have
  \begin{eqnarray*}
     n^{-1}X' Z_S &=& n^{-1}(Z\Pi + \mathbf{v})' Z_S = n^{-1}\left(\Pi' Z' Z_S + \mathbf{v}' Z_S\right)\\
      &=&n^{-1}\Pi' Z' Z\Xi_S' + n^{-1}\mathbf{v}'Z \Xi_S' \\
      &=& \Pi' \left(\frac{1}{n}\sum_{i=1}^n \mathbf{z}_i  \mathbf{z}_i'\right) \Xi_S' + \left(\frac{1}{n} \sum_{i=1}^n \mathbf{v}_i \mathbf{z}_i' \right)\Xi_S'\\
      &=& \left[ \Pi' \left(\frac{1}{n}\sum_{i=1}^n \mathbf{z}_i  \mathbf{z}_i'\right)  + \left(\frac{1}{n} \sum_{i=1}^n \mathbf{v}_i \mathbf{z}_i' \right)\right]\Xi_S'\\
      &\overset{p}{\rightarrow}& \Pi' Q_Z \Xi_S'
  \end{eqnarray*}
where $Q_z = \lim_{n\rightarrow \infty}E_n[\mathbf{z}_i \mathbf{z}_i']$ and we have used the fact that $E_n[\mathbf{v}_i \mathbf{z}_i'] = 0$. It follows that
  \begin{eqnarray*}
    \widehat{K}_n^S &\overset{p}{\rightarrow}& K_S =   \left(\Pi' Q_Z \Xi_S' W_S \Xi_S Q_Z' \Pi\right)^{-1} \Pi'Q_z \Xi_S'W_S
  \end{eqnarray*}
Now, applying Lemma \ref{lem:CLT} to $n^{-1/2}Z'\boldsymbol{\epsilon}$, we have the following result.

\begin{eqnarray*}
   n^{-1/2}Z'\boldsymbol{\epsilon}  &=& \frac{1}{\sqrt{n}}\sum_{i=1}^n \mathbf{z_i}\epsilon_i  = \frac{1}{\sqrt{n}}\sum_{i=1}^n \left[\begin{array}
     {c} \mathbf{z_{i1}} \epsilon_i  \\ \mathbf{z_{i2}} \epsilon_i 
   \end{array}\right]\\
         &\overset{d}{\rightarrow}& N\left(\left[\begin{array}
           {c} \mathbf{0} \\ \boldsymbol{\tau}
         \end{array}\right], \lim_{n\rightarrow \infty}E_n \left[\begin{array}
           {cc} 
           \epsilon_i^2 \mathbf{z}_{i1}\mathbf{z}_{i1}' & \epsilon_i^2 \mathbf{z}_{i1}\mathbf{z}_{i2}'\\
           \epsilon_i^2 \mathbf{z}_{i2}\mathbf{z}_{i1}' & \epsilon_i^2 \mathbf{z}_{i2}\mathbf{z}_{i2}'
          \end{array}\right] \right)
\end{eqnarray*}

\begin{pro}[Limit Distribution for GIV Estimators]
  Fourth and a bit moments (needed to get $L_2$ convergence for the triangular array, see Davidson 19.1), local mis-specification, limiting variance matrix exists and is positive-definite, instruments are relevant imply that
    $$\sqrt{n}\left(\widehat{\beta}_S - \beta \right) \overset{d}{\rightarrow} K_S \Xi_S \left(\left[\begin{array}
           {c} \mathbf{0} \\ \boldsymbol{\tau}
         \end{array}\right] + M \right)$$
  where
      \begin{eqnarray*}
        K_S &=& \left(\Pi' Q_Z \Xi_S' W_S \Xi_S Q_Z' \Pi\right)^{-1} \Pi'Q_Z \Xi_S'W_S\\
        Q_Z &=&\lim_{n\rightarrow \infty}E_n[\mathbf{z}_i \mathbf{z}_i'] \\
        M &\sim& N(\mathbf{0}, \Omega)\\
        \Omega &=& \lim_{n\rightarrow \infty}E_n \left[\begin{array}
           {cc} 
           \epsilon_i^2 \mathbf{z}_{i1}\mathbf{z}_{i1}' & \epsilon_i^2 \mathbf{z}_{i1}\mathbf{z}_{i2}'\\
           \epsilon_i^2 \mathbf{z}_{i2}\mathbf{z}_{i1}' & \epsilon_i^2 \mathbf{z}_{i2}\mathbf{z}_{i2}'
          \end{array}\right]
      \end{eqnarray*}
\end{pro}



\subsection{Estimating $\tau$}
To simplify the notation in this section, define the following selection matrices corresponding to $Z_1$ and $Z_2$: 
  \begin{eqnarray*}
  \Xi_1 &=& \left[\begin{array}{cc} I_{q_1} & 0_{q_1 \times q_2}  \end{array}\right]\\
    \Xi_2 &=& \left[ \begin{array}{cc}
              0_{q_2 \times q_1}& I_{q_2}
            \end{array}\right]
  \end{eqnarray*}
The $q_1\times q$ matrix $\Xi_1$ extracts the first $q_1$ elements of a $(q_1 + q_2)\times 1$ while the $q_2\times q$ matrix $\Xi_2$ extracts the last $q_2$ elements. 

To estimate $\tau$ we need an an asymptotically unbiased estimator of $\beta$ to plug into the moment conditions for the ``suspect'' instruments contained in $Z_2$. 
Let $\widehat{\beta}_1$ denote the GIV estimator based \emph{only} on the instruments contained in $Z_1$:
\begin{equation}
  \widehat{\beta}_1 = \left(X'Z_1 W^V_n Z_1' X\right)^{-1} X'Z_1 W_n^1 Z_1' \mathbf{y}
\end{equation}
By the result in the previous section,
  \begin{eqnarray*}
    \sqrt{n} \left(\widehat{\beta}_1 - \beta \right) &\overset{d}{\rightarrow}& K_1 \left[\begin{array}{cc} I_{q_1} & 0_{q_2 \times q_2}  \end{array}\right] \left(\left[\begin{array}
           {c} \mathbf{0} \\ \boldsymbol{\tau}
         \end{array}\right] + M \right)\\
         &=& K_1 \Xi_1  \left(\left[\begin{array}
           {c} \mathbf{0} \\ \boldsymbol{\tau}
         \end{array}\right] + M \right) = K_1 \Xi_1 M\\
          &\sim& K_1  \times N\left(0, \lim_{n\rightarrow \infty} E_n[\epsilon_i^2 \mathbf{z}_{i1}\mathbf{z}_{i1}']\right)
  \end{eqnarray*}

We estimate $\tau$ as follows:
\begin{eqnarray*}
  \widehat{\tau} &=& \sqrt{n} \left[ Z_2' \left(\textbf{y} - X\widehat{\beta}_1\right)/n\right] = n^{-1/2} Z_2'\left(\textbf{y} - X\widehat{\beta}_1\right)\\
      &=& n^{-1/2} Z_2'(X\beta + \boldsymbol{\epsilon} - X \widehat{\beta}_V) = n^{-1/2} Z_2' \left[\boldsymbol{\epsilon} - X(\widehat{\beta}_1 - \beta)\right]\\
      &=& n^{-1/2} Z_2'\boldsymbol{\epsilon} - n^{-1/2} Z_2' X (\widehat{\beta}_1 - \beta)\\
      &=& n^{-1/2} Z_2'\boldsymbol{\epsilon} - (Z_2' X/n) \left[  \sqrt{n}(\widehat{\beta}_1 - \beta)\right]\\
      &=&n^{-1/2} Z_2'\boldsymbol{\epsilon} - (Z_2' X/n)\widehat{K}_1 \Xi_1 (n^{-1/2}Z'\boldsymbol{\epsilon})
\end{eqnarray*}
We have
  \begin{eqnarray*}
    Z_2'X/n &=& n^{-1} Z_2'(Z\Pi + \mathbf{v}) = n^{-1}Z_2'Z\Pi + n^{-1}Z_2'\mathbf{v}\\  
            &=& n^{-1}Z_2' \left[\begin{array}
              {cc} Z_1 & Z_2 
            \end{array} \right] \Pi + n^{-1}Z_2'\mathbf{v}\\
            &\overset{p}{\rightarrow}&  \left[ \begin{array}{cc}
              0_{q_2 \times q_1}& I_{q_2}
            \end{array}\right] Q_Z \Pi = \Xi_2 Q_Z \Pi
  \end{eqnarray*}
since $n^{-1}Z_2'\mathbf{v} \overset{p}{\rightarrow} 0$. Similarly, the CLT for $n^{-1/2}Z'\boldsymbol{\epsilon}$ gives,
\begin{eqnarray*}
  n^{-1/2} Z_2' \boldsymbol{\epsilon} &\overset{d}{\rightarrow}& \left[ \begin{array}{cc} 0_{q_2 \times q_1}& I_{q_2}
            \end{array}\right]   \left(\left[\begin{array}
           {c} \mathbf{0} \\ \boldsymbol{\tau}
         \end{array}\right] + M \right)\\
         &=& \Xi_2  \left(\left[\begin{array}
           {c} \mathbf{0} \\ \boldsymbol{\tau}
         \end{array}\right] + M \right)\\
         &=& \Xi_2\left[\begin{array}
           {c} \mathbf{0} \\ \boldsymbol{\tau}
         \end{array}\right] + \Xi_2 M \\
         &=& \boldsymbol{\tau} + \Xi_2 M
\end{eqnarray*}
Thus, using the limit distribution for $\sqrt{n}\left(\widehat{\beta}_1 - \beta \right)$ given above, we have
\begin{eqnarray*}
  \widehat{\tau} &\overset{d}{\rightarrow}& \boldsymbol{\tau}  - \Xi_2 Q_Z \Pi K_1 \Xi_1 M + \Xi_2 M\\
    &=& \boldsymbol{\tau} + (-\Xi_2Q_Z \Pi K_1 \Xi_1 + \Xi_2)M\\
    &=& \boldsymbol{\tau} + \left\{-\Xi_2Q_Z \Pi K_1 
      \left[ \begin{array}{cc}
              I_{q_1} & 0_{q_1 \times q_2}
            \end{array}\right] + \left[ \begin{array}{cc}
              0_{q_2 \times q_1}& I_{q_2}
            \end{array}\right]\right\} M\\
          &=& \boldsymbol{\tau} + \left\{
      \left[ \begin{array}{cc}
              -\Xi_2Q_Z \Pi K_1  & 0_{q_2 \times q_2}
            \end{array}\right] + \left[ \begin{array}{cc}
              0_{q_2 \times q_1}& I_{q_2}
            \end{array}\right]\right\} M\\
          &=&\boldsymbol{\tau}  + \left[ \begin{array}{cc}
              -\Xi_2Q_Z \Pi K_1  & I_{q_2}
            \end{array}\right]M\\
          &=& \boldsymbol{\tau}  + \Psi M
\end{eqnarray*}
where we have used the fact that $\Xi_2Q_Z \Pi K_1 $ is a $q_2\times q_1$ matrix. Since all the limit results we have written down depend only on $M$ and constant matrices, we in fact have \emph{joint convergence} of any collection of $\sqrt{n}\left(\widehat{\beta}_S - \beta \right)$ estimators \emph{along} with $\widehat{\tau}$. We will use this fact to construct valid confidence intervals.

\subsection{The FMSC for the Choosing Instruments Example}
Consider a target parameter $\mu$ where $\mu = \mu(\beta)$ denotes $\mu$ evaluated at the true $\beta$ and $\widehat{\mu}_S = \mu(\beta)$ denotes the estimator of $\mu$ implied by $\widehat{\beta}_S$. Then, by the Delta Method and the results from above,
  $$\sqrt{n}\left( \widehat{\mu}_S - \mu \right) \overset{d}{\rightarrow} \nabla_\beta (\beta)' K_S \Xi_S\left(\left[\begin{array}
           {c} \mathbf{0} \\ \boldsymbol{\tau}
         \end{array}\right] + M \right)$$
It follows immediately that 
  \begin{eqnarray*}
    \mbox{ABIAS}^2(\widehat{\mu}_S) &=&  \nabla_\beta (\beta)' K_S \Xi_S  \left[\begin{array}
      {cc}
      0 & 0 \\ 0 & \tau \tau'
    \end{array} \right]  \Xi_S' K_S' \nabla_\beta (\beta)  \\
    \mbox{AVAR}(\widehat{\mu}_S) &=& \nabla_\beta (\beta)' K_S \Xi_S  \Omega  \Xi_S' K_S' \nabla_\beta (\beta)
  \end{eqnarray*}
We estimate $\nabla\mu_\beta(\beta)$ by $\nabla_\beta\mu(\widehat{\beta}_1)$ when necessary (sometimes the derivatives don't depend on $\beta$), and $K_S$ by $\widehat{K}_S$. Both of these will be consistent. Various estimators of $\Omega$ are possible, but the point is that we can easily get a consistent estimator under these asymptotics. The only problem is $\tau \tau'$. We can construct an asymptotically unbiased estimator by using the fact that $\widehat{\tau} \overset{d}{\rightarrow} \Psi M$. It follows that $\widehat{\tau}\widehat{\tau}' - \widehat{\Psi} \widehat{\Omega} \widehat{\Psi}$ is an asymptotically unbiased estimator of $\tau\tau'$ In particular, our estimators are:
\begin{eqnarray*}
  \widehat{\tau} \widehat{\tau}' &=& \left[n^{-1/2} Z_2'\left(\textbf{y} - X\widehat{\beta}_1\right)\right] \left[n^{-1/2} Z_2'\left(\textbf{y} - X\widehat{\beta}_1\right) \right]'\\
    &=&n^{-1} Z_2' \hat{\epsilon}_1 \hat{\epsilon}_1' Z_2 
\end{eqnarray*}
and
\begin{eqnarray*}
  \widehat{\Psi} &=& \left[ \begin{array}{cc}
      -n^{-1} Z_2' \widehat{K}_1 & I_{q_2}
  \end{array}\right]
\end{eqnarray*}
where $\hat{\epsilon}_1$ is the vector of residuals constructed from the estimator $\widehat{\beta}_1$.

\subsection{Special Case: Two-stage Least Squares}
For the special case of two-stage least squares (2SLS), we have
  \begin{eqnarray*}
     W_n^S = (Z_S' Z_S/n)^{-1} = \left[\Xi_S (Z'Z/n) \Xi_S'\right]^{-1}  \overset{p}{\rightarrow} \left(\Xi_S Q_Z \Xi_S'\right)^{-1}
   \end{eqnarray*} 
So we have
  \begin{eqnarray*}
    \widehat{K}_n^S &=& n\left(X'Z \Xi_S' W^S_n \Xi_S Z' X\right)^{-1}X' Z \Xi_S' W^S_n \\
        &=& n\left(X'Z \Xi_S' \left[\Xi_S (Z'Z/n) \Xi_S'\right]^{-1}  \Xi_S Z' X\right)^{-1}X' Z \Xi_S' \left[\Xi_S (Z'Z/n) \Xi_S'\right]^{-1} \\
        &=& n\left[X'Z \Xi_S' \left(\Xi_S Z'Z \Xi_S'\right)^{-1}  \Xi_S Z' X\right]^{-1}X' Z \Xi_S' \left(\Xi_S Z'Z \Xi_S'\right)^{-1}\\
        &=& n\left[X' Z_S (Z_S' Z_S)^{-1} Z_S' X\right]^{-1} X'Z_S (Z_S' Z_S)^{-1}
  \end{eqnarray*}
and
    \begin{equation}
      K_S =  \left[\Pi' Q_Z \Xi_S' \left(\Xi_S Q_Z \Xi_S'\right)^{-1} \Xi_S Q_Z' \Pi\right]^{-1} \Pi'Q_z \Xi_S'\left(\Xi_S Q_Z \Xi_S'\right)^{-1}
    \end{equation}    

\todo[inline]{I don't know of any way to simplify this expression. The problem is that we can't decompose $\Xi_S Q_Z \Xi_S$ into a product of square matrices to there's no way to express the inverse of the product in terms of the product of inverses.}


Under the assumption that there is no heteroscedasticity in the limit, 2SLS is the efficient GMM estimator and the variance matrix from the CLT, $\Omega$, simplifies as follows:
\begin{eqnarray*}
\Omega &=& \lim_{n\rightarrow \infty}E_n \left[\begin{array}
           {cc} 
           \epsilon_i^2 \mathbf{z}_{i1}\mathbf{z}_{i1}' & \epsilon_i^2 \mathbf{z}_{i1}\mathbf{z}_{i2}'\\
           \epsilon_i^2 \mathbf{z}_{i2}\mathbf{z}_{i1}' & \epsilon_i^2 \mathbf{z}_{i2}\mathbf{z}_{i2}'
          \end{array}\right]  = \sigma_\epsilon^2 Q_Z
\end{eqnarray*}
In this case, a natural estimator of $\Omega$ is given by
  $$\widehat{\Omega} = \widehat{\sigma}_\epsilon^2 n^{-1}Z'Z = \left(n^{-1}\hat{\epsilon}_1 ' \hat{\epsilon}_1\right) n^{-1}Z'Z$$



\subsection{Stable and Efficient Computation}
To implement the FMSC for instrument selection we need to repeatedly fit IV regressions and calculate various related quantities. Simply using the formulas from above in the calculations is unwise. For example, it is computational best practice to avoid direct matrix inversion wherever possible. In this section I outline how to carry out stable and efficient calculations using the QR decomposition. Note that in this section $Q_Z$ has a \emph{different meaning} from above.

\paragraph{QR Decomposition} Any $n\times k$ matrix $A$ with full column rank can be decomposed as $A = QR$, where $R$ is an $k\times k$ upper triangular matrix and $Q$ is an $n\times k$ matrix with orthonormal columns. The columns of $A$ are \emph{orthogonalized} in $Q$ via the Gram-Schmidt process. Since $Q$ has orthogonal columns, we have $Q'Q = I_k$. It is \emph{not} in general true that $QQ' = I$, however. In the special case where $A$ is square, $Q^{-1} = Q'$.

\paragraph{Note:} The way we have defined things here is here is sometimes called the ``thin'' or ``economical'' form of the QR decomposition, e.g.\ \texttt{qr\_econ} in Armadillo. In our ``thin'' version, $Q$ is an $n\times k$ matrix with orthogonal columns. In the ``thick'' version, $Q$ is an $n\times n$ \emph{orthogonal} matrix. Let $A = QR$ be the ``thick'' version and $A = Q_1 R_1$ be the ``thin'' version. The connection between the two is as follows:
  $$A = QR = Q \left[\begin{array}
    {c} R_1 \\ 0 
  \end{array} \right] = \left[  \begin{array}
    {cc} Q_1 & Q_2
  \end{array}\right]\left[\begin{array}
    {c} R_1 \\ 0 
  \end{array} \right] = Q_1 R_1$$

\paragraph{Least-Squares via the QR Decomposition} We can calculate the least squares estimator of $\beta$ as follows
\begin{eqnarray*}
  \widehat{\beta} &=& (X'X)^{-1} X'y = \left[(QR)' (QR) \right]^{-1} (QR)' y\\
    &=&\left[ R' Q' Q R\right]^{-1} R'Q' y = (R'R)^{-1} R'Q y\\
    &=& R^{-1} (R')^{-1} R' Qy = R^{-1} Qy
\end{eqnarray*}
In other words, $\widehat{\beta}$ is the solution to $R\beta = Qy$. While it may not be immediately apparent, this is a much easier system to solve that the normal equations $(X'X) \beta = X'y$. Because $R$ is \emph{upper triangular} we can solve $R\beta = Qy$ extremely quickly. The product $Qy$ is simply a vector, call it $v$, so the system is simply
  $$\left[
    \begin{array}
      {cccccc}
      r_{11} & r_{12}  & r_{13}& \cdots & r_{1,n-1} & r_{1k} \\
      0 & r_{22} & r_{23}&\cdots & r_{2,n-1} & r_{2k}\\
      0&  0 &  r_{33}& \cdots & r_{3,n-1} & r_{3k}\\  
      \vdots & \vdots & \ddots& \ddots & \vdots & \vdots\\
      0 & 0 & \cdots &0  & r_{k-1, k-1} & r_{k-1, k} \\
      0 & 0 & \cdots & 0 & 0 & r_{k}
    \end{array}
  \right] \left[ \begin{array}
    {ccc}
    \beta_1 \\ \beta_2 \\ \beta_3 \\ \vdots \\ \beta_{k-1} \\ \beta_k
  \end{array}\right] = \left[ \begin{array}
    {c}
    v_1  \\ v_2  \\ v_3 \\  \vdots \\ v_{k-1} \\ v_{k}
  \end{array}\right]
  $$
Hence, $\beta_k = v_k / r_k$ which we can immediately substitute into $\beta_{k-1} r_{k-1,k-1} + \beta_k r_{k-1,k} = v_{k-1}$ to solve for $\beta_{k-1}$, and so on. This procedure is called \emph{back substitution}. 

To calculate the variance matrix $\sigma^2 (X'X)^{-1}$ for the least-squares estimator, simply note from the derivation above that $(X'X)^{-1} = R^{-1} (R^{-1})'$ . Inverting $R$, however, is easy: we simply apply back-substitution \emph{repeatedly}. Let $A$ be the inverse of $R$, $\mathbf{a}_j$ be the $j$th column of $A$, and $\mathbf{e}_j$ be the $j$th element of the $k\times k$ identity matrix, i.e.\ the $j$th standard basis vector. Inverting $R$ is equivalent to solving $R \mathbf{a}_1 = \mathbf{e}_1$, followed by $R \mathbf{a}_2 = \mathbf{e}_2$, and so on all the way up to $R \mathbf{a}_k = \mathbf{e}_k$. 

\paragraph{Othogonal Projection Matrices and the QR Decomposition}
Consider a projection matrix $P_X = X (X'X)^{-1}X'$. Provided that $X$ has full column rank, we have
begin
  $$P_X  = QR(R'R)^{-1}R'Q' = QRR^{-1} (R')^{-1}R'Q' = QQ'$$
Recall that, in general, it is \emph{not} true that $QQ' = I$ even though $Q'Q = I$.

\paragraph{Two-stage Least Squares via QR} The two-stage least squares (2SLS) estimator is given by
  $$\widehat{\beta}_{2SLS} = (X' P_Z X)^{-1} X' P_Z y$$
where $P_Z = Z(Z'Z)^{-1}Z'$. First, take the QR decomposition of $Z$, namely $Z = Q_Z R_Z$. Using the result for projection matrices from above, we see that
\begin{eqnarray*}
  \widehat{\beta}_{2SLS} &=& (X' Q_Z Q_Z')^{-1} X' Q_Z Q_Z' y=\left[ (Q_Z' X)' (Q_Z' X)\right]^{-1} (Q_Z' X)' Q_Z' y\\
    &=& \left(\widetilde{X}' \widetilde{X}\right)^{-1} \widetilde{X}' \widetilde{y}
\end{eqnarray*}
where $\widetilde{X} = Q_Z' X$ and $\widetilde{y} = Q_Z' y$. Now we simply have a \emph{transformed} system where the matrix $Q_Z'$ has projected both $X$ and $y$ onto the column space of $Z$. Now we use a \emph{second} QR decomposition on the transformed system, namely $\widetilde{X} = \widetilde{Q} \widetilde{R}$. Using our results for least-squares from above, we have 
  $$\widehat{\beta}_{2SLS} = \widetilde{R}^{-1} \widetilde{Q} \widetilde{y} = \widetilde{R}^{-1} \widetilde{Q}' Q_Z' y$$
Thus the 2SLS estimator $\widehat{\beta}_{2SLS}$ is the solution to the system
  $$\widetilde{R} \beta = \widetilde{Q}' \widetilde{y}$$
which we can solve by back-substitution. 

\paragraph{Variance Matrix for 2SLS via QR}
The variance matrix for 2SLS is fairly complicated, as it depends on the product
  $$C = \left[X'Z(Z'Z)^{-1}Z'X\right]^{-1} X'Z (Z'Z)^{-1}$$
Fortunately, we can calculate easily calculate $C$ as a side-effect of the intermediate quantities we used in the 2SLS fit above. Again, we first apply carry out a QR decomposition of $Z$, namely $Z = Q_Z R_Z$,  and then carry out a QR decomposition of the transformed regressors $\widetilde{X} = Q_Z' X$, namely $\widetilde{X} = \widetilde{Q} \widetilde{R}$. We have,
  \begin{eqnarray*}
    C &=& \left(X' P_Z X \right)^{-1} X'(Q_Z R_Z) \left(R_Z' R_Z\right)^{-1}\\
      &=& \left(X' Q_Z Q_Z' X \right)^{-1} X'Q_Z R_Z R_Z^{-1} (R_Z')^{-1}\\
      &=& \left[ (Q_Z' X)' Q_Z'X \right]^{-1} (Q_Z' X)' (R_Z')^{-1}\\
      &=& \left(\widetilde{X}' \widetilde{X}' \right)^{-1} \widetilde{X}' (R_Z')^{-1}\\
      &=& \widetilde{R}^{-1} (\widetilde{R}')^{-1} (\widetilde{Q} \widetilde{R})' (R_Z')^{-1}\\
      &=& \widetilde{R}^{-1} (\widetilde{R}')^{-1} \widetilde{R}' \widetilde{Q}' (R_Z')^{-1}\\
      &=& \widetilde{R}^{-1}\widetilde{Q}' (R'_Z)^{-1}
  \end{eqnarray*}

\paragraph{Weighting Matrices, Variance Estimators, etc.}
The general linear-GMM estimator considered above is 
  $$\widehat{\beta}_{GMM} = (X'Z W_n Z'X)^{-1}X'Z W_n Z'y$$
and 2SLS corresponds to the special case in which $W_n = (Z'Z/n)^{-1}$. Under the standard assumptions with $y = X\beta + \epsilon$, we have
\begin{eqnarray*}
  \widehat{\beta}_{GMM} &=& (X'Z W_n Z'X)^{-1}X'Z W_n Z'y\\
      &=& (X'Z W_n Z'X)^{-1}X'Z W_n Z'(X\beta + \epsilon)\\
      &=&\beta + (X'Z W_n Z'X)^{-1}X'Z W_n Z'\epsilon
\end{eqnarray*}
so that 
\begin{eqnarray*}
  \sqrt{n} \left(\widehat{\beta} - \beta\right) &=& \sqrt{n} (X'Z W_n Z'X)^{-1}X'Z W_n Z'\epsilon\\
    &=& \sqrt{n} \left[\left(X'Z\right) W_n \left(\frac{Z'X}{n} \right) \right]^{-1} \left(\frac{X'Z}{n} \right) W_n Z'\epsilon\\
    &=& \sqrt{n} \cdot \frac{1}{n} \left[\left(\frac{X'Z}{n}\right) W_n \left(\frac{Z'X}{n} \right) \right]^{-1} \left(\frac{X'Z}{n} \right)W_n Z'\epsilon\\
    &=& \left[\left(\frac{X'Z}{n}\right) W_n \left(\frac{Z'X}{n} \right) \right]^{-1} \left(\frac{X'Z}{n} \right)W_n\left(\frac{Z'\epsilon}{\sqrt{n}}\right)\\
    &=& n \left(X'Z W_n Z'X\right)^{-1} X'Z W_n\left(\frac{Z'\epsilon}{\sqrt{n}}\right) \\
    &=& n\,\widehat{C}\left( n^{-1/2} Z'\epsilon\right)
\end{eqnarray*}
By the Central Limit Theorem:
  $$n^{-1/2} Z'\epsilon = \frac{1}{\sqrt{n}} \sum_{i=1}^n \epsilon_i \textbf{z}_i' \overset{d}{\rightarrow} N(0,\Omega)$$
where 
  $$\Omega = \lim_{n\rightarrow \infty} Var \left( \frac{1}{\sqrt{n}} \sum_{i=1}^n \epsilon_i \textbf{z}_i'\right) = \frac{1}{n} Var \left( \sum_{i=1}^n \epsilon_i \textbf{z}_i'\right)$$
and the Law of Large Numbers,
  \begin{eqnarray*}
    n\, \widehat{C} &=& n \left(X'Z W_n Z'X\right)^{-1} X'Z W_n\\ 
        &=& \left[\left(\frac{X'Z}{n}\right) W_n \left(\frac{Z'X}{n} \right) \right]^{-1} \left(\frac{X'Z}{n} \right)W_n\\
        &\overset{p}{\rightarrow}& C = \left( E[\mathbf{x}\mathbf{z}']\,W \,E[\mathbf{z}\mathbf{x}'] \right)^{-1} E[\mathbf{x}\mathbf{z'}] W
  \end{eqnarray*}
For example, in the special case of 2SLS, $W_n = (Z'Z/n)^{-1}$ so $W = E[\mathbf{z}_i\mathbf{z}_i']$. Returning to the general version and combining the CLT and the LLN, 
  $$\sqrt{n}\left(\widehat{\beta}_{2SLS} - \beta_0\right) \overset{d}{\rightarrow} N(0, C\Omega C')$$
If $W_n$ converges in probability to the efficient weighting matrix, namely $\Omega^{-1}$, this simplifies as follows:
  \begin{eqnarray*}
    C\Omega C' &=&  \left( E[\mathbf{x}\mathbf{z}']\, \Omega^{-1} \,E[\mathbf{z}\mathbf{x}'] \right)^{-1} E[\mathbf{x}\mathbf{z'}] \Omega^{-1}\Omega \Omega^{-1}E[\mathbf{z}\mathbf{x}']\left( E[\mathbf{x}\mathbf{z}']\, \Omega^{-1} \,E[\mathbf{z}\mathbf{x}'] \right)^{-1}\\
      &=& \left( E[\mathbf{x}\mathbf{z}']\, \Omega^{-1} \,E[\mathbf{z}\mathbf{x}'] \right)^{-1} E[\mathbf{x}\mathbf{z'}]\Omega^{-1}
      E[\mathbf{z}\mathbf{x}']\left( E[\mathbf{x}\mathbf{z}']\, \Omega^{-1} \,E[\mathbf{z}\mathbf{x}'] \right)^{-1}\\
      &=& \left( E[\mathbf{x}\mathbf{z}']\, \Omega^{-1} \,E[\mathbf{z}\mathbf{x}'] \right)^{-1}
  \end{eqnarray*}

Now, the question is how to estimate $C\Omega C'$ in general.To begin, suppose we have a consistent estimator of $\Omega$, namely $\widehat{\Omega}$. Then, since $n\, \widehat{C} \overset{p}{\rightarrow} C$, it follows that $n^2 \widehat{C} \widehat{\Omega} \widehat{C}\overset{p}{\rightarrow}C\Omega C'$. This gives us an estimate of the asymptotic variance of $\sqrt{n}(\widehat{\beta} - \beta)$. To convert this to an estimate of the asymptotic variance of $\widehat{\beta}$, we need to divide through by $n$, since variance is a quadratic operator. We have
  $$\widehat{\mbox{AVAR}}(\widehat{\beta}) = n\widehat{C} \widehat{\Omega} \widehat{C}$$
In the case of the efficient GMM estimator, the problem simplifies considerably since we only need to estimate $ \left( E[\mathbf{x}\mathbf{z}']\, \Omega^{-1} \,E[\mathbf{z}\mathbf{x}'] \right)^{-1}$ rather than the more general form of $C\Omega C'$. Again, suppose for the moment that we have a consistent estimator $\widehat{\Omega}$ of $\Omega$. Since $X'Z/n \overset{p}{\rightarrow}E[\mathbf{x}\mathbf{z}']$, it follows that
  $$\left(X'Z/n  \,\widehat{\Omega}^{-1} Z'X/n\right)^{-1} = n^2 (X'Z \widehat{\Omega}^{-1} Z'X)^{-1} \overset{p}{\rightarrow}\left( E[\mathbf{x}\mathbf{z}']\, \Omega^{-1} \,E[\mathbf{z}\mathbf{x}'] \right)^{-1}$$
Thus $n^2\left(X'Z  \,\widehat{\Omega}^{-1} Z'X\right)^{-1}$ is a consistent estimator of the asymptotic variance of $\sqrt{n}(\widehat{\beta} - \beta)$, and we can convert this into an estimate of the asymptotic variance of $\widehat{\beta}$ as follows
  $$\widehat{\mbox{AVAR}}(\widehat{\beta}) = n \left(X'Z  \,\widehat{\Omega}^{-1} Z'X\right)^{-1} $$
again, dividing through by $n$. As we will discuss below, 2SLS is the efficient GMM estimator under homoskedasticity, yielding the estimator  
    $$\widehat{\Omega} = \widehat{\sigma}^2_\epsilon Z'Z/n$$
Substituting this into the expression from above gives
     $$\widehat{\mbox{AVAR}}(\widehat{\beta}) = \widehat{\sigma}^2_\epsilon \left(X'Z (Z'Z)^{-1} Z'X\right)^{-1}  = \widehat{\sigma}^2_\epsilon(X' P_Z X)^{-1}$$ 
since the $1/n$ in the innermost parentheses cancels the $n$ at the beginning of the expression. The estimator $\widehat{\sigma}^2_\epsilon$ is constructed from the 2SLS residuals in the usual way, as described below. It is important to note that this simplification only occurs in the situations where 2SLS is efficient. More generally, we would simply substitute $(Z'Z/n)^{-1}$ in the expression for $W_n$ and then use $n\widehat{C} \widehat{\Omega} \widehat{C}$ as our estimator.

The only remaining question is how to estimate $\Omega$. We will assume throughout that  $\epsilon_i \mathbf{z}_i'$ is iid over $i$ so that the expression for $\Omega$ from above simplifies as follows:
$$\Omega = \frac{1}{n} Var \left( \sum_{i=1}^n \epsilon_i \textbf{z}_i'\right) = \frac{1}{n} \sum_{i=1}^n Var(\epsilon_i \textbf{z}_i') = Var(\epsilon_i \textbf{z}_i')$$
Now the question is simply how to estimate $Var(\epsilon_i \textbf{z}_i')$. There are various sets of assumptions that lead to different estimators. The simplest estimator assumes that the instruments are exogeneous so that $E[\epsilon_i \mathbf{z}_i'] = 0$ and hence
  $$Var(\epsilon_i \textbf{z}_i') = E[\epsilon_i^2 \textbf{z}_i\textbf{z}_i']$$
and that the errors are homoscedastic, leading to the further simplification:
  $$Var(\epsilon_i \textbf{z}_i') = E[\epsilon_i^2 \textbf{z}_i\textbf{z}_i'] = E[\epsilon_i^2]E[\textbf{z}_i \textbf{z}_i'] = \sigma_\epsilon^2 E[\textbf{z}_i \textbf{z}_i']$$
Defining $\widehat{\epsilon} = y - X\widehat{\beta}$ and $\widehat{\sigma}_\epsilon^2= \widehat{\epsilon}' \,\widehat{\epsilon}/n$, it follows under these assumptions that
  $$\widehat{\Omega} = \widehat{\sigma}_\epsilon^2 Z'Z/n \overset{p}{\rightarrow} \Omega$$
which implies that 2SLS is the efficient linear GMM estimator. If we are not prepared to assume homoscedasticity, 2SLS is no longer the efficient GMM estimator and we need a more complicated estimator of $\Omega$, namely
  $$\widehat{\Omega} = \frac{1}{n} \sum_{i=1}^n \widehat{\epsilon}_i^{\, 2} \textbf{z}_i \textbf{z}_i' = Z'DZ/n$$
Where $D = diag\{\widehat{\epsilon}_i^{\, 2}\}$. (For unbiased estimation, we could divide by $n-k$ rather than $n$ but this usually doesn't make much difference in practice.) One last refinement we can consider is a \emph{centered} variance matrix estimator. While instrumental variables estimation relies on the assumption that $E[\epsilon_i \mathbf{z}_i'] = 0$, there are settings in which we would like a variance matrix estimator that is robust to violations of this assumption. If we do not assume $E[\epsilon_i \mathbf{z}_i'] = 0$, 
  $$Var(\epsilon_i \textbf{z}_i') = E[\epsilon_i^2 \textbf{z}_i \textbf{z}_i'] - E[\epsilon_i \textbf{z}_i]E[\epsilon_i \textbf{z}_i']$$
by the shortcut formula. We can continue to estimate $E[\epsilon_i^2 \textbf{z}_i \textbf{z}_i']$ by $Z'DZ/n$, or in the case of homoscedasticity by $\widehat{\sigma}_\epsilon^2 Z'Z/n$, so the question is how to estimate the second term. In the presence of over-identifying restrictions, the moment conditions used to estimate the model will not hold exactly in-sample. This is the principle behind the $J$-test. We can use the same idea here to estimate $E[\epsilon_i \textbf{z}_i]$ by the sample analogue $n^{-1} \sum_{i=1}^n \widehat{\epsilon}_i \textbf{z}_i$. This idea leades to the heteroskedasticity-consistent centered variance matrix estimator:
  $$\widehat{\Omega} = \frac{1}{n} \sum_{i=1}^n \widehat{\epsilon}_i^{\, 2} \textbf{z}_i \textbf{z}_i' - \left( \frac{1}{n} \sum_{i=1}^n \widehat{\epsilon}_i \textbf{z}_i\right) \left(  \frac{1}{n} \sum_{i=1}^n \widehat{\epsilon}_i \textbf{z}_i'\right) = Z'\left[ \frac{D}{n} - \frac{\widehat{\epsilon}' \widehat{\epsilon}}{n^2} \right] Z$$

\paragraph{Two-step Efficient GMM via Cholesky and QR}
To calculate the J-test statistic, which we'll need for the Andrews (1999) Moment Selection Criterion described below, we first need to compute the efficient two-step GMM estimator. Suppose $\widehat{\Omega}$ is a consistent estimator of the variance matrix from the CLT for $n^{-1/2}Z'\epsilon$. A natural choice for $\widehat{\Omega}$ is the estimator based on the two-stage least squares residuals, namely $Z'DZ/n$ as described above. Another possiblity is the centered version of the same, which is what we'll use for the Andrews (1999) criteria. The efficient linear GMM estimator is
  $$\widehat{\beta}_{GMM} = \left(X'Z \widehat{\Omega}^{-1} Z'X\right)^{-1}X'Z \widehat{\Omega}^{-1}Z'y$$
Now suppose we take the Cholesky decomposition of $\widehat{\Omega}$  
  $$\widehat{\Omega} = LL'$$
Then, the GMM estimator becomes
  \begin{eqnarray*}
    \widehat{\beta}_{GMM} &=& \left(X'Z \left(LL'\right)^{-1} Z'X\right)^{-1}X'Z \left(LL'\right)^{-1}Z'y\\
      &=&  \left(X'Z (L^{-1})' L^{-1} Z'X\right)^{-1}X'Z (L^{-1})' L^{-1} Z'y\\
      &=&\left[\left(L^{-1}Z' X \right)' \left(L^{-1}Z'X \right)\right]^{-1} \left(L^{-1}Z'X \right)'\left(L^{-1}Z y\right)\\
      &=& (\widetilde{X}'\widetilde{X})^{-1} \widetilde{X}\widetilde{y}
  \end{eqnarray*}
where $\widetilde{X} = L^{-1}Z' X$ and $\widetilde{y} = L^{-1}Z'y$. These expressions can be rewritten as
  \begin{eqnarray*}
    L\widetilde{X} = Z'X \\
    L\widetilde{y} = Z'y
  \end{eqnarray*}
and since $L$ is lower triangular, we can efficiently solve for $\widetilde{X}$ and $\widetilde{y}$ as follows:
  \begin{eqnarray*}
      \widetilde{X} &=& \texttt{forwardsolve}(L, Z'X)\\
      \widetilde{y} &=& \texttt{forwardsolve}(L, Z'y)
  \end{eqnarray*}
Finally, we can calculte $\widehat{\beta}_{GMM}$ via a the QR decomposition $\widetilde{X}=QR$
\begin{eqnarray*}
   \widehat{\beta}_{GMM} = (\widetilde{X}'\widetilde{X})^{-1} \widetilde{X}\widetilde{y}&=& \left((QR)'QR\right)^{-1}(QR)'\widetilde{y}\\
    &=& (R'Q'QR)^{-1}R'Q'\widetilde{y} =  (R'R)^{-1}R'Q \widetilde{y}\\
    &=&  R^{-1} (R')^{-1}R'Q'\widetilde{y} = R^{-1}Q'\widetilde{y}
\end{eqnarray*}
It follows that $\widehat{\beta}_{GMM}$ is the solution to $R\widehat{\beta}_{GMM} = Q'\widetilde{y}$, which we can easily calculate via backsubstitution. Hence, the full algorithm is
  \begin{enumerate}
    \item $L = \texttt{chol}(\widehat{\Omega})$ 
    \item $\widetilde{X} = \texttt{forwardsolve}(L, Z'X)$
    \item $\widetilde{y} = \texttt{forwardsolve}(L, Z'y)$
    \item $(Q, R) = \texttt{qr}(\widetilde{X})$
    \item $\widehat{\beta}_{GMM} = \texttt{backsolve}(R, Q'\widetilde{y})$
  \end{enumerate}
Now, as discussed above, if we use the efficient weighting matrix, the asymptotic variance matrix of the linear GMM estimator reduces to
  $$C\Omega C' =\left( E[\mathbf{x}\mathbf{z}']\, \Omega^{-1} \,E[\mathbf{z}\mathbf{x}'] \right)^{-1}$$
We already have a consistent estimator of $\Omega$, namely $\widehat{\Omega}$ which we used to estimate $\widehat{\beta}_{GMM}$. It is more common, however, to construct a \emph{new} estimator of $\Omega$ using the residuals from the efficient estimator. Define:
  $$\widehat{u} = y - X\widehat{\beta}_{GMM}$$
Using these residuals, we can construct estimators of $\Omega$ exactly as we did above using the two-stage least squares residuals. Let $\widetilde{\Omega}$ be such an estimator. Then we can use $X'Z/n$ to estimate $E[\mathbf{x}\mathbf{z}']$ and calculate the variance matrix. Rather than directly inverting, we can use the Cholesky and QR decompositions as above. Finally, to calculate the J-test statistic we need to use both $\widetilde{\Omega}$ and $\widehat{u}$. For the Andrews (1999) criteria, we will use a centered, heteroskedasticity-robust estimator, as described above. If we take a Cholesky Decomposition of $\widetilde{\Omega}$, namely $\widetilde{\Omega} = \widetilde{L}\widetilde{L}'$, we can calculate the statistic as follows
  \begin{eqnarray*}
    J &=& n^{-1} (\widehat{u}'Z) \widetilde{\Omega}^{-1}(Z' \widehat{u})\\
      &=& n^{-1} (\widehat{u}'Z) \left(\widetilde{L}\widetilde{L}'\right)^{-1}(Z'\widetilde{u})\\
      &=& n^{-1} (\widehat{u}'Z) (\widetilde{L}')^{-1}\widetilde{L}^{-1}(Z'\widetilde{u})\\
      &=& n^{-1} \left(\widetilde{L}^{-1}Z' \widehat{u} \right)'\left(\widetilde{L}^{-1}Z' \widehat{u} \right)\\
      &=& v'v/n
  \end{eqnarray*}
where $v=\widetilde{L}^{-1}Z' \widehat{u}$ solves $\widetilde{L}v = Z'\widehat{u}$. Since $\widetilde{L}$ is lower-triangular, we can calculate $v$ by forward substitution, namely $v = \texttt{forwardsolve}(\widetilde{L}, Z'\widehat{u})$. 




\subsection{Valid Post-Selection Confidence Intervals}
\todo[inline]{Express limit distribution of FMSC criterion for this example in terms of M, write out the moment averaging estimator as a weighted sum, rearrange and write down the limit distribution. Finally, give the simulation-based procedure in terms of the notation of this example.}

\subsection{Andrews (1999) Moment Selection Criterion}
Need to write out exactly how we will compute the J-test statistic, which covariance matrix we will use, etc.

\subsection{Canonical Correlation Information Criterion}
For more details on the material discussed here see Hall \& Peixe (2003) and Jana (2005), and Hall (2005)

Let $c$ index moment sets and consider moment conditions of the form 
  $$E[z_t(c)u_t(\theta_0)]=0$$
where $\theta_0$ is a $p$-vector. In a linear instrumental variables model, this is simply
  $$E[z_t(c) (y_t - x_t'\theta_0)] = 0$$
Now, let $\rho_1(c), \hdots, \rho_p(c)$ denote the canonical correlations between $\partial u_t(\theta)/\partial \theta$ and $u_t(\theta)$. For a linear instrumental variables model, 
  $$\frac{\partial u_t(\theta)}{\partial \theta} = x_t$$
so $\rho_i(c)$ is the $i$th canonical correlation between $x_t$ and $z_t(c)$. 

Essentially, Hall \& Peixe (2003) use a result of Sargent to express the variance matrix of the GIV estimator, under certain assumptions, in terms of these canonical correlations. Under the standard first-order limit theory, adding extra instruments can never increase the asymptotic variance. However, adding redundant instruments, instruments that are simply a linear combination of the instruments already in the model, creates problems in finite samples. The idea is to eliminate such ``redundant'' instruments. It turns out that redundancy can be related directly to the canonical correlations in this setting. In particular, a collection of instruments $z(c_2)$ is redundant given $z(c_1)$ if the canonical correlations do not change when the instruments in $z(c_2)$ are added to a model that already includes $z(c_1)$. Using their notation, $z_t(c_2)$ is redundant given $z(c_1)$ if and only if
  $$\rho_i(c_1 + c_2) = \rho_1(c_1) \quad \forall i = 1,2, \hdots, p$$
The idea behind the CCIC is to consider a similar condition for the sample analogue of the canonical correlations. In some cases, although not in the linear IV model, $\partial u_t(\theta)/\partial \theta$ will depend on $\theta$. Since this is an unknown, when needed we substitute a preliminary estimator $\widetilde{\theta}_T$, yielding $\partial u_t(\widetilde{\theta}_T)/\partial \theta$.

The canonical correlations information criterion (CCIC) is given by
  $$CCIC(c) = T\sum_{i=1}^p \log\left[1 - r_{i,T}(c)^2 \right] + (|c| - p)\mu_T$$
where $r_{i,T}(c)$ is the $i$th sample canonical correlation between $\partial u_t(\theta)/\partial \theta$ and $u_t(\theta)$. Again, for a linear IV model, this reduces to the $i$th sample canonical correlation between $x_t$ and $z_t(c)$. For a linear IV model in which $x_t$ is a scalar, there is only a single sample canonical correlation and when squared it equals the \emph{first-stage} $R^2$ using the corresponding set of instruments (Jana, 2005). The quantity $|c| - p$ measures the degree of over-identification provided by the instrument $z_t(c)$ since $|c|$ is the number of instruments, and $p$ the number of regressors. Different choices for $\mu_T$ give rise to different versions of the CCIC by analogy to Andrews' (1999) GMM moment selection criteria. In particular, 
  \begin{eqnarray*}
    \mbox{AIC-type} && \mu_T = 2\\
    \mbox{BIC-type} && \mu_T = \log(T)\\
    \mbox{Hannan-Quinn-Type} && \mu_T = b \log(\log(T)), \quad b > 2
  \end{eqnarray*}
In each case we select the specification that \emph{minimizes the criterion}.

\subsection{Canonical Correlation Analysis} 
To implement the CCIC, we need to know how to calculate the canonical correlations and understand what they mean. Since I'll only be looking at linear models, I simply need to consider canonical correlations between $x_t$ and $z_t(c)$. Although I use different notation here, following the notation of Mardia, Kent \& Bibby (1980) upon which this discussion is based, throughout we can think of $\mathbf{x}$ as $x_t$ and $\mathbf{y}$ as $z_t(c)$.


Let $\mathbf{x}$ be a random $q$-vector and and $\mathbf{y}$ be a random $p$-vector with variance-covariance matrix
  $$Var\left[\begin{array}
    {c} \textbf{x} \\ \textbf{y}
  \end{array}\right] = \left[ \begin{array}
    {cc} \Sigma_{xx} & \Sigma_{xy}\\
    \Sigma_{yx} & \Sigma_{yy}
  \end{array}\right]$$
and let $\mathbf{a}$ and $\mathbf{b}$ be $q$ and $p$-vectors of weights, respectively. Then,
\begin{eqnarray*}
  Corr(\textbf{a}'\textbf{x}, \textbf{b}'\textbf{y}) &=& \frac{Cov(\textbf{a}'\textbf{x}, \textbf{b}'\textbf{y})}{SD(\textbf{a}'\textbf{x})SD(\textbf{b}'\textbf{y})} = \frac{\mathbf{a}'\Sigma_{xy}\mathbf{b}}{\left(\mathbf{a}'\Sigma_{xx}\mathbf{a} \right)^{1/2}\left(\mathbf{b}'\Sigma_{yy}\mathbf{b} \right)^{1/2}}\\
\end{eqnarray*}
Since
\begin{eqnarray*}
  Var\left[\begin{array}
    {c} \mathbf{a}'\textbf{x} \\ \mathbf{b}'\textbf{y}
  \end{array}\right] &=& Var\left(\left[ \begin{array}
    {cc} \mathbf{a}' & \mathbf{0}_p' \\
       \mathbf{0}_q' & \mathbf{b}'
  \end{array}\right] \left[\begin{array}
    {c} \textbf{x} \\ \textbf{y}
  \end{array}\right]\right)\\
  &=& \left[ \begin{array}
    {cc} \mathbf{a}' & \mathbf{0}_p' \\
       \mathbf{0}_q' & \mathbf{b}'
  \end{array}\right]\left[ \begin{array}
    {cc} \Sigma_{xx} & \Sigma_{xy}\\
    \Sigma_{yx} & \Sigma_{yy}
  \end{array}\right] \left[ \begin{array}
    {cc} \mathbf{a} & \mathbf{0}_q \\
      \mathbf{0}_p & \mathbf{b}
  \end{array}\right]\\
  &=& \left[ \begin{array}
    {cc} \mathbf{a}' & \mathbf{0}_p' \\
       \mathbf{0}_q' & \mathbf{b}'
  \end{array}\right] \left[\begin{array}
    {cc}
    \Sigma_{xx} \mathbf{a} & \Sigma_{xy} \mathbf{b}\\
    \Sigma_{yx} \mathbf{a} & \Sigma_{yy} \mathbf{b}
  \end{array} \right]\\
    &=& \left[ \begin{array}
      {cc}
      \mathbf{a}'\Sigma_{xx}\mathbf{a} & \mathbf{a}'\Sigma_{xy}\mathbf{b} \\
      \mathbf{b}'\Sigma_{yx} \mathbf{a}& \mathbf{b}' \Sigma_{yy} \mathbf{b}
    \end{array}\right]
\end{eqnarray*}
Now, define $\rho(\mathbf{a}, \mathbf{b}) = Corr(\textbf{a}'\textbf{x}, \textbf{b}'\textbf{y})$. Canonical Correlation analysis begins by asking which values of $\mathbf{a}$ and $\mathbf{b}$ \emph{maximize} this correlation. This is equivalent to the constrained maximization problem
$$\max_{\textbf{a}, \textbf{b}} \textbf{a}'\Sigma_{xy}\textbf{b} \quad \mbox{subject to} \quad \textbf{a}'\Sigma_{xx}\textbf{a} = \textbf{b}' \Sigma_{yy} \textbf{b} = 1$$
Note that the \emph{sign} of the correlation that solves this problem is \emph{not identified} since we can always reverse it by multiplying $\mathbf{a}$ by $-1$. Thus we will maximize the \emph{squared correlation} instead. Since we can re-write the squared objective function as follows
$$ \left(\textbf{a}'\Sigma_{xy}\textbf{b}\right)^2  =  \left(\textbf{a}'\Sigma_{xy}\textbf{b}\right)'  \left(\textbf{a}'\Sigma_{xy}\textbf{b}\right) = \textbf{b}' \Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy}\textbf{b}$$
our problem becomes
$$\max_{\textbf{b}} \textbf{b}' \Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy}\textbf{b} \quad \mbox{subject to} \quad  \textbf{a}'\Sigma_{xx}\textbf{a} = \textbf{b}' \Sigma_{yy} \textbf{b} = 1$$
The solutions $\mathbf{a}_1, \mathbf{b}_1$ of this problem are called the \emph{first canonical vectors} for $\mathbf{x}$ and $\mathbf{y}$. The linear combinations $\mathbf{a}_1' \mathbf{x}$ and $\mathbf{b}_1' \mathbf{y}$ are called the \emph{first canonical correlation variables}.

But we don't have to stop here. After finding the linear combinations that maximize the correlation, it makes sense to ask ``what's left over?'' The way this is phrased in canonical correlation analysis is to ask which vectors $\mathbf{a}_2, \mathbf{b_2}$ solve the same problem as before subject to the \emph{additional constraint} 
  $$Corr(\mathbf{a}_1' \mathbf{x}, \mathbf{a}_2' \mathbf{x}) = Corr(\mathbf{b}_1' \mathbf{y}, \mathbf{b}_2' \mathbf{y}) =0$$
In other words, we requires that the \emph{second} canonical correlation variable for $\mathbf{x}$ be uncorrelated with the \emph{first} canonical correlation variable for $\mathbf{x}$ and that the second canonical correlation variable for $\mathbf{y}$ be uncorrelated with the first canonical correlation variable for $\mathbf{y}$. More generally, we can define the \emph{$i$th canonical correlation vectors} $\mathbf{a}_i, \mathbf{b}_i$ as the vectors that maximize $Corr(\mathbf{a}_i \mathbf{x}, \mathbf{b}_i \mathbf{y})$ subject to the constraints,
  \begin{eqnarray*}
    Var(\mathbf{a}_i' \mathbf{x}) = Var(\mathbf{b}_i \mathbf{y}) &=& 1 \\
    Corr(\mathbf{a}_i' \mathbf{x}, \mathbf{a}_j' \mathbf{x}) &=& 0 \quad \forall j = 1, 2, \hdots, i-1\\
    Corr(\mathbf{b}_i' \mathbf{y}, \mathbf{b}_j' \mathbf{y}) &=& 0 \quad \forall j = 1, 2, \hdots, i-1
  \end{eqnarray*}
The variance constraints are the same as above but the correlation constraints are new. To understand them, first write
$$Corr(\mathbf{a}_i' \mathbf{x}, \mathbf{a}_j' \mathbf{x}) = \frac{Cov(\mathbf{a}_i' \mathbf{x}, \mathbf{a}_j' \mathbf{x})}{SD(\mathbf{a}_i'\mathbf{x}) SD(\mathbf{a}_j \mathbf{x})}$$
Expressed in matrix form, we have
  \begin{eqnarray*}
    Var\left[\begin{array}
      {c} \mathbf{a}_i'\mathbf{x} \\ \mathbf{a}_j'\mathbf{x}
    \end{array} \right] &=& Var\left(\left[ \begin{array}
      {c} \mathbf{a}_i' \\ \mathbf{a}_j'
    \end{array}\right] \mathbf{x}\right)= \left[ \begin{array}
      {c} \mathbf{a}_i' \\ \mathbf{a}_j'
    \end{array}\right] \Sigma_{xx}\left[ \begin{array}
      {cc} \mathbf{a}_i & \mathbf{a}_j
    \end{array}\right]\\
      &=& \left[ \begin{array}
        {cc} \mathbf{a}_i'\Sigma_{xx} \mathbf{a}_i & \mathbf{a}_i'\Sigma_{xx} \mathbf{a}_j\\
        \mathbf{a}_j' \Sigma_{xx}\mathbf{a}_i& \mathbf{a}_j' \Sigma_{xx} \mathbf{a}_j
      \end{array}\right]
  \end{eqnarray*}
so the correlation constraint is simply $\mathbf{a}_i' \Sigma_{xx} \mathbf{a}_j = 0$. Using identical reasoning, the equivalent constraint for $\mathbf{y}$ is $\mathbf{b}_i' \Sigma_{yy} \mathbf{b}_j = 0$. It will turn out that we do not need to directly impose the constraint for $\mathbf{y}$ as it will be satisfied automatically provided that we impose the constraint for $\mathbf{x}$. Therefore, to find the $i$th canonical correlation vectors, we solve 
$$\max_{\textbf{a},\textbf{b}}\; \textbf{b}' \Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy}\textbf{b}$$ 
subject to the constraints
\begin{eqnarray*}
   \textbf{a}'\Sigma_{xx}\textbf{a} &=& \textbf{b}' \Sigma_{yy} \textbf{b} = 1\\
    \mathbf{a}_i' \Sigma_{xx} \mathbf{a}_j &=& 0, \; j = 1, \hdots, i-1
\end{eqnarray*}
While we won't impose it as a constraint, it will also turn out that $Corr(\mathbf{a}_i'\mathbf{x}, \mathbf{b}_j \mathbf{y}) = 0$ for all $i,j$. In other words, the canonical correlation variables for $\mathbf{x}$ are uncorrelated with the canonical correlation variables for $\mathbf{y}$. In total, we can construct $k$ canonical correlation vectors, where $k$ is the rank of $\Sigma_{xy}$. We will solve this problem in three steps.


\paragraph{Step \#1} In the first step we \emph{fix} $\mathbf{a}$ and maximize over $\mathbf{b}$, solving
$$\max_{\textbf{b}} \; \textbf{b}' \Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy}\textbf{b} \quad \mbox{subject to} \quad  \textbf{b}' \Sigma_{yy} \textbf{b} = 1$$
Since we have fixed $\mathbf{a}$, we only need to impose the constraints for $\mathbf{b}$. Hence, the Lagrangian is
  $$\mathcal{L}(\textbf{b}, \lambda_b; \textbf{a}) = \textbf{b}'\left(\Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy} \right)\textbf{b} - \lambda_b\left( \textbf{b}'\Sigma_{yy}\textbf{b} - 1\right)$$
yielding the following first-order condition for $\mathbf{b}$
  $$2\left(\Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy} \right)\textbf{b} - 2\lambda_b \Sigma_{yy} \textbf{b} = \textbf{0}$$
since both $\left(\Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy} \right)$ and $\Sigma_{yy}$ are symmetric matrices. We can simplify the first-order condition as follows:
$$\Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy}\textbf{b} =\lambda_b \Sigma_{yy} \textbf{b} $$
Substituting this into the objective function, we have
  $$ \textbf{b}' \left(\Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy}\textbf{b}\right) =\textbf{b}' (\lambda_b\Sigma_{yy}\textbf{b}) = \lambda_b (\textbf{b}' \Sigma_{yy} \textbf{b}) = \lambda_b$$
because of the constraint $\textbf{b}' \Sigma_{yy} \textbf{b} = 1$. In other words, $\lambda_b$ \emph{equals the maximized value of the objective function}. 

It remains to actually \emph{calculate} $\lambda_b$. Rearranging the first-order condition and left-multiplying by $\Sigma_{yy}^{-1}$, we have
  $$\left(\Sigma_{yy}^{-1}\Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy} - \lambda_b \textbf{I}\right)\textbf{b} = \textbf{0}$$
Hence, $\lambda_b$ is an \emph{eigenvector} of the matrix $\Sigma_{yy}^{-1}\Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy}$. 
Since it is simply the outer-product of the vector $\mathbf{a}$, the matrix $\mathbf{a}\mathbf{a}'$ has rank one.\footnote{Its $j$th column is simply $a_j \mathbf{a}$, where $a_j$ is the $j$th element of $\mathbf{a}$.} 
Since the rank of a product of matrices cannot exceed the \emph{minimum} rank of the matrices that are being multiplied, it follows that $\Sigma_{yy}^{-1}\Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy}$ has rank \emph{no greater than} one. 
Hence, this matrix has a most one non-zero eigenvalue, up to multiplicity. To find it, we will use the following result from Mardia, Kent \& Bibby (A.6.2)
\begin{thm}
  Let $A$ be an $m\times n$ matrix and $B$ be a $n \times m$ matrix. Then the non-zero eigenvalues of $AB$ and $BA$ are the same and have the same multiplicity. If $\mathbf{v}$ is a non-trivial, i.e.\ nonzero, eigenvector of $AB$ corresponding to eigenvalue $\lambda \neq 0$, then $B \mathbf{v}$ is a non-trivial eigenvector of $BA$.
\end{thm}
Returning to our example, the role of $A$ is played by $\Sigma_{yy}^{-1}\Sigma_{yx}\mathbf{a}$ and that of $B$ is played by $\mathbf{a}'\Sigma_{xy}$. Hence, to find the non-zero eigenvalue of $\Sigma_{yy}^{-1}\Sigma_{yx} \textbf{a}\textbf{a}' \Sigma_{xy}$, if it exists, it is equivalent to find the nonzero eigenvalue of $\mathbf{a}'(\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})\mathbf{a}$. But this is easy since $\mathbf{a}'(\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})\mathbf{a}$ is a \emph{scalar} and any scalar is \emph{its own} eigenvalue.\footnote{Recall that eigenvalue-eigenvector pairs are defined by $A \mathbf{v} = \lambda \mathbf{v}$. If $A$ is a scalar, say $a$, this becomes $a \mathbf{v} = \lambda \mathbf{v}$ so clearly $\lambda = a$ unless $\mathbf{v}=\mathbf{0}$.} Hence, $\lambda_b = \mathbf{a}'(\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})\mathbf{a}$.

\paragraph{Step \#2} In the preceding step, we  ``concentrated out'' $\mathbf{b}$ from the objective function. That is, we determined that the maximized value of our original objective function, $Corr(\mathbf{a}'\mathbf{x}, \mathbf{b}'\mathbf{y})^2$, equals $\mathbf{a}'(\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})\mathbf{a}$, an expression that does not depend on $\mathbf{b}$. Now we will maximized this \emph{concentrated objective function} subject to the constraints for $\mathbf{a}$, namely
$$\max_\mathbf{a} \; \mathbf{a}'(\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})\mathbf{a}$$
subject to
\begin{eqnarray*}
   \textbf{a}'\Sigma_{xx}\textbf{a} &=& 1\\
    \mathbf{a}_i' \Sigma_{xx} \mathbf{a}_j &=& 0, \; j = 1, \hdots, i-1
\end{eqnarray*}
Now, if $i=1$ so that we are solving for the first canonical correlation vector, the second constraint drops out. In this case the Lagrangian is
  $$\mathcal{L}(\textbf{a}, \lambda) = \mathbf{a}'(\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})\mathbf{a} - \lambda \left(\mathbf{a}' \Sigma_{xx} \mathbf{a}   - 1\right) $$
yielding the first-order condition
  $$2\Sigma_{xy}\Sigma_{yy}^{-1} \Sigma_{yx} - 2\lambda \Sigma_{xx} \textbf{a} = 0$$
since $\Sigma_{xy}\Sigma_{yy}^{-1} \Sigma_{yx}$ and $\Sigma_{xx}$ are symmetric. Simplifying and rearranging, we have
  $$\Sigma_{xy}\Sigma_{yy}^{-1} \Sigma_{yx} \textbf{a} = \lambda\Sigma_{xx} \textbf{a}$$
Substituting this into the concentrated objective function along with the constraint $\mathbf{a}'\Sigma_{xx}\mathbf{a} = 1$, it follows that
  $$\mathbf{a}'(\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}\mathbf{a}) = \textbf{a}'(\lambda\Sigma_{xx} \textbf{a}) = \lambda(\textbf{a}'\Sigma_{xx}\textbf{a}) = \lambda$$
Hence, $\lambda$ \emph{equals the maximized value of the concentrated objective function}. Rearranging the first-order condition and left-multiplying by $\Sigma_{xx}^{-1}$ we have
  $$\left(\Sigma_{xx}^{-1}\Sigma_{xy}\Sigma_{yy}^{-1} \Sigma_{yx}  - \lambda \textbf{I}\right)\textbf{a} = \textbf{0}$$
Therefore, $\lambda$ is an eigenvector of the matrix $\Sigma_{xx}^{-1}\Sigma_{xy}\Sigma_{yy}^{-1} \Sigma_{yx}$. The question is, which one? Since it equals the maximized value of the objective function, $\lambda$ must be as large as possible. Therefore, $\lambda$ must be the \emph{largest eigenvalue} of $\Sigma_{xx}^{-1}\Sigma_{xy}\Sigma_{yy}^{-1} \Sigma_{yx}$. It follows immediately that the first canonical correlation vector for $\mathbf{x}$, namely $\mathbf{a}_1$, equals the \emph{eigenvector corresponding to the largest eigenvalue of} $\Sigma_{xx}^{-1}\Sigma_{xy}\Sigma_{yy}^{-1} \Sigma_{yx}$.

Now, suppose we wanted to find the \emph{second} canonical correlation vector. If we simply \emph{ignored} the correlation constraint, we could proceed exactly as above to show that the Lagrange multiplier will again be an eigenvalue of $\Sigma_{xx}^{-1}\Sigma_{xy}\Sigma_{yy}^{-1} \Sigma_{yx}$ and will equal the maximized value of the concentrated objective function. Accordingly, we should make choose $\lambda$ so it is as large as possible. But now we have to consider the correlation constraint: $\mathbf{a}_2 \Sigma_{xx}\mathbf{a}_1 = 0$. If we choose $\mathbf{a}_2$ to be the eigenvector corresponding to the \emph{second largest} eigenvalue 
\todo[inline]{Need to re-express this in terms of the matrix $K$ defined in Mardia, Kent \& Bibby. Also need to link this to the computation given below and list some properties of canonical correlations. I'll come back to this some day, if I ever need to teach this material, for example!}

\paragraph{Calculating Canonical Correlations}
The exposition above involved the \emph{population} variance-covariance matrix for $\mathbf{x}$ and $\mathbf{y}$, but since this is unknown, to apply canonical correlation analysis to real-data problems we'll need to work with the sample analogue. Rather than using the formulas from above, however, it turns out that we can calculate sample canonical correlations quickly and reliably using the QR decomposition followed by the Singular Value Decomposition. The algorithm is as follows
\begin{alg}[Sample Canonical Correlations Analysis]
Let $X$ be the $n\times q$ matrix of observations of $\mathbf{x}$-observations  and $Y$ be the $n\times p$ matrix of $\mathbf{y}$-observations. Assume that both $X$ and $Y$ have full column rank.
   \begin{enumerate}
     \item Center $X$ and $Y$ by subtracting their respective column means. Call the centered matrices $\widetilde{X}$ and $\widetilde{Y}$.
     \item Calculate the \emph{economical} QR decompositions of the centered data matrices $\widetilde{X}$ and $\widetilde{Y}$ (e.g. \texttt{qr\_econ} in Armadillo)
      \begin{eqnarray*}
        \underset{(n\times q)}{\widetilde{X}} &=& \underset{(n\times q)}{Q_x} \underset{(q\times q)}{R_x}\\
        \underset{(n\times p)}{\widetilde{Y}} &=& \underset{(n\times p)}{Q_y} \underset{(p\times p)}{R_y}\\
      \end{eqnarray*}
    \item Calculate the \emph{economical} Singular Value Decomposition of $Q_x'Q_y$ (e.g.\ \texttt{svd\_econ} in Armadillo) where $r$ is the rank of $Q_x'Q_y$
      $$\underset{(q\times p)}{Q_x'Q_y} = \underset{(q\times r)}{U}\underset{(r\times r)}{D}\underset{(r\times p)}{V'}$$
    \item The non-zero elements of the diagonal matrix $D$, in order from largest to smallest, are the \emph{canonical correlations}.
    \item The canonical correlation vectors for $\mathbf{x}$, in order, are the columns of the matrix $A$ that solves
        $$\underset{(q\times q)(q\times r)}{R_xA} = \underset{(q\times r)}{U}$$
    Since $R_x$ is upper-triangular, this system can be solved by back-substitution.
    \item The canonical correlation vectors for $\mathbf{y}$, in order, are the columns of the matrix $B$ that solves
        $$\underset{(p\times p)(p\times r)}{R_y A} = \underset{(p\times r)}{V}$$
    Since $R_y$ is upper-triangular, this system can be solved by back-substitution.
   \end{enumerate}
 \end{alg} 


\todo[inline]{Why is this formula correct? I have verified that it works, but I still don't fully understand where it comes from. For example, why don't we have to divide through by $n$? Ignoring the factor of $n$, $R_x'Q_x'Q_yR_y$ is the sample covariance between $\mathbf{x}$ and $\mathbf{y}$.}


\end{document}