%!TEX root = main.tex
\subsection{Inference for Moment-Average Estimators}
Suppose that $K_S$, $\varphi_S$, $\theta_0$, $\Omega$ and $\tau$ were all known. 
Then, by simulating from $M$, as defined in Theorem \ref{thm:normality}, the distribution of $\Lambda(\tau)$, defined in Corollary \ref{cor:momentavg}, could be approximated to arbitrary precision. 
This is the basic intuition that I use to devise inference procedures for moment-average and post-selection estimators.

To operationalize this idea, first consider how we would proceed if we knew \emph{only} the value of $\tau$.  
While $K_S$, $\theta_0$, and $\Omega$ are of unknown this presents only a minor difficulty: in their place we can simply substitute the consistent estimators that appeared in the expression for the FMSC above.
To estimate $\varphi_S$, we first need to derive the limit distribution of $\widehat{\omega}_S$, the data-based weights specified by the user. 
As an example, consider the case of moment selection based on the FMSC. Here $\widehat{\omega}_S$ is simply the indicator function
\begin{equation}
	\label{eq:FMSCindicate}
	\widehat{\omega}_S = \mathbf{1}\left\{\mbox{FMSC}_n(S) = \min_{S'\in \mathscr{S}} \mbox{FMSC}_n(S')\right\}
\end{equation}
Substituting estimators of $\Omega$, $K_S$ and $\theta_0$ into $\mbox{FMSC}_S(\tau,M)$, defined in Corollary \ref{cor:FMSClimit}, gives
\begin{equation*}
	\widehat{\mbox{FMSC}}_S(\tau,M) = \nabla_\theta\mu(\widehat{\theta})'\widehat{K}_S\Xi_S \left\{\left[\begin{array}{cc}0&0\\0&\widehat{\mathcal{B}}(\tau,M) \end{array}\right] + \widehat{\Omega}\right\}\Xi_S'\widehat{K}_S'\nabla_\theta\mu(\widehat{\theta})
\end{equation*}
where $\widehat{\mathcal{B}}(\tau,M) = (\widehat{\Psi} M + \tau)(\widehat{\Psi} M + \tau)' - \widehat{\Psi} \widehat{\Omega} \widehat{\Psi}$.
Combining this with Equation \ref{eq:FMSCindicate},
\begin{equation*}
	\widehat{\varphi}_S(\tau,M) = \mathbf{1}\left\{\widehat{\mbox{FMSC}}_S(\tau,M) = \min_{S'\in \mathscr{S}} \widehat{\mbox{FMSC}}_{S'}(\tau,M)\right\}.
\end{equation*}
For GMM-AIC moment selection or selection based on a downward $J$-test, $\varphi_S(\cdot,\cdot)$ may be estimated analogously, following  Theorem \ref{pro:jstat}. 
Continuing to assume for the moment that $\tau$ is known, consider the following algorithm:
\begin{alg}[Simulation-based CI for $\widehat{\mu}$ given $\tau$]
\mbox{}
		\begin{enumerate}
\label{alg:conf_tau_known}
			\item Generate $J$ independent draws $M_j \sim N_{p+q}(0, \widehat{\Omega})$.
			\item Set $\Lambda_j(\tau) = -\nabla_\theta\mu(\widehat{\theta})'\left[\sum_{S \in \mathscr{S}} \widehat{\varphi}_S(\tau,M_j) \widehat{K}_S\Xi_S\right] (M_j + \tau)$.
			\item Using $\{\Lambda_j(\tau)\}_{j=1}^J$, calculate $\widehat{a}(\tau)$, $\widehat{b}(\tau)$ such that
		$P\left\{ \widehat{a}(\tau) \leq\Lambda(\tau)\leq \widehat{b}(\tau) \right\} = 1 - \alpha$.
  \item Define the interval 
    $ \mbox{CI}_{sim}=\left[ \widehat{\mu} - \widehat{b}(\tau)/\sqrt{n}, \quad \widehat{\mu} - \widehat{a}(\tau)/\sqrt{n} \right]$.
		\end{enumerate}
\end{alg}

Given knowledge of $\tau$, Algorithm \ref{alg:conf_tau_known} yields valid inference for $\mu$.
The problem, of course, is that $\tau$ is unknown and cannot even be consistently estimated.
One idea would be to substitute the asymptotically unbiased estimator $\widehat{\tau}$ from \ref{thm:tau} in place of the unknown $\tau$.
This gives rise to a procedure that I call the ``1-Step'' confidence interval:

\begin{alg}[1-Step CI] 
  \label{alg:1step}
Carry out of Algorithm \ref{alg:conf_tau_known} with $\tau$ set equal to the estimator $\widehat{\tau}$ from Theorem \ref{thm:tau}, yielding 
$ \widehat{\mbox{CI}}_{1}=\left[ \widehat{\mu} - \widehat{b}(\widehat{\tau})/\sqrt{n}, \quad \widehat{\mu} - \widehat{a}(\widehat{\tau})/\sqrt{n} \right]$.
\end{alg}

The 1-Step interval defined in Algorithm \ref{alg:1step} is conceptually simple, easy to compute, and can perform well in practice, as I explore below.
But as it fails to account for sampling uncertainty in $\widehat{\tau}$,  it does \emph{not} necessarily yield asymptotically valid inference for $\mu$.
Fully valid inference requires the addition of a second step to the algorithm:

\begin{alg}[2-Step CI]
\label{alg:conf}
\mbox{}
\begin{enumerate}
  \item Construct a $(1-\delta)\times 100\%$ confidence region $\mathscr{T}$ for $\tau$ using Theorem \ref{thm:tau}. 
  \item For each $\tau^* \in \mathscr{T}$ carry out Algorithm \ref{alg:conf_tau_known}, yielding a $(1 - \alpha)\times 100\%$ confidence interval $\left[\widehat{a}(\tau^*),\widehat{b}(\tau^*)\right]$ for $\Lambda(\tau^*)$.  
	\item Set $\displaystyle \widehat{a}_{min}=\min_{\tau^* \in \mathscr{T}} \widehat{a}(\tau^*)$ and $\displaystyle \widehat{b}_{max}= \max_{\tau^* \in \mathscr{T}} \widehat{b}(\tau^*)$. 
	\item Construct the interval 
    $ \widehat{\mbox{CI}}_{2}=\left[ \widehat{\mu} - \widehat{b}_{max}/\sqrt{n}, \quad \widehat{\mu} - \widehat{a}_{min}/\sqrt{n} \right]$
\end{enumerate}
\end{alg}

The 2-Step interval defined in Algorithm \ref{alg:conf} has asymptotic coverage probability of \emph{at least} $(1-\alpha-\delta)\times 100\%$.

\begin{thm}[2-Step CI]
\label{thm:sim}
Let $\widehat{\Psi}$, $\widehat{\Omega}$, $\widehat{\theta}$, $\widehat{K}_S$, $\widehat{\varphi}_S$ be consistent estimators of $\Psi$, $\Omega$, $\theta_0$, $K_S$, $\varphi_S$ and define 
$\Delta_n(\widehat{\tau},\tau^*) = \left(\widehat{\tau} - \tau^*\right)' (\widehat{\Psi}\widehat{\Omega}\widehat{\Psi}')^{-1} \left(\widehat{\tau} - \tau^*\right)$ 
and 
$\mathscr{T}(\widehat{\tau},\delta) = \left\{\tau^* \colon  \Delta_n(\widehat{\tau},\tau^*) \leq \chi^2_q(\delta)\right\}$
where $\chi^2_q(\delta)$ denotes the $1-\delta$ quantile of a $\chi^2$ distribution with $q$ degrees of freedom.
Then, the interval $\mbox{CI}_{sim}$ defined in Algorithm \ref{alg:conf} has asymptotic coverage probability no less than $1-(\alpha + \delta)$ as $J,n\rightarrow \infty$.
\end{thm}

